\input{draft-header}
\def\myshorttitle{Publishing while female}
\def\mysubtitle{Are women held to higher standards? Evidence from peer review.}
\def\myauthor{Erin Hengel}
\def\mydate{August 2021}
\def\orgdate{(First version: September 2015)}
\def\address{Chatham Street, Liverpool L69 7ZH, U.K.}
\def\telephone{+44 (0)7 824 863 784}
\def\affiliation{University of Liverpool}
\def\department{Department of Economics}
\def\position{Lecturer in Economics}
\def\email{erin.hengel@gmail.com}
\def\thanksbitches{This paper is a revised version of the third chapter of my dissertation (University of Cambridge, September 2015). I am grateful to my supervisor Christopher Harris for (a) excellent guidance and (b) thinking this was a good idea. I am similarly indebted to Jeremy Edwards and my examination committee (Leonardo Felli and Hamish Low) for considerable input and advice. I also thank Miguel Almunia, Carolina Alves, Oriana Bandiera, Anne Boring, Cheryl Carleton, Gary Cook, Dominique Demougin, Harris Dellas, Carola Frege, Claudia Goldin, Olga Gorelkina, Jane Hunt, Ali Ismail, Adam Jaffe, Katya Kartashova, John Leahy, Brendan McCabe, Reshef Meir, Imran Rasul, Ludovic Renou, Kevin Schnepel, Joel Sobel, Heidi Williams, Jarrod Zhang, numerous anonymous referees and editors, and participants at many seminars, workshops and conferences. Finally, this paper could not have been written without substantial, careful research assistance by Michael Hengel (my dad), Eileen Hengel (my sister) and Lunna Ai (my actual research assistant). All errors, of course, are mine.}
\def\keywords{gender, readability, discrimination, bias, academic publishing}
\def\subject{Gender bias}
\def\jel{A11, J16, J24}
\def\wordcount{20,300}
\input{draft-begin-single}

\section{Introduction}
\label{introduction}

Female academics are less likely to make tenure, take longer when they do and earn much less than their male peers~\citep{Bandiera2016,Ceci2014,Ginther2004,Weisshaar2017}. In economics, only a quarter to a third of assistant and associate professors---and no more than 15 percent of professors---are women~\citep{Lundberg2019,Gamage2020,Bateman2021}.\footnote{\citet{Auriol2019} find female economists are slightly better represented at European institutions.}

There are a number of factors driving these outcomes. Women have smaller research networks~\citep{Ductor2018}, make different career choices and face different constraints (\emph{e.g.}, motherhood). They may also be held to tougher standards. For example, evidence suggests that their qualifications and ability are underestimated~\citep{Foschi1996,Grunspan2016,Moss-Racusin2012,Reuben2014}; female-authored papers are evaluated more critically~\citep{Goldberg1968,Krawczyk2016,Paludi1983}; when collaborating with men, women are given less credit~\citep{Heilman2005,Sarsons2020}.

In this paper, I investigate whether top economics journals apply similar standards to men's and women's manuscripts---and specifically, similar writing standards. In the English language, clearly written prose is better prose, all things equal. Thoughtful word choice and simple sentence structure make text easier to understand, more interesting to read and expose inconsistencies long-winded writing often hides. Journal editors tend to agree. \emph{Econometrica} asks authors to write ``crisply but clearly'' and to take ``the extra effort involved in revising and reworking the manuscript until it will be clear to most if not all of our readers'' (\href{http://www.econometricsociety.org/publications/econometrica/information-authors/instructions-submitting-articles}{\emph{Econometrica} submission guidelines}, June 2016).

To measure writing clarity, I apply five highly tested ``readability'' formulas to 9,117 article abstracts published between 1950--2015 in the \emph{American Economic Review} (\emph{AER}), \emph{Econometrica} (\emph{ECA}), \emph{Journal of Political Economy} (\emph{JPE}) and \emph{Quarterly Journal of Economics} (\emph{QJE}). With these data, I document several stylised facts.

First, women aren't published that often in ``top-four'' economics journals. The average share of female authors per paper across the entire sample is 7.5 percent. In 2015, that share was still only 15 percent; just eight percent of papers were majority female-authored and only four percent were written entirely by women. Between 2015--2017, \emph{QJE} did not publish a single exclusively female-authored paper. In several recent years, \emph{Econometrica} and \emph{JPE} have not either.

Second, the female-authored abstracts that are published in these journals are 1--6 percent more readable than those by men. Women write better despite adjusting for other factors correlated with quality---including citations, author prominence, seniority and individual fixed effects---accounting for English fluency and adding editor, journal, year and primary and tertiary \emph{JEL} category dummies and roughly controlling for how theoretical vs. empirical a paper is.

Third, the gender gap in readability is 2--3 times larger in the published version of a manuscript compared to its pre-submission version. To arrive at these estimates, I match National Bureau of Economic Research (NBER) working papers with their final published articles. Assuming authors release their manuscripts as NBER working papers at about the same time that they submit them to peer review, these results suggest that female-authored abstracts become 2--5 percent more readable while under review.

Fourth, the portion of the gap formed in peer review reversed direction in journals that blinded referees to authors' identities before the internet. Although standard errors are large and sample sizes small, this evidence tentatively suggests that blind review can mitigate the impact of gender under certain circumstances. It also points to the possibility that editorial\slash refereeing bias at least partially contributes to women's better writing.

Fifth, I do not find evidence that men compensate for their lower quality writing by raising quality on another dimension. More specifically, better writing by female economists could arguably compensate for some other advantage present in men's papers. But as long as men and women are equally capable researchers and similarly informed conditional on controls, the cost to both genders of implementing their respective publication strategies should be equal---otherwise, women could reduce the cost of producing a paper while holding acceptance rates constant by adopting a strategy marginally closer to men's (or visa versa). A rough test of this hypothesis using submit-accept times from \emph{Econometrica} and the \emph{Review of Economic Studies} (\emph{REStud}) suggests this isn't the case. The cost to men of revising a paper appears to be much lower than the cost to women: female-authored papers spend three to six months longer under review compared to observably equivalent male-authored papers. The effect persists across a range of specifications that account for, among other things, citations, readability, author seniority, motherhood, childbirth and field.

Finally, it does not appear that women are rewarded for their better writing. Recent evidence from a set of comparable journals suggests female-authored papers are not accepted at higher rates after conditioning on similar co-variates~\citep{Card2020}.

These stylised facts suggest women spend too much time rewriting old papers and not enough time writing new papers, relative to men. The lack of a gender gap under blind review points to external factors beyond their control, but women's better writing could also be driven by internal factors such as higher risk-aversion ~\citep[for a review, see][]{Croson2009}, lower confidence ~\citep[see, \emph{e.g.},][]{Coffman2014,Exley2019}, a tendency to update too much when faced with negative signals~\citep{Mobius2014}, be more easily swayed by the opinions of others~\citep{Born2019} or exert more effort on low stakes tasks~\citep{Schlosser2019} and those which do not yield obvious benefits~\citep{Babcock2017}.

To investigate these mechanisms, I model an author's decision-making process as if it were governed by the rational behaviour of women who update their beliefs about the readability thresholds they are held to as they gain experience in peer review. The intuition of the model is as follows. A gender readability gap that decreases with experience suggests that women initially overestimate referees' and editors' writing thresholds but revise their beliefs downward as they submit more papers to peer review. This pattern indicates that internal factors predominantly drive the gap. On the other hand, a gap that increases with experience suggests that women initially underestimate writing thresholds but revise their beliefs upwards as they gain a better understanding of peer review. In this case, tougher standards probably play a role in women's choices unless their extra effort is rewarded with higher acceptance rates relative to men.

The model identifies three testable conditions which can help establish whether external factors are at all important to the existence and evolution of the gender readability gap: (1) experienced women write better than equivalent men; (2) women improve their writing over time; (3) female-authored papers are accepted no more often than equivalent male-authored papers. Evidence from the pooled sample of authors suggests conditions (1) and (2) hold: on average, women's writing gradually gets better but men's does not; between authors' first and third published articles, the readability gap increases by up to 12 percent. Although my data do not identify Condition (3), female-authored papers are accepted less often than equivalent male-authored papers at a similar set of journals~\citep{Card2020}.

The validity and accuracy of these results primarily rely on two critical---and strong---assumptions. First, the more experience a women gains in peer review, the fewer mistakes she makes about referees' and editors' standards. Second, male and female authors write papers that are identical with respect to topic, novelty and quality, conditional on controls. The second assumption is especially likely to be violated. Furthermore, concluding that higher standards are present requires that all three conditions hold for the same author---that is, the same woman must write better than an equivalent man, not be accepted at rates higher than he is \emph{and} raise the quality of her writing over time.

To improve my estimates in both respects, I additionally restrict the sample to authors with three or more top-four publications and match observably similar male and female economists based on characteristics---including citations and field---that predict the topic, novelty and quality of their research. Within-person readability comparisons are used to determine if Condition (2) was satisfied for each author in a matched pair. Between-person comparisons after authors gain experience in peer review are used to establish whether Condition (1) was satisfied for the male or female member.

I find that Conditions (1) and (2) were satisfied for the same author in 65 percent of matched pairs; in two-thirds of those, the member who satisfied them was female. A counterfactual analysis suggests that higher standards mean women write, on average, 5 percent more readably than they otherwise would. I emphasise, however, that the reliability of these estimates depends on the extent to which the observables authors are matched on fully capture differences in the non-readability qualities of papers. Within each matched pair, they also require that Condition (3) is satisfied for the same author who satisfied Conditions (1) and (2), something I cannot directly test with my data. Additionally, the validity of the counterfactual analysis and the precision of its estimates rely on strong assumptions about men's and women's beliefs and the impact co-authors have on an article's readability.

I conclude by showing suggestive evidence that women navigate higher standards by altering their behaviour. Guided by the model, I compare papers pre- and post-review as authors gain experience. This allows me to tease out the direct effect of higher standards---readability changes made \emph{in} peer review---from its ``feedback'' effect---readability changes made \emph{before} peer review in anticipation of those higher standards. I find that the direct effect dominates in authors' earliest papers. In fact, there is no significant gender difference between draft readabilities in men's and women's first top publications; it emerges entirely in peer review. In later papers, however, women write well upfront; the gap chiefly materialises \emph{before} peer review. These results further support the hypothesis that women do not initially expect higher standards and instead learn about them over time. They also suggest that women adapt to higher standards by writing their future papers more readably prior to submission.

This paper contributes to the literature in several ways. First, to the best of my knowledge, I am the first to document empirical evidence that suggests women may be held to higher standards in the peer review process (as opposed to its outcome). Higher standards have recently been established using citations as a proxy for manuscript quality~\citep{Card2020,Moon2020,Grossbard2018}.\footnote{Data from a field journal find female-authored manuscripts are subject to greater scrutiny and spend longer under review~\citep{Alexander2021}. A review-time gap was not, however, present in a set of journals that semi-overlap with those analysed here~\citep{Card2020}.} They also align with research on employee performance reviews, teaching evaluations and online comments: women receive more abusive feedback, less credit for intelligence and creativity and are expected to be more organised, prepared and clear~\citep[see, \emph{e.g.},][]{Boring2017,Mengel2017,Correll2016,Gardiner2016,Wu2019}.

Second, this paper proposes a novel explanation for academia's ``Publishing Paradox'', ``Leaky Pipe\-line'' and general promotion gap. Higher standards cause collateral damage to women's productivity: spending more time revising old research means there's less time for new research; fewer papers results in fewer promotions, possibly driving women into fairer fields.\footnote{See also \citet{Bright2017} for a similar idea in the philosophy literature. This idea has also been informed by extensive research on editorial patterns~\citep{Ellison2002a,Card2013,Clain2017,Casnici2016,Card2020}, bias in editorial decisions~\citep{Abrevaya2012,Card2017,Bransch2017,Card2020} and female academics' lagging productivity and underrepresentation~\citep{Ductor2018,Bayer2016,Ginther2004,Teele2017,Chari2017}.} They may also help explain why so few women publish entirely female-authored papers, despite being the work tenure committees give them the most credit for~\citep{Sarsons2020}.

Third, my conclusions relate to a more general debate about gender differences in labour market outcomes.\footnote{Traditional hypotheses focus on obvious discrimination~\citep{Goldin2000}, motherhood~\citep{Bertrand2010} and differences in behaviour ~\citep[\emph{e.g.},][]{Niederle2010}. Contemporary theories tend to stress inflexible working conditions~\citep{Goldin2014, Goldin2016}, preferences~\citep[for a review, see][]{Blau2016} and policy design~\citep{Antecol2016}.} Higher standards impose a quantity vs. quality trade-off that characterises female output in many professions---\emph{e.g.}, doctors, real estate agents and airline pilots ~\citep[for a discussion, see][]{Hengel2017}. Their downstream effects may contribute to several employment phenomena, including women's tendencies to concentrate in certain sectors and occupations~\citep{Blau2016,Cortes2016}, under-negotiate pay~\citep{Babcock2003} and apply only to jobs they feel fully qualified for~\citep{Mohr2014}. They may also reinforce work habits---\emph{e.g.}, conscientiousness, tenacity and diligence---that correlate with quality and connote ``femininity'': for example, female physicians consult longer with patients~\citep{Roter2004}, female politicians fundraise more intensely~\citep{Jenkins2007}, female faculty commit fewer instances of academic misconduct~\citep{Fang2013} and female lawyers make fewer ethical violations~\citep{Hatamyar2004}.

Fourth, this paper joins an emerging body of economic research studying how the experience and anticipation of discrimination affects choices and behaviour. Earlier theoretical work focused on the impact discrimination has on investment in education and occupational choice~\citep[see, \emph{e.g.},][]{Lundberg1983,Coate1993,Goldin2014a}. More recent empirical research explores how stereotypes negatively impact performance~\citep{Coffman2014,Bordalo2016,Lavy2015,Glover2017,Carlana2019}. My results suggest that rational responses to discrimination can distort productivity measurement~\citep[see also][]{Parsons2011} and blur the line between biased treatment and voluntary choice.

Finally, this paper makes a related methodological contribution. Discrimination is generally identified from the actions~\citep[\emph{e.g.},][]{Neumark1996,Bertrand2004} and\slash or learning processes~\citep[\emph{e.g.},][]{Altonji2001,Fryer2013} of those who discriminate. But repeatedly observing individuals' choices can also bring to light the bias they are exposed to. In particular, multiple choices made under changing conditions reveals information about agents' intrinsic preferences and knowledge of underlying processes. Using this information, one can isolate group differences in the observed equilibrium from those that would have occurred in a non-discriminatory counterfactual one. For example, assuming intrinsic preferences are fixed over time, earlier choices provide an upper bound on the impact they play in gender readability gaps; assuming authors update beliefs about the relationship between readability and acceptance rates means later choices are made with more accurate beliefs. Although this strategy relies on several strong assumptions, it may be useful for understanding and bounding the effect discrimination has on the long-term decision-making processes of those who experience it.

The remainder of the paper proceeds in the following order. \autoref{data} describes the data. In \autoref{stylisedfacts}, I present the stylised facts about gender, readability and review times in top economics journals. \autoref{mechanisms} investigates mechanisms driving these stylised facts. \autoref{conclusion} concludes.

\section{Data}
\label{data}

The data include every English-language article published with an abstract in \emph{AER}, \emph{ECA}, \emph{JPE} and \emph{QJE} between January 1950 and December 2015 (inclusive). The largest sample is from \emph{Econometrica} which consistently published abstracts with its articles prior to 1950. \emph{JPE} added them in the 1960s and \emph{QJE} in 1980. \emph{AER} came last in 1986. Errata and corrigenda are excluded, as are articles from \emph{Papers \& Proceedings} (\emph{P\&P}) issues of \emph{AER}, unless otherwise mentioned. \aref{appendixarticlecount} displays data coverage by journal and decade.

For textual input, I use abstracts. Abstract readability is strongly positively correlated with the readability of other sections of a paper (see \autoref{figure3} and  \citet{Hartley2003b,Plaven-Sigray2017}). Its structure is standardised in a manner optimal for computing readability scores. Many abstracts have also been converted to accurate machine readable text therefore curbing errors in transcription.

For the analysis in \autoref{nber}, I collected draft abstracts from NBER Technical and Working Paper Series. To match published articles with their NBER drafts, I used citation data from RePEc and searched NBER's database directly for unmatched papers authored by NBER family members. I eventually matched 1,988 NBER working papers to 1,986 published articles. (The mapping is not one-for-one because a small number of working papers were eventually published as multiple articles or combined into one.) This represents approximately one-fifth of all manuscripts in the data and a third of all manuscripts published between 1990--2015. Descriptive statistics are shown in \autoref{nberresults}.

The analysis in \autoref{duration} compiles submit-accept times at \emph{Econometrica} (1970--2015) and \emph{REStud} (1976--2015), a fifth highly respected economics journal. (\emph{AER}, \emph{JPE} and \emph{QJE} do not publish the dates manuscripts were submitted and accepted.) I obtained the data from journals' online archives or extracted it from digitised articles using the open source command utility \texttt{pdftotext}. \autoref{duration} displays and discusses basic summary statistics.

In \autoref{quantification}, I analyse readability at the author-level using both the entire sample and the sample of published articles matched with NBER working papers. To generate a panel dataset following author $i$ over the $t\in\{1,\ldots\,T_i\}$ papers he publishes in a top-four journal, I duplicate each article $N_j$ times, where $N_j$ is the number of co-authors on paper $j$. I then assign observation $j_n$ article $j$'s $n\text{th}\in\{1,\ldots,N_j\}$ co-author. To account for duplicate articles, observations in relevant estimates are weighted by $1/N_j$.

To control for the impact of blinded review, I constructed a dummy variable equal to one if a paper underwent double-blind review before the internet at \emph{AER} and \emph{QJE}, the only two journals with an official double-blind review policy in place at some point during the time period covered by the data.\footnote{Double-blind review was likely less effective after the internet was adopted~\citep[for anecdotal evidence, see, \emph{e.g.},][]{Goldberg2014}. I therefore only evaluate the impact of blind review pre-internet, which I define as having been published before Google incorporated in 1998.} \emph{QJE} employed double-blind procedures until 1 June, 2005; \emph{AER} between 1 July, 1989 and 1 July, 2011. From 1 May 1987 to 31 May 1989, the \emph{AER} conducted a randomised controlled trial whereby half of all submitted papers were evaluated by single-blind review; the remaining half were subjected to double-blind review~\citep{Blank1991}. Since referees correctly identified at least one author in 45.6 percent of double-blind reviewed papers, however, only about a quarter of these manuscripts were truly blind reviewed. I therefore classify every paper published during the trial as having undergone single-blind review.

Other control variables used in the analysis include editor fixed effects, dynamic institution fixed effects, primary and tertiary \emph{JEL} fixed effects, controls for author prominence and seniority, English fluency dummies, citation counts (asinh), and controls for motherhood and childbirth (\autoref{duration}, only). I additionally categorised each tertiary \emph{JEL} code as either theory\slash methodology, empirical or other in order to roughly account for how theoretical vs. empirical a paper is. See \aref{appendixcontrols} for further information on how each of these variables were calculated.

\subsection{Readability scores}
\label{readability}

To measure writing clarity, I use the five most common, widely tested and reliable readability formulas for adult-level material: Flesch Reading Ease, Flesch-Kincaid, Gunning Fog, SMOG (Simple Measure of Gobbledegook) and Dale-Chall. The formulas for each are shown in \autoref{tab:formulas}. \aref{appendixreadability} discusses the scores in more detail and reviews the literature on their validity.

\input{/USERS/ERINHENGEL/Dropbox/Readability/draft/0-tex/fixed/Table-1}

The Flesch Reading Ease formula ranks passages of text in ascending order---\emph{i.e.}, more readable passages earn higher scores. The other four formulas generate grade levels estimating the minimum years of schooling needed to confidently understand an evaluated text; as a result, more readable passages earn lower scores. In order to simplify interpretation, I multiple the four grade-level scores by negative one. Thus, higher scores universally correspond to clearer writing throughout this paper.

To calculate the scores, I wrote the Python module \texttt{Textatistic}. Its code and documentation are available on GitHub; a brief description is provided in \aref{appendixtextatistic}. For added robustness, I also re-calculate scores and replicate most results using the \texttt{R readability} package (\aref{appendixalternativemeasure}).

\subsection{Gender}
\label{gender}

Authors were initially assigned a gender using \href{http://genderchecker.com}{GenderChecker.com}'s database of male and female names. Three separate Mechanical Turk workers, a research assistant or I then manually verified them based on photos and other information found on faculty websites, Wikipedia articles, \emph{etc.} In situations where the author could not be found but several people with the same first and last name were and all shared the same gender, the author was also assigned that gender. For the remaining cases, I emailed or telephoned colleagues and institutions associated with the author.

Determining the ``gender'' of a paper is not nearly as straightforward. For solo-authored manu\-scripts---of which there are 4,014 in the sample---gender corresponds to the sex of the author. As discussed in \autoref{underrepresentation}, however, top economics journals have collectively published just 266 by women. Only a slightly larger number were written entirely---\emph{or even mostly}---by women.\footnote{313 papers in the sample were authored entirely by women. Women made up more than 50 percent of all authors in another 47. In 35 observations, a woman was the lead author---\emph{i.e.}, the first author was female in a paper with authors listed non-alphabetically or in which contributions were explicitly noted.}

\input{/USERS/ERINHENGEL/Dropbox/Readability/draft/0-tex/fixed/Figure-1}

Instead, I assume an article's gender is related to its proportion of female authors. A gender readability gap---if it exists---is presumably a function of (i) the probability a passage of text was written and\slash or revised by a female co-author; and (ii) referees' beliefs about female authors' contributions to the writing and\slash or revision of a co-authored paper. Prior research suggests co-authors---regardless of seniority---share responsibility for writing and (especially) revising collaborative work~\citep[see, \emph{e.g.}, ][]{Hart2000a,Kumar2016}. Thus, the intersection of (i) and (ii) is likely positively related to the ratio of female authors on a paper.

\autoref{figureX} corroborates this hypothesis. It plots abstract readability against a paper's ratio of female authors. The slope of the regression line is positive, relatively large (1.88 points on the Flesch Reading Ease scale) and statistically significant; however the relationship is not entirely linear. In particular, it appears to be close to zero when women make up fewer than 50 percent of authors and increasing in the share of female authors only after that. For this reason, I define papers with a strict minority of female authors as male-authored; for papers with 50 percent or more female authors, I allow an article's gender to increase linearly with its proportion of female authors. For robustness, however, I also repeat most analyses (a) on the sample of solo-authored papers, only; (b) comparing papers with a senior female co-author to entirely male-authored papers; (c) on the subset of papers authored by a single gender; (d) using a binary variable equal to one if at least one author is female; and (e) using a binary variable equal to one if at least half of all authors are female. Standard errors from (a) and (c) tend to be larger; those from (b), (d) and (e) usually similarly sized or smaller. In general, however, results do not meaningfully change (\aref{appendixalternativemeasure}).

\section{Stylised facts}
\label{stylisedfacts}

\subsection{Women are under-represented in top-four economics journals}
\label{underrepresentation}

\input{/USERS/ERINHENGEL/Dropbox/Readability/draft/0-tex/fixed/Figure-2}

The right-hand graph in \autoref{figure5} illustrates women's representation in top-four economics journals over time. The number of papers these journals publish with at least one female author has been steadily increasing over the past several decades---from about 5 percent in the late 1980s to around 25 percent in 2015. Growth in the average share of female authors per paper, however, is more muted: it was about 3--4 percent in the late 1980s; by 2015 it had only increased to 15 percent. The discrepancy between these two figures is because growth in female authorship is largely thanks to an increase in the number of mixed-gendered papers authored by a strict minority of women. In fact, there has been almost no growth in the percentage of papers that are either majority or exclusively female-authored: both figures have hovered around 4--5 percent since the early 1990s. \emph{Econometrica} publishes the fewest exclusively female-authored papers (3 percent of all papers published since 1990), \emph{AER} the most (6 percent); \emph{JPE} and \emph{QJE} fall in between (4 and 5 percent, respectively). Percentages are only slightly higher (and rankings identical) for papers with a strict majority of female authors.

The left-hand graph in \autoref{figure5} plots the average percentage of female authors per paper across primary \emph{JEL} categories for articles published between 1990--2015. There are clear differences across fields. The average percentage of female authors per paper was lowest in \emph{JEL} codes B (history of economic thought, methodology and heterodox approaches), C (mathematical and quantitative methods), D (microeconomics) and E (macroeconomics and monetary economics) and highest in I (health, education and welfare), Z (other special topics) and O (economic development, innovation, technological change, and growth). Despite this variation, the percentage of female authors per paper does not exceed 20 percent in any field.

\subsection{Women's papers are more readable}
\label{articlelevel}

\autoref{table2} compares textual characteristics between male-authored papers (defined as having a ratio of female authors below 50 percent) and female-authored papers (defined as having a ratio of female authors at or above 50 percent). It suggests women write shorter, simpler sentences: they contain fewer characters, fewer syllables, fewer words and fewer ``hard'' words. Differences are highly statistically significant.

\input{/USERS/ERINHENGEL/Dropbox/Readability/draft/0-tex/generated/Table-2}

\autoref{table3_FemRatio} presents results from 45 separate ordinary least squares (OLS) regressions of readability scores on the ratio of female authors (papers with fewer than 50 percent female authors are classified as male, see \autoref{gender}). Column (1) includes journal and editor fixed effects and controls for blind review and its interaction with the ratio of female authors on a paper. Columns (2) and (3) add journal-year interaction dummies.\footnote{The coefficients on the journal dummies in (2) are presented in \aref{appendixarticlelevel}. Compared to \emph{AER}, all five scores agree that \emph{Econometrica} is harder to read; four out of five scores suggest \emph{JPE} is, too, while \emph{QJE} is easier.} Column (4) introduces controls for paper $j$'s number of co-authors ($N_j$) and the dynamic institution effects described in \aref{appendixcontrols}. Column (5) adds a dummy variable capturing English fluency; it also controls for article quality (citations (asinh)), co-author prominence ($\text{max. }T$) and seniority at the time of publication ($\text{max. }t$). Columns (6)--(9) are estimated on the sample of articles published after 1990. (7) includes fixed effects for primary \emph{JEL} categories. (8) replaces it with three binary variables meant to capture how theoretical vs. empirical a paper is. (9) includes fixed effects for tertiary \emph{JEL} categories.\footnote{\label{FootnoteAERpp}Due to small sample sizes, column (9) includes 563 articles from \emph{AER P\&P}, coded as a separate journal. Papers published in \emph{AER P\&P} are selected and edited by the American Economic Association's president-elect with the help of a Program Committee (see \href{https://www.aeaweb.org/journals/pandp/about-pandp}{www.aeaweb.org} for more details). \emph{P\&P} does not publish abstracts in its print version; only select years (2003 and 2011--2015) and papers were available online when I collected the data. Excluding these articles does not impact results or conclusions: coefficients are similar to those in column (9), but standard errors are somewhat higher.}

Results in \autoref{table3_FemRatio} suggest that abstracts written by women score about 1--2 points higher on the Flesch Reading Ease scale; according to the four grade-level measures, they take about 1--5 fewer months of schooling to understand. Percentage-wise, women write about 1--4 percent better than men.

\input{/USERS/ERINHENGEL/Dropbox/Readability/draft/0-tex/generated/Table-3-FemRatio}

\aref{appendixjel} explores field in more detail; \aref{appendixauthorlevel} analyses readability at the author-level. Conditional on other explanatory variables, I find little evidence that field drives results in \autoref{table3_FemRatio}. After accounting for author-specific heterogeneity, the gender gap in readability rises to 2--6 percent.

\subsection{Women's papers improve more during peer review}
\label{nber}

\subsubsection{Estimation strategy}
\label{nberidentification}

In this section, I analyse readability changes that occurred during peer review by comparing abstracts pre- and post-review. My first estimation strategy simply regresses each paper's change in score on its gender composition. To understand it, note that the readability of a published paper depends on its earlier draft readability as well as factors that affect writing clarity any time after it was initially drafted:\input{/USERS/ERINHENGEL/Dropbox/Readability/draft/0-tex/equations/equation1}where $R_{jP}$ and $R_{jW}$ are readability scores for working ($W$) and published ($P$) versions of paper $j$, respectively. $\beta_{1P}$ is the coefficient of interest and reflects the particular impact $\text{female ratio}_j$ has in peer review. $\vect X_{jP}$ and $\mu_{jP}$ are $P$-specific observable and unobservable components, respectively. $\vep_{jP}$ is $P$'s error term.

Correlation between $R_{jW}$ and $\text{female ratio}_j$ may bias OLS estimates of $\beta_{1P}$. \autoref{equation3} eliminates the distortion by subtracting $R_{jW}$ from both sides of \autoref{equation2}:\input{/USERS/ERINHENGEL/Dropbox/Readability/draft/0-tex/equations/equation2}Assuming zero partial correlation between $\text{female ratio}_j$ and $\mu_{jP}$, OLS generates an unbiased estimate of $\beta_{1P}$.\footnote{\label{FootnoteColliders}Note that zero correlation between $\text{female ratio}_j$ and $\mu_{jP}$ does not preclude biased estimates of $\beta_{1P}$ when $\mu_{jP}$ is correlated with other explanatory variables that are, in turn, correlated with $\text{female ratio}_j$ by some factor independent of $\mu_{jP}$. Unbiasedness instead requires zero \emph{partial} correlation between $\mu_{jP}$ and $\text{female ratio}_j$.}

An alternative strategy based on \citet{Ashenfelter1994} separately estimates gender differences in the draft and final versions of papers using generalised least squares (GLS). The contemporaneous effect of peer review is identified post-estimation by subtracting coefficients. To implement this set-up, I combine \autoref{equation2} with: (i) the relationship between readability scores and the gender composition of a paper before peer review; and (ii) an equation accounting for potential correlation between observable controls and version-invariant unobservables. The former is defined as:\input{/USERS/ERINHENGEL/Dropbox/Readability/draft/0-tex/equations/equation3}where $\beta_{1W}$ reflects $\text{female ratio}_j$'s impact on readability prior to peer review; $\vect X_{jW}$ and $\mu_{jW}$ are version-invariant observable and unobservable components, respectively; $\vep_{jW}$ is version $W$'s error term. \autoref{equation5} then defines a general structure for potential correlation between $\mu_{jW}$ and observable variables in both \autoref{equation4} and \autoref{equation2}:\input{/USERS/ERINHENGEL/Dropbox/Readability/draft/0-tex/equations/equation4}where $\omega_j$ is uncorrelated with $\text{female ratio}_j$, $\vect X_{jW}$ and $\vect X_{jP}$. Substituting \autoref{equation5} into \autoref{equation4} generates the following reduced form representation of $R_{jW}$:\input{/USERS/ERINHENGEL/Dropbox/Readability/draft/0-tex/equations/equation5}where $\wt\beta_{0W}=\beta_{0W}+\gamma$, $\wt\beta_{1W}=\beta_{1W}+\eta$, $\wt{\bm\uptheta}_W=\bm\uptheta_W+\bm\updelta_W$ and $\wt\vep_{jW}=\vep_{jW}+\omega_j$. $R_{jP}$'s reduced form is similarly found by substituting \autoref{equation6} into \autoref{equation2}:\input{/USERS/ERINHENGEL/Dropbox/Readability/draft/0-tex/equations/equation6}where $\wt{\bm\uptheta}_P=\bm\uptheta_P+\bm\updelta_P$ and $\wt\vep_{jP}=\wt\vep_{jW}+\vep_{jP}$. \autoref{equation6} and \autoref{equation7} are explicitly estimated via feasible GLS (FGLS). $\beta_{1P}$ is identified post-estimation by subtracting reduced form coefficients. Assuming zero partial correlation between $\mu_{jP}$ and $\text{female ratio}_j$, it also generates an unbiased estimate of $\beta_{1P}$.

\subsubsection{Results}
\label{nberresults}

\input{/USERS/ERINHENGEL/Dropbox/Readability/draft/0-tex/generated/Table-4}

\autoref{table5} compares textual characteristics between a paper's draft and final versions in the samples of male-authored (female ratio below 50 percent) and female-authored manuscripts (female ratio at or above 50 percent). It suggests abstract text is altered during peer review. According to the first panel, draft abstracts are longer---more characters, words and sentences---and denser---more syllables, polysyllabic words and difficult words. The biggest changes are made to female-authored papers: figures in column six are 20--30 percent higher (in absolute value) than those in column three. The second panel of \autoref{table5} suggests women's papers become more readable during peer review relative to men's. More generally, they also seem to indicate that peer review improves readability, although results are less clear for male-authored papers.

\autoref{table6_FemRatio}'s first panel displays results from OLS estimation of \autoref{equation2}. Conditional on draft readability, published female-authored papers are more readable than published male-authored papers. Moreover, published article readability positively correlates with draft readability: coefficients on $R_{jW}$ are about 0.8 and consistently highly significant.

\input{/USERS/ERINHENGEL/Dropbox/Readability/draft/0-tex/generated/Table-5-FemRatio}

\autoref{table6_FemRatio}'s remaining columns show results from the two strategies presented in \autoref{nberidentification}. The first strategy regresses each paper's change in score on its ratio of female authors (papers with fewer than 50 percent female authors are classified as male, see \autoref{gender}). As already discussed, an advantage of this strategy is that it more effectively removes the impact of confounding factors---\emph{e.g.}, research field---that are constant between manuscript versions. The FGLS strategy estimates the coefficient on female ratio separately among the sample of working papers and published articles; the impact of gender on the readability gap formed during peer review is the difference between them. The advantage of this strategy is that it allows us to observe an estimate of the gap both before and after peer review.

Results from the first strategy are shown in panel two; results from the second are shown in panel three. Both strategies' estimates of the effect of gender formed during peer review are very similar (columns 4 and 10). They suggest that female-authored abstracts become 2--5 percent more readable while under review. FGLS results further indicate that the gender readability gap is 2--3 times larger in papers' published versions than it was in their pre-print versions.

Interestingly, although citations and abstract readability generally positively correlate with one another (\aref{appendixreadability}), the relationship between citations and the \emph{change} in readability between draft and final versions of a paper is either negative or zero (\aref{appendixdraftcorr}). Although I do not observe how many citations papers would have received had they not gone through peer review, these results tentatively suggest that the revisions women are asked to make during the process may not improve the general quality of their papers as proxied for by citations.

Also included in \autoref{table6_FemRatio} are coefficients on the interaction between female ratio and a dummy variable equal to 1 for papers that underwent double-blind review before the internet. These estimates consistently suggest that the gender readability gap reversed direction when papers were subjected to blind review, although none are statistically significant at traditional levels. In \aref{appendixeventstudy}, I plot average residuals over time for papers published in the \emph{AER} or \emph{QJE} before and after they introduced (or removed) double-blind review from a regression of the differenced readability scores on the ratio of female authors. As the figures illustrates, there is a clear discontinuity in women's average unexplained changes to readability when journals switched to single-blind review (or the internet was introduced). For men, however, unexplained changes do not appear to have been substantially affected by double-blind review, conditional on included controls.

The number of manuscripts---and especially female-authored manuscripts---subjected to double-blind review is small so the coefficients on the interaction between blind review and the ratio of female authors on a paper should be interpreted with caution. Nevertheless, they do provide some (weak) indication that the gender readability gap at least partially results from factors outside of women's control, \emph{e.g.}, editorial and refereeing bias. They may also suggest that masking authors' identities can help reduce peer review's impact on the gender readability gap---but only under certain circumstances. In \aref{appendixdoubleblind}, I analyse the policy's post-internet impact. Gender differences are positive regardless of a journal's official review policy, suggesting that double-blind review may be effective only as long as authors are not identifiable by other means.

\subsubsection{Robustness}
\label{nberrobustness}

In order to conclude that the results presented in the previous section suggest women's papers become more readable while under review, I assume that NBER working papers are not generally released before their authors submit them for peer review.\footnote{Concluding that the gender readability gap is \emph{caused} by peer review requires making the additional assumption that authors do not make post-submission changes to their papers unless requested by referees.} As \aref{appendixtiming} illustrates, this appears to be the case: most manuscripts---and especially most female-authored manuscripts---are submitted to peer review at the same time or before being released as NBER Working Papers.

Another concern is that gender differences in how authors conform to abstract word limits may bias results in \autoref{table6_FemRatio}. To investigate this possibility, I exclude the 642 observations---about 40 percent of the sample---with NBER abstracts longer than the official word limit of the journals in which they were eventually published. Results are presented in \aref{appendixwordlimits}. Coefficient magnitudes are similar to those in \autoref{table6_FemRatio}; standard errors are somewhat larger.

Finally, in an effort to maximise sample sizes, I do not control for field. Although estimates in the second panel arguably implicitly account for field already, I additionally replicate \autoref{table6_FemRatio} with fixed effects for primary \emph{JEL} categories. Results are shown in \aref{appendixnberfield}. Adding \emph{JEL} fixed effects slightly increases standard errors; they otherwise make little difference.

\subsection{Women's papers spend longer under review}
\label{duration}

\begin{quote}

``Writing simply and directly only looks easy''~\citep[][p. 53]{Kimble1994}.
\end{quote}

Good writing takes time~\citep{Hartvigsen1981,Kroll1990}: skilled writers spend longer contemplating a writing assignment, brainstorming and editing; they also write fewer words per minute and produce more drafts~\citep{Faigley1981,Stallard1974}. As a consequence, higher writing standards---and, indeed, higher standards applied more generally~\citep[see, \emph{e.g.},][]{Card2020,Moon2020}---should result in female authors spending longer in peer review, all things equal.

On the other hand, better writing by female economists could perfectly offset some other advantage present in men's papers, conditional on quality. In this case, the time-cost of publishing a paper will instead be gender neutral---since if it weren't, women could reduce their time spent in review by adopting a strategy marginally closer to men's (or visa versa).

To formalise this idea, consider male and female researchers who use strategies $x_m,x_f\in\mathcal X$ to produce papers of identical quality $Q\in\mathcal Q$. Let $q$ represent the function mapping $\mathcal X$ onto $\mathcal Q$ and define $q^{-1}(Q)$ as the set of strategies in $\mathcal X$ that achieve the same $Q$.

If men and women are held to identical standards in peer review, then both will accrue identical rewards, conditional on $Q$, \emph{i.e.},\input{/USERS/ERINHENGEL/Dropbox/Readability/draft/0-tex/equations/equation7}where $x$ is any strategy in $q^{-1}(Q)$ and $a_g(x_g,Q)$ is the acceptance rate for gender $g\in{m,f}$ given strategy $x_g$ and quality $Q$.\footnote{Higher standards come from accepting male-authored papers more often than female-authored papers, conditional on $Q$---\emph{i.e.}, $a_m(x,Q)>a_f(x,Q)$---rewarding men's strategies more than women's strategies even though they both generate identical $Q$---\emph{i.e.}, $a(x_m,Q)>a(x_f,Q)$---or both.}

If men and women are also equally capable researchers, then neither side should have to exert more effort, conditional on acceptance rate (and, hence, $Q$)---\emph{i.e.}, given \autoref{reward}, there must exist some $\hat x_m,\hat x_f\in q^{-1}(Q)$ such that\input{/USERS/ERINHENGEL/Dropbox/Readability/draft/0-tex/equations/equation8}where $c_g(x_g)$ is the cost to gender $g$ of implementing the strategy $x_g$. In the absence of higher standards, \autoref{cost} implies that men's and women's time-cost of review should be equal, conditional on $Q$.

Men's and women's time-cost of review does not appear to be equal. \autoref{figure10} displays histograms of time (in months) between dates male- (defined as having a ratio of female authors below 50 percent) and female-authored papers (defined as having a ratio of female authors above 50 percent) are first submitted to and their final revisions received by the editorial offices of \emph{Econometria} and \emph{REStud}. Women's review times disproportionately cluster above the mean: their articles are five times more likely to experience delays above the 75th percentile than they are to enjoy speedy revisions below the 25th.

\input{/USERS/ERINHENGEL/Dropbox/Readability/draft/0-tex/fixed/Figure-3}

\subsubsection{Estimation strategy and results}
\label{durationresults}

For more precision on gender differences in the time-cost of review---and in order to condition explicitly on quality---I build on a model by  \citet{Ellison2002a}:\input{/USERS/ERINHENGEL/Dropbox/Readability/draft/0-tex/equations/equation9}where $\text{female ratio}_j$ is the ratio of female authors on paper $j$ (papers with fewer than 50 percent female authors are classified as male, see \autoref{gender}), $\text{mother}_j$ and $\text{birth}_j$ are binary variables equal to 1 if article $j$'s authors were all mothers to children younger than five and gave birth, respectively, at some point during peer review, $\max.~t_j$ is the number of prior papers published in a top-five economics journal by article $j$'s most prolific co-author, $\text{no. pages}_j$ refers to the page length of the published article, $\text{order}_j$ is the order in which article $j$ appeared in an issue, $\text{citations}_j$ are the asinh-transformed number of subsequent papers citing $j$, $\text{flesch}_j$ is its Flesch Reading Ease score, the dummy variables $\text{theory}_j$, $\text{empirical}_j$ and $\text{other}_j$ account for how theoretical vs. empirical a paper is, and $\vect X_j$ captures additional fixed effects.

I first estimate \autoref{equation16} on data from \emph{Econometrica}. I then re-estimate it excluding readability, motherhood and childbirth controls---which I do not have for papers published in \emph{REStud}---on the entire sample and each journal separately.

\input{/USERS/ERINHENGEL/Dropbox/Readability/draft/0-tex/generated/Table-6-FemRatio}

\autoref{table10_FemRatio} displays results for \emph{Econometrica}. All models include editor, acceptance year and institution fixed effects.\footnote{See \aref{appendixalternativeyear} for results controlling for years of submission and publication, instead.} Column (1) does not control for motherhood or childbirth; (2) drops papers authored entirely by women who had children younger than five and\slash or gave birth during peer review; (3) controls for motherhood but not childbirth; (4) controls for childbirth but not motherhood; (5) controls for both childbirth and motherhood; (6) and (7) restrict the sample to papers published after 1990; (7) includes fixed effects for primary \emph{JEL} categories.

Every paper published in \emph{Econometrica} undergoes extensive review, but the consistently large and highly significant coefficient on female ratio suggests women bear the brunt of it. The average male-authored paper takes about 18.5 months to complete all revisions; papers by women need almost seven months longer.

Results pooling data from both journals and on each alone without readability, motherhood and childbirth controls are shown in \autoref{table11_FemRatio}. Estimates from \emph{Econometrica} (columns one and four) coincide with those shown in \autoref{table10_FemRatio}. Women take 2--4 months longer in review at \emph{REStud} (columns two and five). When observations from both journals are combined, female-authored papers take, on average, 3--6 months longer in peer review (columns three and six).

\input{/USERS/ERINHENGEL/Dropbox/Readability/draft/0-tex/generated/Table-7-FemRatio}

Remaining coefficients in \autoref{table10_FemRatio} and \autoref{table11_FemRatio} largely correspond to earlier estimates by  \citet{Ellison2002a}. Longer papers take more time to review, as do papers with more co-authors and (generally) those that appear earlier in an issue. Authors with an established publication history, highly cited papers and more readable papers enjoy faster reviews, although the latter effects are only noisily estimated. Papers classified as empirical take longer in review; papers classified as other spend less time under review. Finally, giving birth slows down review, but having a young child appears to have the opposite effect, at least in this particular sample.\footnote{This result is consistent with  \citet{Ginther2004}, who find that women with children are more productive than male and childless female doctoral recipients 10 years after receiving their Ph.D. I would interpret it with caution, however, given (i) counter-intuitive results, (ii) obtaining an unbiased estimate of $\beta_2$ was \emph{not} this study's objective and (iii) $\text{mother}_j$ equals one for only a small number of articles in the sample.}

\aref{appendixquantile} re-estimates column (5) in \autoref{table10_FemRatio} and the third column of \autoref{table11_FemRatio} using a quantile regression model. \aref{appendixmotherhood} replicates \autoref{table10_FemRatio}, column (5) altering the age-threshold on $\text{mother}_j$. The gender gap is positive and significant across the entire distribution; it also does not depend on the precise definition of motherhood.

\section{Mechanisms}
\label{mechanisms}

\subsection{Theoretical framework}
\label{seumodel}

The previous section documents several stylised facts which, combined, suggest that women may spend too much time rewriting old papers and not enough time writing new papers relative to men. In this section, I investigate two potential mechanisms that can help explain why: (i) women voluntarily write better papers---\emph{e.g.}, because they're more sensitive to referee criticism---or (ii) better written papers are women's response to external factors they do not control---\emph{i.e.}, higher standards imposed by referees and\slash or editors.

To help distinguish between (i) and (ii), I develop a simple model of an author's decision making process. It follows an author---denoted by $i$---who publishes several articles in prestigious academic journals over the course of his career. Each article is roughly equivalent in terms of topic, novelty and quality, but may vary on readability. Upon submission to a journal, it is refereed by a review group $s\in\Sigma$, where $\Sigma$ is the (finite) set of all potential review groups and $\mu_i$ are strictly positive probability measures on $\Sigma$.

I assume $s$ accepts $i$'s papers if and only if $R_{it}\ge\widetilde R_i^s$ where $R_{it}$ is the readability of $i$'s $t$th paper and $\widetilde R_i^s$ is the threshold that $s$ applies specifically to $i$.\footnote{See  \citet{Hengel2017} for a version of the model with a two stage refereeing process, where papers are either rejected or offered a revise and resubmit in the first stage and rejected or accepted in the second.} $\widetilde R_i^s$ can depend on other qualities of $i$`s papers---\emph{e.g.}, methodological rigour, data, originality or policy relevance. It may also reflect reviewers' objectives, idiosyncratic preferences and relative weight in determining outcomes. For example, an editor who does not care about readability and is willing to override the opinion of referees will implement a lower $\widetilde R_i^s$ (all else equal). I assume $\Sigma$ and $\mu_i$ are known to $i$ but $\widetilde R_i^s$ is not, although the process of peer review provides enough information---\emph{e.g.}, via referee reports---for $i$ to distinguish each $s\in\Sigma$.

$i$ forms expectations about $\widetilde R_i^s$ by assigning subjective probabilities $\pi_{it}^s(R)$ to all $R$. He then regularly updates $\pi_{it}^s$ using relevant information from his own experience in peer review and by observing others' readability choices and publication outcomes. I assume this process of learning is sufficient to ensure that $\pi_{it}^s(R_{it})$ uniformly converges on some closed interval $\mathcal R$ where $R_{it}\in\mathcal R$.\footnote{Effectively, this assumption rules out systematic mistakes in beliefs that are only corrected at the limit.}

\autoref{eq:EU} defines $i$'s subjective expected utility at time $t$ from writing a paper as readable as $R_{it}$:\input{/USERS/ERINHENGEL/Dropbox/Readability/draft/0-tex/equations/equation10}where $\Pi_{it}^s(R_{it})$ is the probability that $\widetilde R_i^s\le R_{it}$ (\emph{i.e.}, the cumulative sum of $\pi_{it}^s(R)$ for all $R\le R_{it}$), $u_i$ is the utility of having a paper accepted in a prestigious journal,\footnote{Authors probably care about getting their papers accepted and they may care about writing well, but their marginal utility from the intersection of the two events---\emph{i.e.}, higher utility from writing well \emph{only} because the paper is published in a top-four journal (as opposed to a top field journal or second-tier general interest journal)---is assumed to be negligible.} and $\phi_i$ and $c_{i}$ are the satisfaction and cost, respectively, $i$ derives from writing readable papers. $\phi_i$ is increasing and concave in its arguments, $c_{i}$ increasing and convex---marginally higher $R_{it}$ generates proportionally less satisfaction but needs more effort when the paper is already well written.\footnote{See  \citet{Hengel2020} for a version of the model where $c_{it}$ changes over time, \emph{e.g.}, due to learning-by-doing.}

\autoref{eq:EU} incorporates a variety of factors that potentially affect authors' readability choices---editorial standards conditional on other qualities in the paper ($\widetilde R_i^s$); ambition ($u_i$); the cost of drafting and revising manuscripts ($c_{i}$); an otherwise unexplained intrinsic satisfaction from writing readable papers ($\phi_i$). Poor information, overconfidence and sensitivity to criticism are not explicitly included, on the assumption that people do not \emph{want} to be poorly informed, overconfident or excessively sensitive. These factors nevertheless enter \autoref{eq:EU}---and hence influence choices---via the subjective expectations authors form about $\widetilde R_i^s$.

A single $R_{it}$ cannot, therefore, establish if and to what extent $i$'s choices are motivated by (a) intrinsic preferences and costs specific to him ($u_i$, $\phi_i$, $c_{i}$), (b) conditional editorial standards and\slash or referee assignment outside his control ($\widetilde R_i^s$, $\mu_i$) or (c) miscellaneous confounding factors mopped up by $\Pi_{it}^s$. Because $i$'s intrinsic preference and cost functions are assumed to be time independent, however, observing an increase in his choice of readability at two separate $t$ distinguishes (a) from the combined impact of (b) and (c): $i$ may be more sensitive to criticism and he might prefer writing more clearly; nevertheless, he persistently improves the readability of his future papers relative to his past papers only when he believes that doing so boosts the probability that those future papers will be accepted. Moreover, because (c) does not reflect activities or states the author enjoys, its impact on choices declines with better information---\emph{i.e.}, authors may miscalculate referee expectations and misconstrue their reports, but with experience they correct those mistakes. I capture this idea in \autoref{Theorem1}, which is proved in \aref{appendixproofs}.

\input{/USERS/ERINHENGEL/Dropbox/Readability/draft/0-tex/theorems/theorem}

\autoref{Theorem1} identifies three testable conditions that, when satisfied, provide suggestive evidence that either editors assign women ``tougher'' referees---\emph{i.e.}, those with higher $\widetilde R_{i}^s$---or, on average, referees and\slash or editors apply higher standards to women's writing. The first condition states that the readability of $i$'s and $k$'s papers never converges---\emph{i.e.}, past some point, $i$'s papers are always more readable than $k$'s papers by some fixed amount $K'$. The second condition says that $i$'s future papers are always more readable---this time by some fixed amount $K''$---than at least one paper he wrote in the past. In other words, $i$'s readability does not converge to his most poorly written paper. Finally, the third condition states that $i$'s papers are not accepted more often than $k$'s (on average).

The intuition behind these conditions is simple. Experience can serve as a way to complete information, so observing how women's choices change as it increases can help determine which factors are predominantly motivating those choices. More specifically, information imperfections combined with lower confidence, higher risk aversion or a tendency to update too much when faced with negative signals can mean women write more readably than otherwise equivalent men despite their papers being accepted at similar rates. In this case, the gender readability gap is primarily caused by mis-information; it should therefore decline as women's information gets better. Alternatively, women may simply prefer writing more readably than men. In this case, there should be no obvious change in the gender readability gap as women gain a better understanding of peer review. A final possibility is that the gender gap increases with experience. This pattern of behaviour indicates that women revise their beliefs \emph{upwards} about the standards they are being held to as they learn more about those standards. Assuming women make fewer mistakes about referees' and editors' thresholds as they gain experience in peer review, this would suggest that tougher standards play a role in how they make their choices.

Of course, arriving at this conclusion requires making several strong assumptions. First, it is assumed that \autoref{eq:EU} defines an author's optimal readability choice. Second, experience must indeed reduce information imperfections and asymmetries between the sexes. That is, \autoref{Theorem1}`s conclusion only applies if women make fewer mistakes about referees' and editors' standards as they gain more experience in peer review. (Or, at least, this statement become true at some point.) However, if women are unable to obtain the knowledge required to correct mistaken beliefs, then the gaps in women's information relative to men's will not necessarily decline---and could even increase---as women gain experience in peer review. For example, women could systematically mis-perceive a higher threshold, improve readability as a result, get accepted and then have no reason to update their (mistaken) beliefs. Moreover, this could then lead to learning so that future improvements in readability are lower cost for women than they are for men, thereby exacerbating gender differences in readability.

Third---and most critically---$i$'s and $k$'s papers must be identical on every dimension except readability. This assumption applies over $i$ and $k$'s entire lifespan and not just to a single point in time. As a result, it effectively rules out the possibility that $i$ and $k$ specialise over time in different dimensions of quality---\emph{e.g.}, $i$ on readability and $k$ on, say, mathematical rigour---even while the general quality of their work is the same. (See \autoref{duration} for an indirect test of this hypothesis.)

\subsection{Suggestive descriptive evidence}
\label{mechanismsdescriptive}

In this section, I show suggestive descriptive evidence that, on average, female authors satisfy \autoref{Theorem1}'s three conditions relative to men.

\input{/USERS/ERINHENGEL/Dropbox/Readability/draft/0-tex/fixed/Figure-4}

I first consider whether female-authored papers are accepted more often than male-authored papers (Condition 3). The articles in my data have already been published, so I cannot analyse gender differences in acceptance rates. Nevertheless, the topic has been extensively studied elsewhere. A recent study of four comparable journals suggests that exclusively male- and female-authored manuscripts receive a revise and resubmit decision 8 and 6 percent of the time, respectively~\citep{Card2020}.  \citet{Blank1991} found that 12.7 and 10.6 percent of male- and female-authored papers were accepted at the \emph{AER}. A study of \emph{JAMA}'s editorial process indicated that 44.8 percent of referees accept male-authored papers as is or if suitably revised; 29.6 percent summarily reject them. Corresponding figures for female-authored papers were 38.3 and 33.3 percent~\citep{Gilbert1994}. Studies from other disciplines find female-authored papers subjected to single-blind peer review are accepted less often than would be expected by chance~\citep{McGillivray2018,Handley2015}. There appear to be no gender differences in acceptance rates to NBER's Summer Institute ~\citep{Chari2017}. See \aref{appendixacceptance} for a table summarising these and other studies.

As for Conditions 1 and 2, women write more clearly than men (Condition 1) and their future papers are more readable than their past papers (Condition 2). As shown in \autoref{underrepresentation}, female-authored abstracts are 16 percent more readable than those by men. \autoref{figure7} plots an author's Flesch Reading Ease score against $t$, where $t=1$ for his first top-four publication, $t=2$ for his second, \emph{etc.} As $t$ increases, women's readability improves whereas men's does not.\footnote{As shown in \aref{appendixseuempirical}, women's average readability scores are 1--5 percent higher than the readability of their first papers, their latest papers 1--7 percent higher. For a man, however, his average and last papers are more poorly written than his first.}

\autoref{tableH2_FemRatio} presents the marginal effect on female ratio (papers with fewer than 50 percent female authors are classified as male, see \autoref{gender}) for female authors ($\beta_1$) from estimating \autoref{equationX} on subsamples of authors with $t=1$, $t=2$, \emph{etc.}:\input{/USERS/ERINHENGEL/Dropbox/Readability/draft/0-tex/equations/equation11}where $R_{it}$ is the readability score for author $i$'s $t$th top-four publication, gender enters twice, $\text{male}_i$ and $\text{female ratio}_{it}$, to account for $i$'s sex and the sex of his co-authors, $\vect X_{it}$ is a vector of observable controls and $\varepsilon_{it}$ is the error term.

\input{/USERS/ERINHENGEL/Dropbox/Readability/draft/0-tex/generated/Table-8-FemRatio}

All figures in \autoref{tableH2_FemRatio} agree---women write better---but the magnitude and significance of that difference increases as $t$ increases. Between $t=1$ and $t=2$, the gap marginally widens but is not significant. After that, it triples (at least); the increase is significant ($p<0.05$) for all five scores (\aref{appendixequality}). At higher publication counts, figures are less precisely estimated and smaller than in column 3, but still noticeably larger than estimates in columns 1 and 2.\footnote{Only 40 female authors have 4--5 publications in the data; just 28 have six or more.}

\subsection{Quantifying the counterfactual}
\label{quantification}

Evidence in the previous section suggests women satisfy \autoref{Theorem1}'s three conditions relative to men, on average. However, included controls undoubtedly fail to fully account for differences in the non-readability aspects of men's and women's papers (Assumption 2). Furthermore, concluding that higher standards are present actually requires that all three of \autoref{Theorem1}'s conditions hold for the same author---that is, the same woman must write better than an equivalent man, not be accepted at rates higher than he is \emph{and} raise the quality of her writing over time.

In this section, I attempt to improve my estimates in both respects by restricting the sample to authors with three or more top-four publications. I then match observably similar male and female economists based on characteristics that predict the topic, novelty and quality of their research. Within-person readability comparisons are used to determine if Condition 2 was satisfied for each author in a matched pair. Between-person comparisons after authors gain experience in peer review are used to establish whether Condition 1 was satisfied for the male or female member. I then use these results to construct a counterfactual estimate of the impact higher standards have played in women's readability choices.

\subsubsection{Theoretical strategy}
\label{matchingtheory}

To understand the counterfactual analysis, note that $i$'s optimal choice of readability (defined in \autoref{eq:EU}) can be formulated as a binary decision problem. Specifically, $i$ initially faces the following choice: he can either ignore the impact readability has on his acceptance rate entirely and simply set $R_{it}$ so that marginal cost equals the marginal benefit of the intrinsic satisfaction he derives from writing readable papers (\emph{i.e.}, $R_{it}=R_i^\star$ where $R_i^\star$ solves $c_i'(R)=\phi_i'(R)$); alternatively, he can chase after higher acceptance rates and set $R_{it}>R_i^\star$. If the latter option is chosen, then $i$ optimally minimises $R$ conditional on acceptance rate---\emph{i.e.}, he sets $R_{it}$ just equal to the readability threshold of the last review group he believes will accept his paper. Thus, $R_{it}=\widetilde R_i^{\bar s} + e_{it}$, where $e_{it}$ is the error in $i$'s time $t$ beliefs about $\widetilde R_i^{\bar s}$ and $\bar s$ is the toughest review group to accept $i$'s papers.

Suppose $i$ satisfies the assumptions and conditions of \autoref{Theorem1} relative to $k$ and assume that at time $t'$ $i$ and $k$ are sufficiently experienced in peer review to ensure that $e_{it}$ and $e_{kt}$: (i) are on a path converging to zero (\emph{i.e.}, both getting closer and closer to zero each time $i$ and $k$ go through another round of peer review); and (ii) have already converged to one another (\emph{i.e.}, $e_{it}=e_{kt}$). When these assumptions hold, \autoref{Corollary1} suggests a conservative measure of the impact external factors have on $i$'s time $t'$ readability choice. It is proved in \aref{appendixproofs}.

\input{/USERS/ERINHENGEL/Dropbox/Readability/draft/0-tex/theorems/corollary}

$D_{ik}$ represents a lower bound on the difference between $i$'s time $t'$ optimal readability score given he's held to higher standards than $k$ (\emph{i.e.}, $R_{it'}$) and the readability score he would have chosen had be been subject to the same standards as $k$ (\emph{i.e.}, $R\le\max\{R_{kt'},R_{it''}\}$). The intuition behind it is simple. First note that it is never optimal for $i$ to choose an $R$ less than $R_i^\star$. Since $i$ already chose $R_{it''}<R_{it'}$ (\autoref{Theorem1}'s Condition 2), that must mean that $R_i^\star\le R_{it''}$. For all $R>R_i^\star$, $i$ prefers to minimise $R$ conditional on acceptance rate, thus, $i$ prefers any $R\in[R_{it''},R_{it'})$ to $R_{it'}$ if it achieves the same acceptance rate as $R_{it'}$. Furthermore, from Condition 3, we know that $i$'s acceptance rate at $R_{it'}$ is identical to $k$'s acceptance rate at $R_{kt'}$. Suppose $R_{kt'}<R_{it''}$. If $i$ and $k$ were subject to identical standards, then $i$'s acceptance rate a $R_{it'}$ would be the same as his acceptance rate at $R_{kt'}$ and therefore also the same as his acceptance rate at $R_{it''}$. Because it may be that $R_{kt'}<R_i^\star\le R_{it''}$, $i$ does not necessarily prefer $R_{kt'}$ to $R_{it''}$, conditional on acceptance rate. We can, however, conclude that $i$ would, at the very least, prefer $R_{it''}$ to $R_{it'}$. Suppose $R_{it''}\le R_{kt'}$. As before, $i$'s acceptance rate at $R_{it'}$ is the same as $k$'s acceptance rate at $R_{kt'}$, but it is no longer necessarily the case that $i$'s acceptance rate at $R_{it''}$ is the same as $k$'s acceptance rate at $R_{kt'}$. Thus, if both were subject to the same standards, then $i$ would, at the very least, prefer $R_{kt'}$ to $R_{it'}$.

\subsubsection{Estimation strategy}
\label{matchingestimation}

Implementing \autoref{Corollary1} first requires that measurement occurs at time $t'$---\emph{i.e.}, a point at which authors are sufficiently experienced for Assumptions 4 and 5 to hold. I assume this point occurs at or before authors' third top-four paper. Authors with one or two top-four publications are probably tenured and well-established in their fields. By publication three, all frequently referee (and some edit) prestigious economics journals. I assume this accumulated experience means: (a) equivalent authors are equally accurate about the standards they are being held to and remaining errors are no longer gender specific; and (b) those errors are getting smaller and smaller each time authors go through another round of peer review.

Additionally, in order to estimate $D_{ik}$, \autoref{Theorem1}'s Assumptions 1--3 must also hold. Most critically, Assumption 2 requires that $i$'s and $k$'s papers are identical with respect to topic, novelty and overall quality. I attempt to satisfy this assumption by matching every female author with three or more top-four publications to her closest male counterpart. Matches were made using a Mahalanobis procedure with the following co-variates: (1) maximum citation count over $t$; (2) institutional rank at $t=1$; (3) fraction of papers published per decade; (4) fraction of papers published by each journal; and (5) number of articles per primary \emph{JEL} category.\footnote{Two notes on co-variate choice. First, I eschew mean, median and minimum citation counts in favour of the maximum on the assumption that an author's ``quality'' is principally a function of his best paper. Second, most people are at top ranked institutions by $t=3$; by matching on institutions at $t=1$, I try to pair authors with similar career paths.} Co-variate balance pre- and post-match are shown in \aref{appendixmatchingbalance}. \aref{appendixmatchingnames} lists each matched pair.

Assume authors are indeed well-matched and also sufficiently experienced at $t=3$. Then under ideal circumstances, comparing $R_{i3}$ to $R_{i1}$ determines the impact information (as proxied for by experience) has on readability, conditional on gender (Condition 2); comparing $R_{i3}$ to $R_{k3}$ determines the impact of gender, conditional on information (Condition 1). Because of co-authoring, however, circumstances are not ideal. In particular, co-authoring means that article gender is neither fixed over $t$ conditional on $i$, nor is $i$'s and $k$'s experience---and hence information---necessarily identical at time $t=3$. I attempt to account for this by predicting $i$'s $t$th paper readability had it only been co-authored with members of $i$'s same sex. To do so, I reconstruct $i$'s time $t$ readability choice at female ratio equal to 1 for women and 0 for men using errors and coefficients from OLS estimation of \autoref{equation14} in the gender and time appropriate subsample of authors:\footnote{More specifically, I separately estimate \autoref{equation14} in the following four subsamples: (i) female authors at $t=1$; (ii) male authors at $t=1$; (iii) female authors at $t=3$; (iv) male authors at $t=3$. I then generate $\widehat R_{it}$ using the appropriate coefficients and errors for each author: (i) $\widehat R_{i1}=\alpha_{1f}+\beta_{1f}+\varepsilon_{i1}$ for a female $i$ at $t=1$; (ii) $\widehat R_{i1}=\alpha_{1m}+\varepsilon_{i1}$ for a male $i$ at $t=1$; \emph{etc.}}\input{/USERS/ERINHENGEL/Dropbox/Readability/draft/0-tex/equations/equation13}where $g_i=m,f$ if $i$ is male or female, respectively, ``female ratio'' defines papers with a strict minority of female authors as male-authored; for papers with 50 percent or more female authors, it is the ratio of female authors on a paper (see \autoref{gender} for more details) and $\varepsilon_{it}$ is the estimated error term. As long as $\varepsilon_{it}$ does not partially correlate with a paper's ratio of female authors conditional on $t$ and $g_i$, then $\widehat R_{it}$ provides an unbiased prediction of $R_{it}$. Regression output from \autoref{equation14} is shown in \aref{appendixreconstruction}. To adjust for the degrees of freedom lost when generating $\widehat R_{it}$, standard errors in subsequent calculations are inflated by 1.05. \aref{appendixseur} presents results using the unadjusted, observed $R_{it}$ instead of $\widehat R_{it}$.

\subsubsection{Results}
\label{matchingresults}

\input{/USERS/ERINHENGEL/Dropbox/Readability/draft/0-tex/generated/Table-9-base}

\autoref{table8_base}'s first and second panels display means and standard deviations of $D_{ik}$ (\autoref{eq:correq1}) for matched pairs where one member satisfies both Conditions 1 and 2 relative to the other member. In the first panel, the female member does; in the second, it's the male member. Male scores are subtracted from female scores, so $D_{ik}$ is, by definition, positive in panel one and negative in panel two.

Results in \autoref{table8_base} suggest that Conditions 1 and 2 were satisfied for the same author in 65 percent of matched pairs. In two-thirds of those, the member who satisfied them was female. Moreover, $D_{ik}$'s magnitude is (on average) 1.5 times larger in matched pairs where the female member satisfied Conditions 1 and 2 compared to pairs in which the male member did. \autoref{figure8_base} emphasises this point. It displays the distribution of $D_{ik}$: when the evidence suggests a man is subject to higher standards, $D_{ik}$ clusters close to zero; when it suggests the woman is, $D_{ik}$ is far more spread out.

\autoref{table8_base}'s final panel averages $D_{ik}$ over all observations, setting $D_{ik}=0$ in matched pairs where neither member satisfied Conditions 1 and 2. Mean $D_{ik}$ is consistently positive and significant indicating that higher standards are, on average, directed at women: women write about 3 points more readably on the Flesch Reading Ease scale; according to the four grade-level scores, their writing takes around 4--9 fewer months of schooling to understand than it would if they weren't subject to higher standards. Percentage-wise, these results suggest that higher standards mean women write, on average, 5 percent more readably than they otherwise would.

I emphasis, however, that these conclusions are predicated on several strong assumptions; if any are violated, then higher standards against women cannot be inferred from \autoref{table8_base} and \autoref{figure8_base}. First, and most critically, $i$ and $k$ must be identical with respect to topic, novelty and overall quality. Second, at time $t=3$ authors must have learned enough about the process of peer review for Assumptions 4 and 5 in \autoref{Corollary1} to hold. Finally, \autoref{equation14} must accurately predict the readability of $i$'s and $k$'s papers as if they had been co-authored by members of their same sex. Please see \aref{appendixmatchinglimitations} where I discuss the validity and robustness of these assumptions in more detail.

\input{/USERS/ERINHENGEL/Dropbox/Readability/draft/0-tex/fixed/Figure-5-base}

\subsection{Understanding how women respond to higher standards}
\label{indirecteffect}

Women can respond to higher standards in two different ways: immediately (direct effect) and pre-emptively (feedback effect). As emphasised in \autoref{seumodel}, the weight of each effect likely depends on authors' information about---hence experience with---the peer review process. In this section, I illustrate the evolution of the relative importance of each by comparing papers pre- and post-review as authors' publication counts rise. To do so, I estimate the following equation:\input{/USERS/ERINHENGEL/Dropbox/Readability/draft/0-tex/equations/equation14}where $m=W,P$ for working papers and published articles, respectively, ``female ratio'' defines papers with a strict minority of female authors as male-authored; for papers with 50 percent or more female authors, it is the ratio of female authors on a paper (see \autoref{gender} for more details), $t_{it}$ is author $i$'s number of top-four papers at time $t$, $\vect X_{it}$ is a vector of observable controls and $\varepsilon_{it}$ is the error term. 

Results from estimating \autoref{equation15} are shown in \autoref{figure9} and \autoref{table9}. In \autoref{figure9}, hollow circles denote draft readability; solid diamonds reflect readability in the final, published versions of those same papers. Dashed lines trace readability as papers undergo peer review (direct effect) and correspond to estimates in the first panel of \autoref{table9}. \autoref{table9}'s second panel shows the marginal effect of female ratio (papers with fewer than 50 percent female authors are classified as male) for each version of a manuscript over increasing $t$. Estimates for published articles correspond to the difference between pink and blue diamonds in \autoref{figure9}; estimates for draft papers represent feedback effects and correspond to differences between hollow circles. Difference-in-differences are shown in the final panel of \autoref{table9}.

\input{/USERS/ERINHENGEL/Dropbox/Readability/draft/0-tex/fixed/Figure-6}

\autoref{figure9} and \autoref{table9} suggest that the gender readability gap in published articles is statistically significant and relatively stable at almost every $t$. At $t=1$, it is formed almost entirely during peer review; by $t\ge4$ is is largely formed prior to submission. That is, gender differences in the direct effect of peer review start off large, positive and significant but as $t$ increases, they gradually go away. For the feedback effect, however, the pattern is reversed. Differences in draft readability do not appear to contribute to the gender gap at $t=1$, but this gap rises as $t$ increases.

\input{/USERS/ERINHENGEL/Dropbox/Readability/draft/0-tex/generated/Table-10}

\subsubsection{Interpretation}
\label{indirecteffectinterpretation}

A number of \emph{tentative} conclusions about the gender readability gap can be made from \autoref{figure9} and \autoref{table9}. First, inexperienced men and women seem to make similar choices in draft readability. This suggests similar initial preferences for and beliefs about the impact of writing well. In one important sense, however, men are still better informed: the standards they believe apply actually do; junior women appear to mistakenly assume similar standards apply to them, too.

Second, experienced men \emph{and} women seem to sacrifice time upfront in order to improve their odds in peer review. By anticipating referees' demands, authors can partially insure themselves against rejection and\slash or excessively long review. The price is having to spend more time revising a manuscript before submitting it. Assuming choices by senior economists express optimal trade-offs with full information, \autoref{figure9} implies little---if any---gender differences in these preferences for insurance. Again, however, higher standards will mean that the price of that insurance is greater for women than it is for men.

Finally, \autoref{figure9} suggests the direct effect of peer review dominates when women have less experience; the feedback effect dominates when they have more experience. This pattern of behaviour implies that women initially underestimate referees' thresholds but learn about them over time and adapt by writing their future papers more readably prior to submission. This last observation suggests inexperienced female economists go through the toughest review, conditional on acceptance. To investigate further, I test the impact of experience on time spent in review by re-estimating \autoref{equation16} on sub-samples of junior and senior authors. Results are displayed in \aref{appendixtimeexp}. They suggest papers by junior women do indeed take longer in review; the gender gap is significantly smaller---albeit still positive---for senior women.

\section{Conclusion}
\label{conclusion}

Most raw numerical counts suggest women produce less than men: female real estate agents list fewer homes~\citep{Seagraves2013}; female lawyers bill fewer hours~\citep{Azmat2017}; female physicians see fewer patients~\citep{Bloor2008}; female academics write fewer papers~\citep{Ceci2014}. When evaluated by narrowly defined quality measures, however, women often outperform: houses listed by female real estate agents sell for higher prices~\citep{Salter2012,Seagraves2013}; female lawyers make fewer ethical violations~\citep{Hatamyar2004}; patients treated by female physicians are less likely to die or be readmitted to hospital~\citep{Tsugawa2016}.

As I argue in this paper, female economists surpass men on another dimension: writing clarity. Abstracts written by women are 1--6 percent more readable than similar abstracts by men. They also become 2--5 percent more readable while under review when referees aren't blinded to authors' identities. Furthermore, the cost to women of revising their papers appears to be much higher than the cost to men: female-authored papers spend 3--6 months longer under review compared to observably equivalent male-authored papers. Finally, it does not appear that women are rewarded for their better writing: recent evidence from a set of comparable journals suggests female-authored papers are accepted at lower rates, conditional on quality~\citep{Card2020}.

To interpret these stylised facts, I model an author's decision-making process as if it were governed by the rational behaviour of women who update their beliefs about the readability thresholds they are held to as they gain experience in peer review. I then derive three testable conditions which can help establish whether higher standards are at all important to the existence and evolution of the gender readability gap: (1) experienced women write better than equivalent men; (2) women improve their writing over time; and (3) female-authored papers are accepted no more often than equivalent male-authored papers.

On average, I find that all three conditions hold: women's writing gradually gets better but men's does not; between authors' first and third published articles, the readability gap increases by up to 12 percent; as already mentioned, female-authored papers are not accepted at higher rates after conditioning on similar co-variates~\citep{Card2020}. I then conduct a counterfactual analysis that exploits within- and between-individual variation in readability among well-published economists. Its results suggest that higher standards lead women to write, on average, 5 percent more readably than they otherwise would.

I emphasis, however, that these conclusions are predicated on several strong assumptions; if any are violated then other hypotheses are also consistent with the data. For example, if matching does not fully account for differences in the non-readability aspects of men's and women's papers, then gender differences in readability may be influenced by gender differences in specialisation over time. Similarly, no control perfectly captures how empirical vs. theoretical a paper is; as a result, the gender gaps I observe may be biased by differences between fields. Additionally, the validity of the counterfactual analysis and the precision of its estimates rely on strong assumptions about men's and women's beliefs and the impact co-authors have on an article's readability.

Higher standards, wherever present, reduce women's labour market opportunities. Work that is evaluated more critically at any point in the production process will be systematically better (holding prices fixed) or systematically cheaper (holding quality fixed). This will reduce women's wages, distort measurements of their productivity and negatively impact their labour market outcomes. For example, if judges require better writing in female-authored briefs, female attorneys must charge lower fees and\slash or under-report hours to compete with men; billable hours and client revenue will decline, making female lawyers appear less productive than they truly are. In academia, higher standards coupled with longer peer review times likely affect women's probability of obtaining tenure.

Unfortunately, there are no easy solutions for addressing higher standards. But least intrusive---and arguably most effective---is simple awareness and constant supervision. I hope journals are challenged to address the tougher standards they likely impose on women, open to policies that transparently monitor them and supportive of research that helps us better understand them.


% Bibliography.
\ifappendixlast
    \begin{SingleSpace}
        \printbibliography[heading=subbibliography]
    \end{SingleSpace}

\clearpage
\begin{appendices}
\begin{refsection}


\section{Proofs}
\label{appendixproofs}


\input{/USERS/ERINHENGEL/Dropbox/Readability/draft/0-tex/theorems/proofs}
\clearpage


\section{Readability data coverage}
\label{appendixarticlecount}

\autoref{table1} displays readability data coverage by journal and decade.


\input{/USERS/ERINHENGEL/Dropbox/Readability/draft/0-tex/generated/Table-B.1}
\clearpage


\section{Description of control variables}
\label{appendixcontrols}

\paragraph{Institutions}
\label{appendixcontrolsinstitutions}

For every article I recorded authors' institutional affiliations. Individual universities in U.S. State University Systems were coded separately (\emph{e.g.}, UCLA and UC Berkeley) but think tanks and research organisations operating under the umbrella of a single university were grouped together with that university (\emph{e.g.}, the Cowles Foundation and Yale University). Institutions linked to multiple universities are coded as separate entities (\emph{e.g.}, cole des hautes tudes en sciences sociales).

In total, 1,039 different institutions were identified. For each institution, I count the number of articles in which it was listed as an affiliation in a given year and smooth the average over a five-year period. Institutions are ranked on an annual basis using this figure and then grouped to create fifteen dynamic dummy variables. Institutions ranked in positions 1--9 are assigned individual dummy variables. Those in positions 10--59 are grouped in bins of 10 to form six dummy variables. Institutions ranked 60 or above were collectively grouped to form a final dummy variable. When multiple institutions are associated with an observation, only the dummy variable with the highest rank is used, \emph{i.e.}, the highest-ranked institution per author when data is analysed at the author-level and the highest-ranked institution for all authors when data is analysed at the article-level.

\paragraph{Citations}
\label{appendixcontrolscitations}

I use article citations from \href{https://login.webofknowledge.com/error/Error?Error=IPError&PathInfo=%2F&RouterURL=https%3A%2F%2Fwww.webofknowledge.com%2F&Domain=.webofknowledge.com&Src=IP&Alias=WOK5}{Web of Science}. Unless otherwise mentioned, citation counts are transformed using the inverse hyperbolic sine function (asinh).

\paragraph{Author prominence}
\label{appendixcontrolsprominence}

I generate 37 dummy variables that group authors by their career-total top-five journal (\emph{AER}, \emph{Econometrica}, \emph{JPE}, \emph{QJE} and \emph{REStud}) publications as of December 2015. For example, Jean Tirole forms one group (59 articles); James Heckman and Gene Grossman form another (34 articles).

\paragraph{Author seniority}
\label{appendixcontrolsseniority}

To account for author seniority, I control for an author's number of top-five (\emph{AER}, \emph{Econometrica}, \emph{JPE}, \emph{QJE} and \emph{REStud}) publications at the time a paper was published. For co-authored articles, only the data corresponding to the most prolific author is used.

\paragraph{English fluency}
\label{appendixcontrolsenglish}

To account for English fluency, most regressions include a dummy variable equal to one if an article is co-authored by at least one native (or almost native) English speaker. I assume an author is ``native'' if he: (i) was raised in an English-speaking country; (ii) obtained all post-secondary eduction from English speaking institutions;\footnote{Non-native speakers who meet this criteria have been continuously exposed to spoken and written English since age 18. This continuous exposure likely means they write as well as native English speakers. To qualify as an English-speaking institution, all courses---not just the course studied by an author---must be primarily taught in English. \emph{E.g.}, McGill University is classified as English-speaking; University of Bonn is not (although most of its graduate economics instruction is in English).} or (iii) spoke with no discernible (to me) non-native accent. This information was almost always found---by me or a research assistant---in authors' CVs, websites, Wikipedia articles, faculty bios or obituaries. In the few instances where the criteria were ambiguously satisfied---or no information was available---I asked friends and colleagues of the author or inferred English fluency from the author's first name, country of residence or surname (in that order).

\paragraph{Field}
\label{appendixcontrolsfield}

I create dummy variables corresponding to the 20 primary and over 700 tertiary \emph{JEL} categories to control for subject matter. The \emph{JEL} system was significantly revised in 1990; because exact mapping from one system to another is not possible, I collected these data only for articles published post-reform---about 60 percent of the dataset. Codes were recorded whenever found in the text of an article or on the websites where bibliographic information was scraped. Remaining articles were classified using codes from the American Economic Association's Econlit database.

I additionally categorised each tertiary \emph{JEL} code as either theory\slash methodology, empirical or other. For example, C02 (mathematical methods) and D85 (network formation and analysis: theory) are classified as theory\slash methodology, whereas D12 (consumer economics: empirical analysis) and F14 (empirical studies of trade) are classified as empirical. Tertiary codes that are not distinctly related to empirical or theory\slash methodology---\emph{e.g.}, L29 (firm objectives, organisation and behaviour: general) or O10 (economic development: general)---are classified as ``other''. (Papers published before 1990 are not classified in any category.) \autoref{jel_list} lists the \emph{JEL} codes assigned to each category.

In total, I classified 99 tertiary \emph{JEL} codes as theory\slash methodology, four as empirical and the remaining 756 as ``other''. (Given the small number of distinctly empirical codes, most ``other'' papers are likely empirical papers.) When combined with my 1990--2015 dataset, there are 1,764 theory\slash methodology articles (34 percent), 412 empirical articles (8 percent) and 4,608 ``other'' articles (88 percent). (Given articles can be both theory and empirical, these percentages do not sum to 100 percent.)

\input{/USERS/ERINHENGEL/Dropbox/Readability/draft/0-tex/generated/Table-C.1}

\paragraph{Editorial policy}
\label{appendixcontrolseditorial}

To control for editorial policy, I recorded editor\slash editorial board member names from issue mastheads. \emph{AER} and \emph{Econometrica} employ an individual to oversee policy. \emph{JPE} and \emph{QJE} do not generally name one lead editor and instead rely on boards composed of four to five faculty members at the University of Chicago and Harvard, respectively. \emph{REStud} is also headed by an editorial board, the size of which has been gradually increasing---from two members in the 1970s to 7--8 members today. Members are also located all over the world.

Editor controls are based on distinct lead editor\slash editorial boards---\emph{i.e.}, they differ by at least one member. Among top four journals, 74 groups are formed in this manner. \emph{REStud} adds another 34. Given the size of \emph{Restud}'s editorial board and the fact that members serve fixed 3--4 full-year terms, editorial controls are highly correlated with year fixed effects. Moreover, unlike at \emph{JPE} and \emph{QJE}, editors are not located at the same institution. Thus, editor fixed effects may be less informative about editorial policy at \emph{REStud} than they are for the other four journals.

\paragraph{Family commitments}
\label{appendixcontrolsfamily}

To control for motherhood's impact on revision times, I recorded children's birth years for women with at least one entirely female-authored paper in \emph{Econometrica}. I personally (and, I apologise, rather unsettlingly) gleaned this information from published profiles, CVs, acknowledgements, Wikipedia, personal websites, Facebook pages, background checks and local school district\slash popular extra-curricular activity websites. Exact years were recorded whenever found; otherwise, they were approximated by subtracting a child's actual or estimated age from the date the source material was posted online. In several instances, I obtained this information from acquaintances, friends and colleagues or by asking the woman directly. If an exhaustive search turned up no reference to children, I assumed the woman in question did not have any.

\clearpage

\section{Readability scores}
\label{appendixreadability}

\subsection{Validity}
\label{appendixvalidity}

Advanced vocabulary and complicated sentences are the two strongest predictors of text difficulty~\citep{Chall1995}. Hundreds of readability formulas exploit this relationship. The five most widely used, tested and reliable formulas for adult reading material are the Flesch Reading Ease, Flesch-Kincaid, Gunning Fog, SMOG (Simple Measure of Gobbledegook) and Dale-Chall~\citep{DuBay2004}. Each are listed in \autoref{tab:formulas}.

These five readability scores generally produce similar rankings: the yellow box plot in \autoref{figure2} summarises 169 inter-score correlations found in 26 studies; the median is 0.87. Moreover, they tend to correlate with (i) oral reading fluency, (ii) human judgement, (iii) reading comprehension tests and (iv) the cloze procedure.\footnote{Oral reading fluency is generally measured as the number of words read aloud correctly per minute. The cloze procedure ranks passages of text according to average readers' ability to correctly guess randomly deleted words.} The dark blue box plots in \autoref{figure2} summarise 167 correlations in 38 published cross-validation studies. (See \aref{appendixmetaanalysis} for a list of studies included in the analysis.)

Furthermore, numerous studies have validated readability scores against surrogate measures of reading comprehension. More readable high school and college-level correspondence courses have higher completion rates~\citep{Klare1973}. More readable academic journals enjoy larger readerships~\citep{Richardson1977,Swanson1948}; their most readable articles win more awards~\citep{Sawyer2008} and are downloaded more often~\citep{Guerini2012}. More readable abstracts are also (generally) cited more frequently (see  \citet{Dowling2018,McCannon2019} and \autoref{figure2}). They are also more likely to be published in top-five and other higher ranking journals~\citep{MarinoFages2020}. In a \href{http://lukaspuettmann.com/2017/12/09/voxeu-gobbledygook/}{blog post}, Lukas Pttmann compares abstract readability to page views of \href{http://www.voxeu.org}{VoxEU.org} columns: more readable columns are viewed three percent more often~\citep{Puttmann2017}. Evidence from other studies linking readability and citations is, however, weaker~\citep{Lei2016, Berninger2017,Laband1992}. My own data suggest a positive relationship in papers published after 1990---and particularly those published post--2000---but no relationship before that (\autoref{figure2}).

Thanks to high predictive power and ease of use, readability formulas are widely employed in education, business and government. The U.S. Securities and Exchange Commission encourages clearer financial disclosure forms benchmarked against the Gunning Fog, Flesch-Kincaid and Flesch Reading Ease scores~\citep{Cox2007}. The formulas have also guided readability assessments of, \emph{inter alia}, standardised test questions~\citep{Chall1977,Chall1983}, medical inserts~\citep[\emph{e.g.},][]{Wallace2008}, technical manuals~\citep[\emph{e.g.},][]{Hussin2012,Klare1973}, health pamphlets~\citep[\emph{e.g.},][]{Foster2002,Meade1989} and data security policies~\citep{Alkhurayyif2017}.

In research, readability scores are often used to proxy for ``complexity''.  \citet{Enke2018} controls for language sophistication using the Flesch Reading Ease formula in a study of moral values in U.S. presidential elections.  \citet{Spirling2016} employs the same score to show that British parliamentarians simplified speeches to appeal to less educated voters in the wake of the Great Reform Act. Legal research has found that judges are more reliant on legislative history when interpreting complex legal statutes, as measured by the Flesch-Kincaid formula~\citep{Law2010}. In finance, the scores have linked clarity of financial communication to better firm and market financial health~\citep{Li2008,Biddle2009,Jansen2011}, larger investment and trading volume ~\citep{Miller2010,Thrnqvist2015,DeFranco2015,Lawrence2013} and lower demand for---albeit higher reliability of---outside research by sell-side analysts~\citep{Lehavy2011}. See also  \citet{Loughran2016} for a review of the use of readability scores in finance and accounting research.

\input{/USERS/ERINHENGEL/Dropbox/Readability/draft/0-tex/fixed/Figure-D.1}

\subsection{Measurement error}
\label{appendixmeasurementerror}

Readability scores fail to capture many elements relevant to reading comprehension, including gram\-mar---\emph{e.g.}, active vs. passive tense~\citep{Coleman1964,Coleman1965}---legibility---\emph{e.g.}, typeface or layout---and content---\emph{e.g.}, coherence, organisation and general appeal~\citep{Kintsch1984,Kemper1983,Meyer1982,Armbruster1984}. Nevertheless, ``long sentences generally correspond to complex syntactic structures, infrequent words generally refer to complex concepts, and hard texts will generally lead to harder questions about their content''~\citep[][p. 222]{Kintsch1984}.

Still, readability scores' low causal power raises legitimate concerns about measurement error. As long as this error does not partially correlate with the variable of interest (gender), the analytical results I present in this paper attenuate toward zero (classical measurement error). Unfortunately, they are systematically biased in an unknown direction if it does (non-classical measurement error).

Sources of non-classical measurement error are threefold: (a) grammatical, spelling and transcription errors in the textual input; (b) errors in the estimates of vocabulary complexity and sentence length introduced by automating their calculation; or (c) embodied in the jump from using these two variables to infer readability.

Conditional on accurate calculation, readability scores combine very precise estimates of vocabulary complexity with almost perfect measures of sentence length~\citep[for a discussion, see][]{Chall1995}. The weighted average of these two variables is informative in much the same way that inferences about readability are. Thus, measurement error related to (c) should only shift superficial interpretation of observed gender differences---from ``women are better writers'' to ``women use simpler words and write shorter sentences''---but leave conclusions deduced from them intact.

Nevertheless, I try to minimise measurement error from (c) by using abstracts as textual input. Abstracts are self-contained, universally summarise the research and are the first and most frequently read part of an article~\citep{King2006}. Additionally, they follow a more standardised layout compared to other parts of a manuscript: they are generally surrounded by ample whitespace and most editorial management systems anyway reproduce them in pre-formatted cover pages. These factors suggest a relatively homogenous degree of review across journals and subject matter and limit the impact that physical layout, figures and surrounding text have on readability.

Moreover, prior research suggests authors write in a stylistically consistent manner across the abstract, introduction and discussion sections of a paper. According to an analysis of published education and psychology articles, within-manuscript correlations of Flesch Reading Ease scores range from 0.64 (abstracts vs. introductions) to 0.74 (abstracts vs. discussions)~\citep{Hartley2003b}.  \citet{Plaven-Sigray2017} also found a strong positive correlation using full text articles from several scientific journals. \autoref{figure3} plots abstract readability against the readability of a passage from the introduction for 339 NBER Working Papers eventually published in a top-four journal. It suggests a similarly positive relationship holds in economics, as well.\footnote{For comparison, I randomly assigned abstracts to introductions in 1,000 simulated samples. The average coefficient of correlation between abstract text readability and the readability of a passage of text from a randomly selected introduction was --0.0006 for the Gunning Fog score and 0.0007 for the Flesch-Kincaid score.}

\input{/USERS/ERINHENGEL/Dropbox/Readability/draft/0-tex/fixed/Figure-D.2}

In my opinion, non-classical measurement error from (a) and (b) poses a bigger concern to the identification mapped out in this paper. I have taken several steps to reduce it. First, abstract text is also ideal for calculating readability: 100--200 words containing few score-distorting features of academic writing---\emph{e.g.}, citations, abbreviations and equations~\citep{Dale1948}. Additionally, most abstracts have been previously converted to accurate machine-readable text by digital libraries and bibliographic databases, curbing errors in transcription.

Second, I carefully proofread the text in order to identify (and fix) remaining transcription errors,\footnote{\emph{E.g.}, words in transcribed text are often inappropriately hyphenated---typically because the word was divided at the end of the line in the original text.} eliminate non-sentence-ending full stops, and replace typesetting code---typically used to render equations---with equivalent unicode characters.\footnote{When no exact replacement existed, characters were chosen that mimicked as much as possible the equation's original intent while maintaining the same character and word counts. (Equations in abstracts generally only occur in \emph{Econometrica} articles published before 1980.)} Readability scores were determined using the modified text.

Finally, some programs that calculate scores rely on unclear, inconsistent and possibly inaccurate algorithms to count words and syllables, identify sentence terminations and check whether a word is on Dale-Chall's easy word list~\citep[for a discussion, see][]{Sirico2007}. To transparently handle these issues and eliminate ambiguity in how the scores were calculated, I wrote the Python module \texttt{Textatistic}. Its code and documentation are available on \href{https://github.com/erinhengel/Textatistic}{GitHub}; a brief description is provided in \aref{appendixtextatistic}.

For added robustness, I also re-calculate scores and replicate most results using the \href{https://github.com/trinker/readability}{\texttt{R} \texttt{readability} package} (\aref{appendixalternativereadability}). Coefficients are very similar to---and (to my chagrin) standard errors universally smaller than---those presented in the body of the paper.

\subsection{\texttt{Textatistic}}
\label{appendixtextatistic}

I wrote the Python module \texttt{Textatistic} to transparently calculate the readability scores in this study. The code and documentation are available on \href{https://github.com/erinhengel/Textatistic}{GitHub}; I provide a brief description here.

To determine sentence count, the program replaces common abbreviations with their full text,\footnote{Abbreviations which do not include full-stops are not altered. I manually replaced common abbreviations, such as ``\emph{i.e.}'' and ``U.S.'' with their abbreviated versions, sans full stops.} decimals with a zero and deletes question and exclamation marks used in an obvious, mid-sentence rhetorical manner.\footnote{For example, ``?).'' is replaced with ``).''.} The remaining full stops, exclamation and question marks are assumed to end a sentence and counted.

Next, hyphens are deleted from commonly hyphenated single words such as ``co-author'' and the rest are replaced with spaces, remaining punctuation is removed and words are split into an array based on whitespace. Word count is the length of that array.\footnote{Per \citet{Chall1995}, hyphenated words count as two (or more) words.}

An attempt is made to match each word to one on an expanded Dale-Chall list. The count of difficult words is the number that are not found. This expanded list, available on \href{https://github.com/erinhengel/Textatistic}{GitHub}, consists of 8,490 words. It is based on the original 3,000 words, but also includes verb tenses, comparative and superlative adjective forms, plural nouns, \emph{etc.} It was created by first adding to the Dale-Chall list every conceivable alternate form of each word using Python's Pattern library. To eliminate nonsense words, the text of 94 English novels published online with Project Gutenberg were matched with words on the expanded list. Words not found in any of the novels were deleted.

Syllable counts are based on the C library \texttt{libhyphen}, an implementation of the hyphenation algorithm from \citet{Liang1983}.  \citet{Liang1983}'s algorithm is used by \TeX's typesetting system. \texttt{libhyphen} is employed by most open source text processing software, including OpenOffice.

\subsection{Studies included in meta analysis}
\label{appendixmetaanalysis}

Below are the studies included in the analysis from \autoref{figure2}, which summarises correlations between readability scores and alternative measures of reading comprehension found in other research. A few notes on the criteria for inclusion and how some correlations were determined:

\begin{itemize}
\item I include only documents produced for the U.S. government or published peer reviewed studies---with the exception of the present paper,  \citet{Benoit2017} and results from dissertations that were presented and discussed in a peer reviewed manuscript.

\item I include a small number of studies with correlations between alternative readability measures and the number of words not listed on the Dale-Chall word list. In all other cases, however, correlations with only parts of a score (\emph{e.g.}, syllables per words) are omitted.

\item A few earlier studies calculated and listed various readability measures for many passages of text, but did not report coefficients of correlation between them. I manually calculated these correlations myself.

\end{itemize}


\begin{refsegment}
\input{/USERS/ERINHENGEL/Dropbox/Readability/draft/0-tex/generated/Figure-D.1-bib}
\end{refsegment}
\printbibliography[segment=1,heading=none]
\clearpage


\section{Discussion of potential alternative mechanisms}
\label{appendixalternatives}

A gender readability gap exists. It's still there after including editor, journal and year effects---meaning it's probably not caused by antiquated policies and attitudes, long since overcome. The gap is unaffected by field controls, so it doesn't seem to be related to women researching topics that are easier to explain. Nor does it appear to be caused by factors correlated with gender but actually linked to authors' (or co-authors') competence as economists and fluency in English---if so, institution, native speaker and citation controls would reduce it. They do not.\footnote{I also conducted a primitive surname analysis ~\citep[see][pp. 35--36]{Hengel2016}. It suggests that the female authors in my data are no more or less likely to be native English speakers.}

The gap grows between first draft and final publication and over the course of women's careers, likely precluding inborn advantage and one-off improvements in response to external circumstances unrelated to peer review. This probably also rules out gender differences in (i) biology\slash behaviour---\emph{e.g.}, sensitivity to referee criticism\footnote{While women do appear more \emph{internally} responsive to feedback---criticism has a bigger impact on their self-esteem---available evidence suggests they aren't any more \emph{externally} responsive to it, \emph{i.e.}, women and men are equally likely to change behaviour and alter performance after receiving feedback~\citep{Johnson2002,Roberts1989}.}---or (ii) knowledge about referee expectations. If diligently addressing every referee concern has no apparent upside---acceptance rates are unaffected---and a very clear downside---constant redrafting takes time---even the most oversensitive, ill-informed woman would \emph{eventually} re-examine initial beliefs and start acting like a man, no?\footnote{This statement is especially relevant if the opportunity cost to women for ``wasting'' time on needless tasks is higher---\emph{e.g.}, because of family responsibilities.} Yet this is not what we observe. The largest investments in writing well are made by female economists with greatest exposure to peer review---\emph{i.e.}, those with the best opportunity to update their priors.

Women's papers are more likely assigned female referees~\citep{Abrevaya2012,Gilbert1994}.\footnote{Note that women are only a fraction of all referees---8 percent in 1986~\citep{Blank1991}, 10 percent in 1994~\citep{Hamermesh1994} and 14 percent in 2013~\citep{Torgler2013}.  \citet{Abrevaya2012} report female-authored papers were only slightly more likely to be assigned a female referee between 1986--1994; matching increases between 2000--2008.} If women are more demanding critics, clearer writing could reflect their tougher reviews.\footnote{It's not clear whether women's reports are more critical. A study specific to post-graduate biologists suggests yes~\citep{Borsuk2009}; other studies specific to economics suggest not~\citep{Abrevaya2012,Card2020}.} Women concentrate in particular fields, so it's natural female referees more often review female-authored papers. Nevertheless, for the readability gap to exist only because of specialisation, controlling for \emph{JEL} classification should help explain it.\footnote{Specifically, men and women publishing in the same field face the same pool of referees, so controlling for that pool should reduce gender differences in readability if specialisation contributes to it.} It does not: including 20 primary or 731 tertiary \emph{JEL} category dummies has little effect. So if referee assignment is causing the gap, it may be because journals disproportionately refer female-authored papers to the toughest critics.\footnote{Relatedly, perhaps female-authored research is more provocative and therefore warrants more scrutiny. Yet if this explained the gap, controlling for \emph{JEL} classification should reduce (or eliminate) it---unless women's work is systematically more provocative even among researchers in very narrow fields. There is some evidence for this hypothesis---provocative work is (presumably) highly cited work and recent female-authored papers published in top economics journals are cited more~\citep{Moon2020,Card2020}. Yet more provocative, cited research would probably be published at higher rates---and there is no evidence women's paper's are more frequently accepted~\citep{Ceci2014}.} Meaning it isn't referees who are biased---it's editors.\footnote{This is a form of biased referee assignment identified in \autoref{Theorem1}. It would also apply if the readability gap reflects referees' apathy for women's work. Readability is particularly relevant when interest in---and knowledge about---the topic is low~\citep{Klare1976,Fass1978}. Thus, a gap could emerge if editors fail to assign interested and knowledgable referees to female-authored papers.}

\autoref{nber} suggests a link between the gender readability gap and peer review; the evidence presented in \autoref{mechanisms} suggests that factors outside women's control drive it. Yet oversensitivity and\slash or poor information could create the former gap while \emph{another} gender bias unconnected to peer review generates the latter. One in particular comes to mind: the feedback women receive in conferences and seminars. Perhaps experienced female economists tighten prose (before or after submission) in response to audience member remarks? Recent evidence suggests female speakers are indeed given a harder time~\citep{Dupas2021}. Nevertheless, most conference and seminar participants are also current (or future) journal referees. Neutral peer review feedback is inconsistent with non-neutral conference\slash seminar feedback when originating from the same group---especially since gender neutrality is emphasised in both environments.

Perhaps women focus on writing at the expense of some other aspect of a paper due to a comparative advantage? Women's chosen publication strategy results in similar (or lower) acceptance rates and longer review times compared to the one employed by men. If men and women are equally capable researchers then writing well cannot be a comparative advantage and at the same time be strictly dominated by another strategy.\footnote{Assuming men and women are equally capable researchers, women would only emphasise a particular aspect of a paper at the expense of others if doing so achieved a similar outcome\slash effort trade-off as the one employed by men. As discussed in \autoref{duration}, however, the outcome\slash effort combination women \emph{currently} experience appears to be strictly worse than men's.}

In the universe of straightforward alternatives, this leaves us with one: female economists are less capable researchers. As mentioned earlier, factors correlated with gender but actually related to competency should decline when appropriate proxies are included. The sample itself is one such proxy---these are, after all, only articles published in the top four economics journals. Adding other controls---author seniority, institution, total article count, citations and published order in an issue---has no effect.\footnote{Published order in an issue was introduced as a set of indicator variables in an earlier version of this paper ~\citep[][pp. 42 and 44]{Hengel2016}.} The gap is widest for the most productive economists and even exists among articles originally released as NBER working papers---both presumably very clear signals of merit. Indeed, contemporary female-authored papers published in a top-four economics journal are, in fact, cited more than male-authored papers~\citep{Moon2020}.

Yet I cannot rule out the possibility that women's work is systematically worse than men's in a way that is somehow not full captured by citations, proxies for author prominence and seniority or author-specific fixed effects---or that the female and male authors in \autoref{matchingresults} are not really equivalent. And if this is true, editors and referees \emph{should} select and peruse our papers more carefully---a byproduct of which could be better written papers after-the-fact or more attractive prose compensating for structural weaknesses before it.

``Quality'' is subjective; measurement, not easy. Nevertheless, attempts using citation counts and journal acceptance rates do not indicate that men's research is any better, conditional on publication: as discussed in \autoref{seumodel}, men's and women's papers are accepted at similar rates unconditional of quality. As already mentioned, recent research specific to economics suggests female-authored papers may be cited more conditional on publication~\citep{Grossbard2018,Moon2020,Card2020}.\footnote{Nevertheless, a significant amount of research finds evidence of bias against women in the decision to cite, unconditional of publication~\citep{Ferber1986,Ferber1988,Dion2018,Koffi2021}. This suggests that citations under-estimate the quality of female-authored work.}

More complicated, multi-factor explanations could resolve inconsistencies present when each is analysed in isolation. Perhaps female economists are perfectionists, and it gets stronger with age?\footnote{While women score higher on maintaining order~\citep{Feingold1994}---a trait including organisation and perfectionism---significant differences are not universally present in all cultures~\citep{Costa2001}, and differences that are present appear to decline---or even reverse---as people age~\citep{Weisberg2011}.} Or, a preference for writing well coupled with unaccounted for co-author characteristics could combine to cause women's more readable papers \emph{and} their increasing readability.\footnote{\label{FootnoteSeniorWomen}This might occur, for example, if women are excluded from male networks as $t$ increases; consequently, senior female economists may be more likely to co-author with other women than junior female economists. Relatedly, women may have preferred to have written their $t=1$ publication more clearly, but senior male co-authors held them back; at $t=3$, they enjoy more freedom to achieve their desired (higher) readability by writing on their own or with other women. As I show in an earlier version of this paper, however, as $t$ increases, women are more likely to co-author with men, while men are more likely to co-author with women~\citep[][Table 12, p. 25]{Hengel2016}.} Alternatively, measurement error and\slash or co-variate controls could have interacted with gender in ways I did not anticipate.

Still, no explanation matches the simplicity of biased referees and\slash or editors. Coherence and economy do not establish fact, but they are useful guides. This single explanation neatly accounts for all observed patterns. If reviewers apply higher standards to female-authored papers, they will be rejected more often and\slash or subject to tougher review. Added scrutiny should improve exposition but prolong publication. Women would internalise the rewards they receive from writing more clearly, accounting for their better writing over time.\footnote{In support of this hypothesis, existing and ongoing research suggests female workers are held to higher standards in job assessments: they are acknowledged less for creativity and technical expertise, their contributions are infrequently connected to business outcomes; guidance or praise supervisors do offer is vague and tends to under-estimate women's ``potential'' ~\citep{Correll2016,Benson2021}. Students display a similar bias. \href{http://benschmidt.org/profGender/}{Data} from \href{http://www.ratemyprofessors.com/}{Rate My Professors} suggest female lecturers should be ``helpful'', ``clear'', ``organised'' and ``friendly''. Men, instead, are praised (and criticised) for being ``smart'', ``humble'' or ``cool''~\citep{Schmidt2015}. A study of teaching evaluations similarly finds students value preparation, organisation and clarity in female instructors; their male counterparts are considered more knowledgable, praised for their ``animation'' and ``leadership'' and given more credit for contributing to students' intellectual development~\citep{Boring2017}.}

\clearpage

\section{\autoref{articlelevel}, suplemental output}
\label{appendixarticlelevel}

\subsection{Readability differences across journals}
\label{appendixjournal}

\autoref{table3_journal} shows the coefficients on the journal dummies in column (2), \autoref{table3_FemRatio}. They compare \emph{AER}'s readability to the readability of \emph{Econometrica}, \emph{JPE} and \emph{QJE}.


\input{/USERS/ERINHENGEL/Dropbox/Readability/draft/0-tex/generated/Table-F.1}
\clearpage


\subsection{Gender and readability, by \emph{JEL} code}
\label{appendixjel}

\autoref{figureE1} displays results from an ordinary least squares regression on the Dale-Chall score; regressors are: (i) ratio of female co-authors (papers with fewer than 50 percent female authors are classified as male, see \autoref{gender}); (ii) dummies for each primary \emph{JEL} code; (iii) interactions from (i) and (ii); (iv) controls for editor, journal, year, institution and English fluency; and (v) quality controls---citation count, $\text{max. }T$ fixed effects (author prominence) and $\text{max. }t$ (author seniority). Codes A, B, M and P are dropped due to insufficient number of female-authored papers. (Each had fewer than 10 papers authored only by women; no paper is classified under category Y.) Due to small samples---particularly of female authors---\autoref{figureE1} includes 563 articles from \emph{AER Papers \& Proceedings}.\footnote{See  \citet[][pp. 42--43]{Hengel2016} for a version of \autoref{figureE1} excluding \emph{AER Papers \& Proceedings} articles.}

\input{/USERS/ERINHENGEL/Dropbox/Readability/draft/0-tex/fixed/Figure-F.1}

Points in \autoref{figureE1} reflect marginal effects across \emph{JEL} classification; bars represent 90 percent confidence intervals from standard errors clustered by editor. The mean effect at observed \emph{JEL} codes is 0.17 (standard error 0.054). This estimate coincides with results in \autoref{table3_FemRatio}---women's papers require about two fewer months of schooling to understand---and is highly significant.

Women earn higher marks for clarity in 12 out of 15 categories; only five are at least weakly significant: Q (Agricultural and Natural Resource Economics; Environmental and Ecological Economics), N (Economic History), G (International Economics\slash Finance), J (Labour Economics) and D (Microeconomics). Men may be better writers in L (Industrial Organisation), O (Economic Development, Innovation, Technological Change, and Growth) and H (Public Economics); none, however, are statistically different from zero. \autoref{figureE1}'s right-hand graph displays coefficients from interacting the ratio of female co-authors with each \emph{JEL} code. N is (weakly) significantly above the mean; remaining categories are not statistically different from the mean effect.

In general, sample sizes are small and estimates imprecise---only Labour Economics and Microeconomics contain more than 100 papers written only by women (the others average 35). Nevertheless, \autoref{figureE1} suggests two things. First, the mostly insignificant interaction terms indicate outlier fields are probably not driving journals' gender readability gap---nor is any specific field bucking the trend. Second, the number of women in a field appears to have little effect on the size of the gap: Agriculture\slash Environment has one of the lowest concentrations of female-authored papers---but Economic History has one of the highest (Labour Economics falls between the two). Of course, Economic History papers are still overwhelmingly---as in 74 percent---penned just by men. But given the readability gap is present in subfields with both above- and below-average rates of sole female authorship, women may need to be better writers even where more of them publish.

\clearpage

\subsection{Author-level analysis}
\label{appendixauthorlevel}

In this appendix, I analyse readability at the author-level. To disaggregate the data, each article is duplicated $N_j$ times, where $N_j$ is article $j$'s number of co-authors; observation $j_k\in\{1,\ldots,N_j\}$ is assigned article $j$'s $k\text{th}$ author. I then estimate the dynamic panel model in \autoref{equation1}:\input{/USERS/ERINHENGEL/Dropbox/Readability/draft/0-tex/equations/equationF1}$R_{j_{it}}$ is the readability score for article $j$---author $i$'s $t$th top-four publication; $R_{it-1}$ is the corresponding value of author $i$'s $t-1$th top-four paper. Gender enters twice---the binary variable $\text{male}_i$ and $\text{female ratio}_j$---to account for author $i$'s sex and the sex of his co-authors, respectively (papers with fewer than 50 percent female authors are classified as male, see \autoref{gender}). $\vect X_j$ is a vector of observable controls. It includes: editor, journal, institution and English fluency dummies, controls for blind review and quality---citation count (asinh), $\text{max. }T$ (author prominence) and $\text{max. }t$ (author seniority)---and $N_j$ to control for author $i$'s proportional contribution to paper $j$.\footnote{To reduce the number of instruments (and thanks to a high degree of correlation with editor fixed effects) year fixed effects are omitted. Results are similar when they are included ~\citep[see, \emph{e.g.},][p. 17]{Hengel2017}.} $\alpha_i$ are author-specific effects and $\vep_{it}$ is an idiosyncratic error. $\alpha_i$ are eliminated by first-differencing. For each time period, endogeneity in the lagged dependant variable is instrumented with up to five earlier lags~\citep{Arellano1995,Blundell1998}. To account for duplicate articles, the regression is weighted by $1/N_j$. Standard errors are clustered at the level of the author.

\autoref{table4_FemRatio} displays results. Rows one and two present contemporaneous marginal effects on co-authoring with women for female ($\beta_1$) and male ($\beta_1+\beta_2$) authors, respectively. Both estimates are positive---everyone writes more clearly when collaborating with women---although statistically significant only for female authors. Marginal effects for women are up to twice as large as those shown in \autoref{table3_FemRatio}; they suggest women write 2--6 percent better than men.

Coefficients on the lagged dependant variables are small, suggesting readability is mostly determined contemporaneously. Nevertheless, their uniform positivity and occasional significance indicate some persistence. \autoref{table4_FemRatio}'s second panel reports test statistics of model fit. Tests for serial correlation indicate no model misspecification. $p$-values on the overall Hansen test statistic hover between 0.79--0.96, thus failing to reject that the instruments are valid; however, their very high values suggests the model may suffer from instrument proliferations which weakens the test~\citep[for a discussion, see][]{Roodman2009a}. The $p$-value on the Sargan test also fails to reject that the instruments are valid at traditional significance thresholds; but although it is not vulnerable to instrument proliferation, it does require homoskedastic errors. Additional tests (available on request) suggest results are not sensitive to including the full set of (non-collapsed) instruments or to reductions in the number of instruments. Given the possibility of instrument proliferation, however, results in \autoref{table4_FemRatio} should be interpreted with caution.


\input{/USERS/ERINHENGEL/Dropbox/Readability/draft/0-tex/generated/Table-F.2-FemRatio}
\clearpage


\section{\autoref{nber}, suplemental output}
\label{appendixnber}

\subsection{\autoref{table6_FemRatio}, full output (first and final columns)}
\label{appendixdraftcorr}

\autoref{table6_full} displays coefficients from estimating \autoref{equation2} using OLS. The first row displays coefficients on working paper score ($R_{jW}$); the second row shows the coefficient on female ratio (papers with fewer than 50 percent female authors are classified as male, see \autoref{gender}); the third rows shows the coefficient on the interaction between blind review and the ratio of female authors on a paper. (All three coefficients are also shown in the first panel of \autoref{table6_FemRatio}.) Remaining rows present estimated coefficients on the other (non-fixed effects) control variables: $N_j$ (number of co-authors), $\text{max. }t$ (author seniority), $\text{max. }T$ (author prominence), number of citations (asinh) and dummy variables equal to one if article $j$ is authored by at least one native English speaker or is classified as theory, empirical or other. Similarly, \autoref{table6_change_full} displays coefficients from estimating \autoref{equation3}. The coefficients on female ratio and its interaction with blind review correspond to estimates presented in the second panel of \autoref{table6_FemRatio}.

As discussed in \autoref{nberresults}, we do not observe the citations papers would have received had they not undergone peer review. Nevertheless, \autoref{table6_full} suggests a negative (albeit insignificant) relationship between published readability \emph{conditional} on draft readability; \autoref{table6_change_full} suggests a negative (but again insignificant) relationship between citations and the change in readability that occurs \emph{during} peer review. Thus, they tentatively suggest that the readability revisions women are asked to make during peer review may not ultimately improve the quality of their papers.\footnote{The coefficient on citations is negative \textbf{\emph{only}} when controlling for $R_{jW}$ or using the change in readability as the dependant variable. Otherwise, readability positively correlates with both working paper and published paper readability (see \aref{appendixmeasurementerror}; results for draft readability and using the specific sample from \autoref{table6_FemRatio} available on request).}


\input{/USERS/ERINHENGEL/Dropbox/Readability/draft/0-tex/generated/Table-G.1}
\input{/USERS/ERINHENGEL/Dropbox/Readability/draft/0-tex/generated/Table-G.2}
\clearpage


\subsection{\autoref{table6_FemRatio}, accounting for field}
\label{appendixnberfield}

As argued in \autoref{nber}, if field only impacts the readability of a paper when it is first drafted, then the change in readability between versions should not depend on it. For example, using the change in score as the dependent variable should wash out potential bias from, say, concepts in certain areas being easier to explain. Moreover, because FGLS estimates (shown in the final panel of \autoref{table6_FemRatio}) are almost identical to estimates using the change in readability as the dependent variable (shown in the second panel of \autoref{table6_FemRatio}), they may not suffer from substantial bias, either, despite only taking field broadly into account (via the empirical, theory and other dummies).

For added robustness, however, I replicate \autoref{table6_FemRatio} but also control for primary \emph{JEL} categories. Results are shown in \autoref{table6_jel}. Standard errors on the coefficients on female ratio are slightly higher in \autoref{table6_jel} compared to \autoref{table6_FemRatio}; coefficients on the interactions between blind review and the ratio of female authors are also slightly higher. Otherwise, results and conclusions are very similar to those presented in \autoref{table6_FemRatio}.


\input{/USERS/ERINHENGEL/Dropbox/Readability/draft/0-tex/generated/Table-5-jel}
\clearpage


\subsection{Blind review event study}
\label{appendixeventstudy}

In this section, I show a crude event study illustrating the impact of blind review on the gender readability gap formed during peer review. To implement it, I re-estimate \autoref{equation3} without controlling for blind review, but otherwise accounting for the same factors in \autoref{table6_FemRatio}. \autoref{figure-blind-es} plots residuals from this regression for papers published in the \emph{AER} and \emph{QJE} 8--9 years, 6--7 years, \emph{etc.} before they switched to or from single-blind review (or the advent of the internet) and 2--3 years, 4--5 years, \emph{etc.} afterwards.

\input{/USERS/ERINHENGEL/Dropbox/Readability/draft/0-tex/fixed/Figure-G.1}

Most graphs in \autoref{figure-blind-es} suggest a discontinuity in women's unexplained changes in readability at the introduction of single-blind review (or the internet). For men, however, unexplained changes to readability appear largely unaffected by double-blind review, conditional on included controls. \autoref{figure-blind-es} therefore tentatively suggests that women may benefit (on average) from a policy of double-blind review, while men are less affected by it.

\clearpage

\subsection{Semi-blind review}
\label{appendixdoubleblind}

\autoref{table6_FemRatio} tentatively suggests double-blind review may have successfully reduced peer review's impact on the gender readability gap \emph{before} the internet. In this appendix, I show evidence (also tentative) suggesting that it may have been less effective \emph{after} the internet. In particular, I re-estimated \autoref{equation3} on the sample of articles published in or after 1998 and defined a dummy variable equal to one if a journal had in place an official policy of double-blind review at the time a paper was published. The coefficients on female ratio (papers with fewer than 50 percent female authors are classified as male, see \autoref{gender}) and its interaction with the redefined indicator of blind review are presented in \autoref{table7_semiblind}. The coefficient on female ratio reflects the gender gap in journals that did not have in place an official policy of double-blind review (\emph{i.e.}, \emph{Econometrica}, \emph{JPE}, \emph{QJE} after 2004 and \emph{AER} after 2011). The coefficient on the interaction between female ratio and blind review reflects the gender gap in journals that did have an official policy of blind review (\emph{i.e.}, \emph{QJE} before 2004 and \emph{AER} before 2011).

Four out of five scores suggest a positive gender readability gap among both blindly reviewed papers and non-blindly reviewed papers published after the advent of the internet. Thus, it does not appear that blind review was able to alleviate the gender readability gap once referees could easily determine authors' identities by simply Googling them.

Editors knew submitting authors' identities---and therefore genders---both before and after the internet as well as under single- and double-blind review. Thus, the reversed gap in double-blind review pre-internet (\autoref{table6_FemRatio}) and positive gap post-internet (\autoref{table7_semiblind}) may suggest bias from referees---as opposed to editors---drives observed gender differences in readability.\footnote{Many thanks to an anonymous referee for suggesting this idea.} Nevertheless, this conclusion is based on noisy (often insignificant) estimates and should therefore only be made cautiously.


\input{/USERS/ERINHENGEL/Dropbox/Readability/draft/0-tex/generated/Table-G.4}
\clearpage

\subsection{Time between working paper release and journal submission}
\label{appendixtiming}

\autoref{figure6} displays a histogram of the length of time between the date an author releases his draft paper as an NBER working paper and the date he submits it for peer review at \emph{Econometrica}. Pink represents papers with at least one female co-author (41 articles); blue are papers with no female co-authors (187 articles).

\autoref{figure6} suggests most manuscripts are submitted to peer review at the same time or \emph{before} they are released as NBER Working Papers---and this is especially true for papers with at least one female author. This suggests that the estimates presented in \autoref{table6_FemRatio} reflect gender differences in changes made to manuscripts while those manuscripts are indeed under review at the journal in which they will be eventually published.


\input{/USERS/ERINHENGEL/Dropbox/Readability/draft/0-tex/fixed/Figure-G.2}
\clearpage


\subsection{Abstract word limits}
\label{appendixwordlimits}

In \autoref{nber}, I argue that the gender gap in the changes in readability between draft and final versions of a paper likely occur because of the peer review process. Yet NBER working paper abstracts can be of any length while abstracts published in \emph{Econometrica} and \emph{AER} cannot---they are restricted to 150 and 100 words, respectively. Observed readability gaps could consequently result from gender differences in how authors conform to these limits.

To test this hypothesis, I replicated the analysis described \autoref{nberresults} (and shown in \autoref{table6_FemRatio}) on the subset of articles with draft abstracts below the official minimum word limit of the journals in which they were eventually published. Results are shown in \autoref{table6_wordlimit}. Despite dropping about 40 percent of observations, coefficient magnitudes are similar to those reported in \autoref{table6_FemRatio}; standard errors are somewhat larger.\footnote{Results are similar if I also include a control for the number of words in the working paper version of the abstract (available on request).}


\input{/USERS/ERINHENGEL/Dropbox/Readability/draft/0-tex/generated/Table-5-wordlimit}
\clearpage


\section{\autoref{duration}, supplemental output}
\label{appendixduration}

\subsection{Alternative year fixed effects}
\label{appendixalternativeyear}

\autoref{table10_subyear} and \autoref{table10_pubyear} replicate \autoref{table10_FemRatio}, replacing acceptance year fixed effects with fixed effects for submission and publication years, respectively.


\input{/USERS/ERINHENGEL/Dropbox/Readability/draft/0-tex/generated/Table-6-subyear}
\input{/USERS/ERINHENGEL/Dropbox/Readability/draft/0-tex/generated/Table-6-pubyear}
\clearpage


\subsection{Alternative thresholds for $\text{mother}_j$}
\label{appendixmotherhood}

\autoref{table10_thresholds} repeats the regression pre\-sented in \autoref{table10_FemRatio} column (5) using alternative age thresholds to define motherhood: $\text{mother}_j$ equals 1 if paper $j$'s co-authors are all mothers to children younger than three (first column), four (second column), \emph{etc.} Changing this threshold has little effect on female ratio's coefficient. The coefficients on $\text{mother}_j$ and $\text{birth}_j$ are persistently negative and positive (respectively), although magnitudes and standard errors vary. Remaining coefficients are unaffected.


\input{/USERS/ERINHENGEL/Dropbox/Readability/draft/0-tex/generated/Table-H.3}
\clearpage


\subsection{Quantile regression}
\label{appendixquantile}

In \autoref{table10_quantile}, I re-estimate gender differences in time spent under review using a quantile regression model. The first panel replicates \autoref{table10_FemRatio}, column (5) at the 25th, median and 75th percentiles of review times; the second panel similarly replicates the third column of \autoref{table11_FemRatio}.

The coefficient on female ratio is positive and significant across all three percentiles. Its magnitude is greatest in the right-tail of \emph{Econometrica}'s distribution but is similarly sized across all percentiles when estimated using observations from both \emph{Econometrica} and \emph{REStud}.


\input{/USERS/ERINHENGEL/Dropbox/Readability/draft/0-tex/generated/Table-H.4}
\clearpage


\section{\autoref{mechanismsdescriptive}, supplemental output}
\label{appendixseumatching}

\subsection{Studies evaluating gender differences in acceptance rates}
\label{appendixacceptance}


\input{/USERS/ERINHENGEL/Dropbox/Readability/draft/0-tex/fixed/Table-I.1}
\clearpage


\subsection{Authors' average readability scores for their first, mean and final papers}
\label{appendixseuempirical}

\autoref{tableH1} displays authors' average readability scores for their first, mean and final top-four papers. Grade-level scores (Flesch-Kincaid, Gunning Fog, SMOG and Dale-Chall) have been multiplied by negative one (see \autoref{readability}). Sample excludes authors with fewer than three publications.

As their careers advance, women do write more clearly: their average readability scores are 1--5 percent higher than the readability of their first papers; their latest papers 1--7 percent. For a man, however, his average and last paper are about as well written as his first.


\input{/USERS/ERINHENGEL/Dropbox/Readability/draft/0-tex/generated/Table-I.2}
\clearpage


\subsection{\autoref{tableH2_FemRatio}, tests of coefficient equality}
\label{appendixequality}

\autoref{tableH3} tests equality of coefficients in each column of \autoref{tableH2_FemRatio}. It rejects equality between coefficients in the first and third columns at $p<0.05$ for the Flesch Reading Ease, Flesch-Kincaid, Gunning Fog and SMOG scores.


\input{/USERS/ERINHENGEL/Dropbox/Readability/draft/0-tex/generated/Table-I.3}
\clearpage


\section{\autoref{quantification}, supplemental output}
\label{appendixquantification}

\subsection{Co-variate balance}
\label{appendixmatchingbalance}

\autoref{balance0} compares co-variate balance pre- and post-match. The first column displays averages for the 121 female authors with at least three publications in the data. The first column of the first panel (``Pre-match means'') displays corresponding averages for the 1,554 male authors with three or more publications. The first column of the second panel (``Post-match means'') displays (weighted) averages for the 108 male authors matched with a female author. Gender differences are smaller post-match; $t$-statistics are likewise closer to zero.


\input{/USERS/ERINHENGEL/Dropbox/Readability/draft/0-tex/generated/Table-J.1}
\clearpage


\subsection{List of authors in each matched pair}
\label{appendixmatchingnames}


\input{/USERS/ERINHENGEL/Dropbox/Readability/draft/0-tex/generated/Table-J.2}
\clearpage


\subsection{$\widehat R_{it}$ regression output}
\label{appendixreconstruction}

\autoref{Rit_regresults} displays output from time- and gender-specific regressions used to generate $\widehat R_{it}$ (\autoref{equation14}).


\input{/USERS/ERINHENGEL/Dropbox/Readability/draft/0-tex/generated/Table-J.3}
\clearpage


\subsection{$\widehat R_{it}$, controlling for \emph{JEL} category}
\label{appendixseujel}

\autoref{table8_jel} and \autoref{figure8_jel} replicate the analysis in \autoref{matchingresults} but \autoref{equation14} controls for primary \emph{JEL} category. $\widehat R_{it}$ was reconstructed at female ratio equal to 1 for women, 0 for men and for a paper classified in \emph{JEL} categories D (microeconomics) and J (labour and demographic economics).


\vfill
\input{/USERS/ERINHENGEL/Dropbox/Readability/draft/0-tex/generated/Table-9-jel}
\vfill
\input{/USERS/ERINHENGEL/Dropbox/Readability/draft/0-tex/fixed/Figure-5-jel}
\vfill
\clearpage


\subsection{Unadjusted $R_{it}$}
\label{appendixseur}

\autoref{table8_R} and \autoref{figure8_R} replicate the analysis in \autoref{matchingresults} but \autoref{equation14} does not adjust for the ratio of female authors on a paper.


\vfill
\input{/USERS/ERINHENGEL/Dropbox/Readability/draft/0-tex/generated/Table-9-R}
\vfill
\input{/USERS/ERINHENGEL/Dropbox/Readability/draft/0-tex/fixed/Figure-5-R}
\vfill
\clearpage


\subsection{Robustness}
\label{appendixmatchinglimitations}

Conclusions drawn in \autoref{matchingresults} are predicated on several strong assumptions. First, all results depend on match accuracy. Post-match co-variates are well balanced (see \aref{appendixmatchingbalance}). They remain well balanced---and similar to the matched population---when restricted to pairs satisfying $D_{ik}\ne0$ (see Appendix M.1 in the \href{http://www.erinhengel.com/research/publishing_female20180828.pdf}{August 2018} version of the paper). To facilitate further scrutiny, \aref{appendixmatchingnames} lists the names of economists in each pair. Matches using alternative variables (\emph{e.g.}, minimum citation counts, mean institutional rank or fraction of articles per primary \emph{JEL} category) and specifications (\emph{e.g.}, propensity score matching) generate similar figures and conclusions.\footnote{See  \citet[][pp. 30--33]{Hengel2017} for propensity score matches from a probit model performed with replacement and using a wider array of co-variates.}

Additionally, authors must be sufficiently experience at $t'$ for Assumptions 4 and 5 in \autoref{Corollary1} to hold. I assume this point occurs at or before authors' third top-four paper. Fifty percent of women with three or more top publications satisfy Conditions 1 \emph{and} 2 when compared to equivalent men. Among them, $D_{ik}$ is far from zero: these women write, on average, 21 percent more clearly than equivalent men with identical experience. I believe it is unlikely that half of all female economists with three top publications---plus many more second-tier publications and substantial experience refereeing and editing themselves---make mistakes of this magnitude.

To generate the counterfactual $\widehat R_{it}$ (\autoref{equation14}), I assume unobserved co-author characteristics do not partially correlate with $\text{female ratio}_{it}$, conditional on $i$'s gender and experience. To test the robustness of this assumption, \autoref{tableH2_Fem100} (\aref{appendixexclusive}) replicates \autoref{tableH2_FemRatio} on exclusively, majority and senior female-authored papers. I have also repeated the analyses shown in \autoref{table8_base} and \autoref{figure8_base} without adjusting for $\text{female ratio}_{it}$ (\aref{appendixseur}) and on subsets of matched pairs in which the woman's $t=1$ and $t=3$ papers are solo- or exclusively female-authored (16), majority female-authored (20) or at least 50 percent female-authored (76). Although sample sizes for the latter three analyses are small, they also find $D_{ik}\ne0$ in about 70--75 percent of matched pairs; most of those (70 percent) indicate higher standards against the female member; the impact across all five scores also averages about 5 percent.

Moreover, experience appears to be the only $t$-varying factor driving within $i$ changes in readability. \autoref{tableH2_FemRatio} and additional analyses in a 2016 version of this paper~\citep[][pp. 23--24]{Hengel2016} show an identical pattern despite controlling for a large array of potential confounders. In a 2017 version, I reconstructed $\widehat R_{it}$ using several $t$-varying factors (number of co-authors, institutional rank, institutional rank of the highest ranked co-author, $t$ for the most experienced co-author, publication year and dummies for each journal)~\citep[][pp. 30, 61]{Hengel2017}; \aref{appendixseujel} adds \emph{JEL} classification codes to \autoref{equation14}. In \autoref{tableH2_Fem100} (\aref{appendixexclusive}), I restrict \autoref{tableH2_FemRatio}'s analysis to solo-authored papers or those co-authored by members of the same sex. In all instances, women's readability is consistently shown to increase with $t$; when comparable results are estimated, they are similar to those presented in \autoref{table8_base} and \autoref{figure8_base}.

Finally, accurate quantification requires that three additional criteria are also met. Assuming higher standards for $i$: (i) $i$'s acceptance rate is no more than $k$'s; (ii) $i$'s draft readability is at least as high as $k$'s; and (iii) $i$'s draft readability at $t=3$ is at least as high as his draft readability at $t=1$. As already discussed in \autoref{mechanismsdescriptive}, (i) rules out the possibility that $i$ is appropriately rewarded (relative to $k$) for writing more clearly. (ii) and (iii) eliminate situations in which women write more clearly during peer review in order to compensate for poorer writing---and consequently higher desk rejection rates---before peer review.

Unfortunately, my data do not perfectly identify acceptance rates nor do I have $t=1$ and $t=3$ draft readability scores for every matched pair. Nevertheless, the data I do have and prior research suggest (i)--(iii) not only hold on average, but do not exert upward bias on my estimate of $D_{ik}$, more generally. First, I reviewed the literature on gender neutrality in journals' acceptance rates in \autoref{mechanismsdescriptive} and \aref{appendixacceptance}; women are not accepted more often than men. Results and conclusions are similar when I attempt to adjust for acceptance rates explicitly by also requiring that $T_{i}\le T_{k}$ for matched pairs in which $i$ is held to higher standards relative to $k$ (see Appendix M.4 in the \href{http://www.erinhengel.com/research/publishing_female20180828.pdf}{August 2018} version of the paper). As shown in \autoref{nber}, women's draft papers are indeed more readable than men's. \autoref{indirecteffect} provides further confirmation. \autoref{figure9} plots the readability of women's and men's draft and published papers over increasing $t$. Women's drafts are more readable than men's drafts at $t=3$ \emph{and} their own drafts at $t=1$.

\clearpage

\section{\autoref{indirecteffect}, supplemental output}
\label{appendixindirecteffect}

\subsection{Experience and review times}
\label{appendixtimeexp}

In \autoref{indirecteffectinterpretation} I find evidence suggesting that inexperienced female economists go through the toughest review, conditional on acceptance. To investigate further, I test the impact of experience on time spent in review by re-estimating \autoref{equation16} on sub-samples of junior ($t=1$) and senior ($t>1$) authors.\footnote{Three notes on estimation. First, in \autoref{quantification}, I define ``experienced'' as $t=3$. However, most female-authored papers published in \emph{Econometrica} and \emph{REStud} are by women with no (or only one) previous top publication; only 24 have two or more previous papers and were the most senior co-author on a $t>2$ paper. Second, to eliminate confounding by more senior co-authors, I restrict the sample to the senior authors on a paper (\emph{i.e.}, authors satisfying $\text{max. } t=t$). (Including these observations does not substantially impact results or conclusions.) Third, because the sample includes data from \emph{REStud}, readability, motherhood and childbirth controls are not included. See the \href{http://www.erinhengel.com/research/publishing_female20180828.pdf}{August 2018} version of this paper for results that control for these factors (based on data from \emph{Econometrica} alone).} Results are displayed in \autoref{figure11}. They suggest papers by junior women do indeed take longer in review; the gender gap is significantly smaller---albeit still positive---for senior women.


\input{/USERS/ERINHENGEL/Dropbox/Readability/draft/0-tex/fixed/Figure-K.1}
\clearpage

\section{Alternative program for calculating readability scores}
\label{appendixalternativereadability}

In this section, I replicate \autoref{table3_FemRatio}, \autoref{table6_FemRatio}, \autoref{tableH2_FemRatio} and \autoref{table4_FemRatio} using readability scores generated by the \href{https://github.com/trinker/readability}{R \texttt{readability} package}, an alternative program for calculating Flesch-Kincaid, Gunning Fog and SMOG readability scores. Replications for other tables and figures presented in the paper are not shown, but will be made available on request.

\texttt{Textatistic} and \texttt{readability} employ different strategies to adapt the scores to automated calculation---\emph{e.g.}, \texttt{readability} counts semi-colons and dashes as sentence-ending terminations; \texttt{Textatistic} does not.\footnote{Readability scores were originally developed to be calculated by hand. Automating their calculation requires slightly adapting the algorithms. For example, all five scores define sentences as grammatically independent units of thoughts---\emph{e.g.}, two independent clauses connected by a dash or semi-colon count as two separate sentences. Unfortunately, semi-colons and dashes are frequently used in other ways and it is difficult to programmatically distinguish between contexts.} Results appear robust to these (and other) small discrepancies: coefficients are similar to those presented in the body of the paper.


\input{/USERS/ERINHENGEL/Dropbox/Readability/draft/0-tex/generated/Table-3-R}
\clearpage
\input{/USERS/ERINHENGEL/Dropbox/Readability/draft/0-tex/generated/Table-5-R}
\clearpage
\input{/USERS/ERINHENGEL/Dropbox/Readability/draft/0-tex/generated/Table-8-R}
\clearpage
\input{/USERS/ERINHENGEL/Dropbox/Readability/draft/0-tex/generated/Table-F.2-R}
\clearpage


\section{Alternative proxies for article gender}
\label{appendixalternativemeasure}

In this appendix, I replicate \autoref{table3_FemRatio}, \autoref{table6_FemRatio}, \autoref{table10_FemRatio}, \autoref{table11_FemRatio}, \autoref{tableH2_FemRatio} and \autoref{table4_FemRatio} using the alternative proxies for article gender summarised in \autoref{SampleSummaries}.

\input{/USERS/ERINHENGEL/Dropbox/Readability/draft/0-tex/fixed/Table-M.1}

\subsection{Solo-authored}
\label{appendixsolo}


\begin{vplace}[0.7]
\input{/USERS/ERINHENGEL/Dropbox/Readability/draft/0-tex/generated/Table-3-FemSolo}
\end{vplace}
\clearpage
\input{/USERS/ERINHENGEL/Dropbox/Readability/draft/0-tex/generated/Table-5-FemSolo}
\clearpage
\input{/USERS/ERINHENGEL/Dropbox/Readability/draft/0-tex/generated/Table-6-FemSolo}
\clearpage
\input{/USERS/ERINHENGEL/Dropbox/Readability/draft/0-tex/generated/Table-7-FemSolo}
\clearpage
\input{/USERS/ERINHENGEL/Dropbox/Readability/draft/0-tex/generated/Table-8-FemSolo}
\clearpage
\input{/USERS/ERINHENGEL/Dropbox/Readability/draft/0-tex/generated/Table-F.2-FemSolo}
\clearpage


\subsection{Senior female author}
\label{appendixsenior}


\begin{vplace}[0.7]
\input{/USERS/ERINHENGEL/Dropbox/Readability/draft/0-tex/generated/Table-3-FemSenior}
\end{vplace}
\clearpage
\input{/USERS/ERINHENGEL/Dropbox/Readability/draft/0-tex/generated/Table-5-FemSenior}
\clearpage
\input{/USERS/ERINHENGEL/Dropbox/Readability/draft/0-tex/generated/Table-6-FemSenior}
\clearpage
\input{/USERS/ERINHENGEL/Dropbox/Readability/draft/0-tex/generated/Table-7-FemSenior}
\clearpage
\input{/USERS/ERINHENGEL/Dropbox/Readability/draft/0-tex/generated/Table-8-FemSenior}
\clearpage
\input{/USERS/ERINHENGEL/Dropbox/Readability/draft/0-tex/generated/Table-F.2-FemSenior}
\clearpage


\subsection{Majority female-authored}
\label{appendixmajority}


\begin{vplace}[0.7]
\input{/USERS/ERINHENGEL/Dropbox/Readability/draft/0-tex/generated/Table-3-Fem50}
\end{vplace}
\clearpage
\input{/USERS/ERINHENGEL/Dropbox/Readability/draft/0-tex/generated/Table-5-Fem50}
\clearpage
\input{/USERS/ERINHENGEL/Dropbox/Readability/draft/0-tex/generated/Table-6-Fem50}
\clearpage
\input{/USERS/ERINHENGEL/Dropbox/Readability/draft/0-tex/generated/Table-7-Fem50}
\clearpage
\input{/USERS/ERINHENGEL/Dropbox/Readability/draft/0-tex/generated/Table-8-Fem50}
\clearpage
\input{/USERS/ERINHENGEL/Dropbox/Readability/draft/0-tex/generated/Table-F.2-Fem50}
\clearpage


\subsection{At least one female author}
\label{appendixminority}


\begin{vplace}[0.7]
\input{/USERS/ERINHENGEL/Dropbox/Readability/draft/0-tex/generated/Table-3-Fem1}
\end{vplace}
\clearpage
\input{/USERS/ERINHENGEL/Dropbox/Readability/draft/0-tex/generated/Table-5-Fem1}
\clearpage
\input{/USERS/ERINHENGEL/Dropbox/Readability/draft/0-tex/generated/Table-6-Fem1}
\clearpage
\input{/USERS/ERINHENGEL/Dropbox/Readability/draft/0-tex/generated/Table-7-Fem1}
\clearpage
\input{/USERS/ERINHENGEL/Dropbox/Readability/draft/0-tex/generated/Table-8-Fem1}
\clearpage
\input{/USERS/ERINHENGEL/Dropbox/Readability/draft/0-tex/generated/Table-F.2-Fem1}
\clearpage


\subsection{Exclusively female-authored}
\label{appendixexclusive}


\begin{vplace}[0.7]
\input{/USERS/ERINHENGEL/Dropbox/Readability/draft/0-tex/generated/Table-3-Fem100}
\end{vplace}
\clearpage
\input{/USERS/ERINHENGEL/Dropbox/Readability/draft/0-tex/generated/Table-5-Fem100}
\clearpage
\input{/USERS/ERINHENGEL/Dropbox/Readability/draft/0-tex/generated/Table-6-Fem100}
\clearpage
\input{/USERS/ERINHENGEL/Dropbox/Readability/draft/0-tex/generated/Table-7-Fem100}
\clearpage
\input{/USERS/ERINHENGEL/Dropbox/Readability/draft/0-tex/generated/Table-8-Fem100}
\clearpage
\input{/USERS/ERINHENGEL/Dropbox/Readability/draft/0-tex/generated/Table-F.2-Fem100}
\clearpage


\subsection{Senior female author, sample of less experienced authors}
\label{appendixjunior}


\begin{vplace}[0.7]
\input{/USERS/ERINHENGEL/Dropbox/Readability/draft/0-tex/generated/Table-3-FemJunior}
\end{vplace}
\clearpage
\input{/USERS/ERINHENGEL/Dropbox/Readability/draft/0-tex/generated/Table-5-FemJunior}
\clearpage
\input{/USERS/ERINHENGEL/Dropbox/Readability/draft/0-tex/generated/Table-6-FemJunior}
\clearpage
\input{/USERS/ERINHENGEL/Dropbox/Readability/draft/0-tex/generated/Table-7-FemJunior}
\clearpage


\input{draft-footer}

\end{document}
