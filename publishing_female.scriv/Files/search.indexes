<?xml version="1.0" encoding="UTF-8"?>
<SearchIndexes Version="1.0">
    <Documents>
        <Document ID="33922920-97C7-4AB0-9891-78C3EB391B19">
            <Title>New Folder</Title>
        </Document>
        <Document ID="013663EA-2D88-427D-BC6E-5084AC67FE10">
            <Title>Bargaining power</Title>
            <Text>The following section sketches model conclusions when the entrepreneur and creditor share bargaining power during debt renegotiations. Assume first bargaining power is allocated entirely to the entrepreneur. Creditors accept any debt contract that is no worse than their expected returns in formal bankruptcy; entrepreneurs therefore offer a revised debt contract $\wt D$ that solves
$$!e[p1equation14]\ol C_1^W=\ol C_1^B.e!$$
Similarly ,when creditors have all the bargaining power, entrepreneurs accept any workout proposal that gives them at least as much as their expected earnings in formal bankruptcy. Now, $\wt D$ is the solution to
$$!e[p1equation15]\ol E_1^W=\ol E_1^B.e!$$
When entrepreneurs and creditors share bargaining power, each extracts a proportion of the surplus from [](#p1equation14) and [](#p1equation15), respectively. To illustrate the concept, I introduce a new variable, $z$, where $z$ is a number between zero and 1. In this framework, the new debt contract, $\wt D$, solves
$$!e[p1equation16]z(\ol E_1^W-\ol E_1^B)=(1-z)(\ol C_1^W-\ol C_1^B).e!$$
 $z=0$ grants the entrepreneur all bargaining power; $z=1$ grants it to the creditor; $z\in(0,1)$ permits sharing workout surplus. To simplify exposition, [](#p1equation17) expresses [](#p1equation16) in terms of creditor returns, only.
$$!e[p1equation17]\ol C_1^W=C_1^B+z(\ol V_1^C-\ol V_1^B).e!$$
If a settlement is reached, the creditor prefers liquidation to a workout if and only if $\ol C_1^W\le C_1^L$; plugging in [](#p1equation17), the condition is
$$!e[p1equation18]\ol C_1^B+z(\ol V_1^C-\ol V_1^B)\le C_1^L.e!$$
The entrepreneur prefers liquidation to a workout if and only if $\ol E_1^W\le E_1^L$, or equaivalently
$$!e[p1equation19]C_1^L\le C_1^B+z(\ol V_1^C-\ol V_1^B)-(\ol V_1^C-V_1^L).e!$$
$\ol V_1^C\le V_1^L$ is a necessary condition for both [](#p1equation18) and [](#p1equation19) to hold simultaneously; thus, for no $z\in[0,1]$ are viable firms ever inefficiently liquidated in a settlement. Since $\ol C_1^W$ is increasing in $z$, creditor losses from default decline---as do credit rationing and the cost of debt.
For non-viable firms, a necessary and sufficient condition for $\wt D$ to exist is for returns in continuation to exceed creditors' expected returns in bankruptcy plus their bargaining surplus awarded during renegotiations:
$$!e[p1equation20]\ol C_1^B+z(\ol V_1^C-\ol V_1^B)\le \ol V_1^C.e!$$
 Because $\ol C_1^B$ is increasing in $D$, $\ol C_1^B\le\ol V_1^B$. Thus a sufficient (but not *necessary*) condition for [](#p1equation20) to be satisfied is
$$!e[p1equation21]\ol V_1^B\le \ol V_1^C,e!$$
 *i.e.*, the same condition from [](#prop1).
Consider the set of firms that violate [](#p1equation21). As $z$ increases, the left-hand side of [](#p1equation20) declines. Workouts more likely exist. Because time 0 claims by creditors that they will have no choice but to file for bankruptcy are less credible, entrepreneurs are less willing to accept a loan contract with an exhorbitant $D$ per [](#prop2)(ii). The profitable nice lending market [market](#cor1) shrinks. When creditors have all the bargaining power ($z=0$), [](#p1equation20) collapses to $\ol C_1^B\le\ol V_1^B$---which is always satisfied. [](#prop2)(i) applies to all firms; no firm goes bankrupt and there are no supranormal profits in lending.</Text>
        </Document>
        <Document ID="575DB6AE-23C1-4C53-94DC-C1976F581540">
            <Title>Untitled</Title>
        </Document>
        <Document ID="3B5F4B47-602B-4113-947F-9B3824E55EC7">
            <Title>Mixed logit regression</Title>
            <Text>Credit information's ultimate impact depends on firm viability, which is not identified. [](#p2table7) suggests an overall negative effect, but it is small and insignificant. To provide more nuance, I use a mixed logit regression.
Mixed logit models were developed to address unobserved heterogeneity in consumer choice experiments. They permit random taste variation in survey respondents presented with a series of alternatives over several choice occasions.
Despite not corresponding exactly to surveys that usually exploit mixed logit models, my dataset does contain many similar features. That is, although the same firm is never repeatedly sampled, the same type of firm is. Additionally, a particular type of firm should not only be repeatedly sampled in one country, but across countries facing different levels of credit information and contract enforcement costs. 
To apply the model, I decompose the dataset into groups. Each group contains firms that live in a country with similar GDP per capita and bankruptcy legislation and share the same approximate age, industry, size and export, foreign- and state-ownership status. Credit information and contract enforcement cost were averaged over group members that considered financing an obstacle, and separately for those that did not. 
The result is 389 “individuals” with a choice of two alternatives---is finance an obstacle or not? The differentiating factors are credit information and contract enforcement cost. As illustrated in [#Train2009;], by assuming the individual-specific random error component is distributed i.i.d. extreme value, [](#p2equation6) represents the probability $\text{obstacle}_i$ is $k$:where $k$ is 0 or 1, $\vect B$ is a $2\times1$ vector of normally distributed coefficients, $\phi$ their density and $\vect x_{i_k}$ individual $i$'s $2\times1$ vector of explanatory variables for choice $k$. 
[](#p2table8)presents the results. Columns (1) and (2) display means and standard deviations for $\vect B$. (3) shows results from a comparable logit regression. The gulf between coefficients in the mixed logit and standard logit and [probit](#p2table7) models suggests considerable unobserved heterogeneity.
$Y$ is again positive and significant, indicating quick and efficient reorganisation reduces financing obstacles. In the first result set, the coefficient follows a normal distribution; in the second, it is held fixed. The mean is the same regardless of specification. The standard deviation in (1) is close to zero and not significant, implying a very consistent impact throughout the population.
The coefficient on $p$ follows a normal distribution in both mixed logit regressions. Their means are about -28, meaning information considerably mitigates obstacles to finance for the average firm. Standard deviations are also roughly equivalent and significant. Their magnitude indicates substantial variation. Seventy percent of firms think credit information reduces obstacles to finance; for 30 percent, the opposite is true.
The ambiguous effect in [](#p2table8) is due to unobserved heterogeneity. [](#prop5)(ii) proposes one potential cause---firm viability in the context of court supervised reorganisation---but results in [](#p2table8) cannot directly attribute it to any specific factor. In conjunction with [](#p2table7), however, they do suggest the positive relationship is specific to jurisdictions without receivership, as theorised in [](#hypoth3).</Text>
            <Comments>GDP per capita and age were subdivided into two and three equal frequency groups, respectively. Bankruptcy legislation was subdivided into those countries with and without receivership. Industry refers to whether the firm operates in the manufacturing industry or not.
\input{$PPATH/p2/equations/p2equation6}
\input{$PPATH/p2/tables/tex/p2table8}
In a standard logit model, $\phi(\vect B)$ is 1 for a single parameter set and 0 for all others; [](#p2equation6) collapses to the term in parentheses---*i.e.*, the logit probability evaluated at fixed $\vect B$.
Errors in the latter two incorporate parameter variation and thus normalise coefficients by a larger number[see\]\[][#Revelt1998].
$\Phi(-b_n/s_n)$, where $b_n$ and $s_n$ are the estimated mean and standard deviation of the $n\text{th}$ coefficient, respectively.</Comments>
        </Document>
        <Document ID="1725448A-E244-4906-B0E0-8215E1C00505">
            <Title>Data</Title>
        </Document>
        <Document ID="2AA5D637-7E4D-47EC-B6D7-24B613D651E2">
            <Title>Studies evaluating gender differences in acceptance rates</Title>
            <Text>&lt;!--
\input{$HOME/Dropbox/Readability/draft/0-tex/fixed/Table-I.1}
\clearpage
--&gt;</Text>
        </Document>
        <Document ID="5D741475-37DD-4CE3-AE49-EDCF1CD1CA81">
            <Title>Thesis</Title>
        </Document>
        <Document ID="00DDAE17-DE91-4C67-AB69-8B560BC180D6">
            <Title>Outcome.</Title>
            <Text>High bankruptcy costs weaken creditors' ability to solicit fairer workouts. From [](#lem2), however, workouts are no worse than bankruptcy and liquidation may be better. If the former exists, both parties (weakly) prefer to settle; otherwise, insolvent firms sometimes go bankrupt ([](#prop1)).
PROPOSITION
Insolvent projects choose between voluntary liquidation and: (i) a workout if $\,\ol V_1^B\le\ol V_1^C$; (ii) bankruptcy, otherwise.
eprop
Without reorganisation, viable firms optimally continue---and per [](#prop1)(i), this is precisely what they do. Liquidation cannot beat an accepted workout and creditors never reject proposals by viable firms.
Non-viable firms optimally liquidate---but their owners sometimes insist on workouts that their lenders don't refuse. Workouts enable entrepreneurs to extract deviations from absolute priority. How much depends on creditor earnings in bankruptcy. When those earnings are low, creditors tolerate larger write-offs. Thus, entrepreneurs demand a premium to liquidate: the project's piecemeal value must offset the "haircut" debt holders concede in a workout (see [](#p1figure1)).
But workouts aren't always available. Sometimes creditor gains from a possible liquidation outweigh the potential cost of an expensive reorganisation---making bankruptcy a worthwhile gamble. And sometimes, time 1 earnings are so low that entrepreneurs fight liquidation no matter how inefficient continuation may be. The firm goes bankrupt.</Text>
            <Comments>\input{$PPATH/p1/figures/tex/p1figure1}</Comments>
        </Document>
        <Document ID="6E035480-E9AF-4C01-8570-5ED522C3159C">
            <Title>Appendices</Title>
        </Document>
        <Document ID="3D65E866-13F8-4094-BE2C-3E8254E8226D">
            <Title>Co-variate balance</Title>
        </Document>
        <Document ID="49E5CAE9-D36E-4B00-8744-4B194A81892A">
            <Title>Model</Title>
        </Document>
        <Document ID="15BED637-BE8D-43E5-BEB7-7D57E209101E">
            <Title>[](#table6), including quality/productivity controls</Title>
            <Text>To remain consistent with a similar regression in [](#table11), [](#table6) does not include author productivity effects. [](#tableD12) reproduces [](#table6) including these effects. Coefficients and standard errors vary little from those in [](#table6).
&lt;!--\input{$PPATH/tables/tex/tableD12}--&gt;</Text>
            <Notes>&lt;!--\clearpage--&gt;</Notes>
        </Document>
        <Document ID="92C41DA9-4B7F-4A75-9ED9-5FE82D5E7B71">
            <Title>Readability scores</Title>
        </Document>
        <Document ID="386CE913-CCE2-422B-B8C0-02B731120203">
            <Title>Explanations</Title>
            <Text>[](#p3table12) documents a rise in the readability gap as women publish more articles. [](#p3eqSUP5) is an author-level estimation of the impact publication counts have on men and women's readability; the positive differences observed in [](#p3table13) therefore point to individual-specific explanations driving this phenomenon.
One such explanation is "learning-by-doing". If the payoff from lucid exposition is high, people will catch on---either by internalising explicit comments on text readability in referee reports from earlier papers or making the (un)conscious connection that review times are faster when text is clearer. Applying that payoff only to women yields a succinct explanation for the gap's observed growth.
But there are two convincing alternatives---one a complement, the other a substitute to "learning-by-doing". I now investigate both and show that neither is likely.
Consider first the substitute. As discussed in [][p3authorlevel], readability and female ratio may be nonlinearly related---specifically, evidence suggests the latter is increasing and convex in the former (see [](#fn3) and the Online Appendix). Thus, if women are more likely to co-author with other women (or by themselves) when they already have several publications behind them, the observed readability increase would merely reflect that late-career concentration.
This is not the case. [](#p3table16)displays male and female marginal effects on the number of prior papers from a fixed effects regression on female ratio. As publication counts increase, genders diversify: men publish with more women and women publish with more men.
The complementary explanation applies to the evolution of female writers as a group---perhaps the clearest women publish more often. [](#p3equation9) investigates:where $\Phi$ is the standard normal cumulative distribution function, $T_i\ge x$ equals one if author $i$'s total publication count $T_i$ is greater than or equal to the positive integer $x$ and subscript 1 denotes author $i$'s first paper. [](#p3table14)displays $\Phi^\prime\cdot\l(\beta_2+\beta_4\r)$---the marginal effect of female authors' first papers' readability scores on the probability of publishing multiple times.
The readability of a woman's first paper does not predict how often she will publish. Regardless of the threshold chosen, it has no impact on her eventual productivity and is indistinguishable from zero in all estimates. Several alternative dependant variables, independent variables, functional forms and dataset restrictions were tried. None suggest initially good writers, male or female, publish more frequently than bad ones (see Online Appendix).</Text>
            <Comments>Another related explanation is that women are more responsive to referee reports. This and other explanations are addressed in [][Discussion].
\input{/Users/erinhengel/Desktop/tables/tex/p3table16}
\input{/Users/erinhengel/Desktop/equations/p3equation9}
\input{/Users/erinhengel/Desktop/tables/tex/p3table14}</Comments>
        </Document>
        <Document ID="17818161-DA57-4FCE-8C30-68D0C68AC96D">
            <Title>Time-cost theoretical framework</Title>
            <Text>Consider one male and one female researcher, each of whom produces a paper of quality $X$. The male researcher used strategy $x_m$ to achieve quality $X$; the female researcher used strategy $x_f$ to achieve the same $X$. The male researcher is rewarded for his effort with an acceptance rate $a_m(x_m,X)$; the female researcher is rewarded for her effort with the acceptance rate $a_f(x_f,X)$. The cost to each person of implementing their respective strategy is $c_m(x_m)$ and $c_f(x_f)$.
Suppose the reward-cost tradeoff to the male researcher of his strategy $x_m$ is greater than the reward-cost tradeoff to the female researcher of her strategy, $x_f$, *i.e.*,
&lt;!--\begin{equation}\label{cost-reward}
	\frac{a_m(x_m,X)}{c_m(x_m)}&gt;\frac{a_f(x_f,X)}{c_f(x_f)}.
\end{equation}--&gt;
 [](#cost-reward) implies either
&lt;!--\begin{equation}\label{reward}
	a_m(x_m,X)&gt;a_f(x_f,X)
\end{equation}--&gt;
 or
&lt;!--\begin{equation}\label{cost}
	c_m(x_m)&lt;c_f(x_f),
\end{equation}--&gt;
 or both.
[](#reward) is the definition of higher standards---*i.e.*, the publication strategy women employ generates a lower reward than the publication strategy men employ even though both strategies result in the exact same $X$.
[](#cost) implies that the women's strategy is costlier to implement even though it generates the same quality as the man's strategy. In other words, she has to exert more effort to produce work that is the same quality as the man's---*i.e.*, she is a less capable researcher.
&lt;!--\clearpage--&gt;</Text>
            <Comments>Note that higher standards could be the result of always accepting male-authored papers more often than female-authored papers, conditional on strategy (and $X$): $$a_m(x,X)&gt;a_f(x,X),$$ or rewarding men's strategy more even though it generates the same quality as women's strategy, $$a(x_m,X)&gt;a(x_f,X),$$ or both.
Similarly, higher costs could imply that men's costs are always lower for a given strategy, *i.e.*, $$c_m(x)&lt;c_f(x),$$ or $x_f$ is costlier to implement for both parties, *i.e.*, $$c(x_m)&lt;c(x_f),$$ or both.</Comments>
        </Document>
        <Document ID="F9E2FC29-2F12-4D32-8B72-A2BECBF61F58">
            <Title>Meta-Data</Title>
            <Text>The wider gap post-peer review suggests causality with peer review. In this section, I investigate if and how higher standards for women are a contributory factor. To help organise the discussion, I develop a stylised model of readability's marginal impact on the editorial decision making process.</Text>
        </Document>
        <Document ID="E8C2F9DA-7160-4BE1-8B89-4DA8E012BE0D">
            <Title>Front Matter</Title>
        </Document>
        <Document ID="A9CB7415-AD23-4D32-BC4E-8F3D95A31C57">
            <Title>[](#figure1), excluding *AER Papers &amp; Proceedings*</Title>
            <Text>Due to small samples of female authors, [](#figure1) includes 561 articles from *AER Papers &amp; Proceedings*. [](#figureD1) replicates its analysis excluding these observations. In addition to dropping *JEL* codes A (General Economics, Handbooks and Teaching), B (History or Economic Thought, Methodology and Heterodox Approaches), M (Business Administration and Business Economics; Marketing; Accounting; Personnel Economics) and P (Economic Systems) (see [](#footnote33)), [](#figureD1) excludes K (Law and Economics), N (Economic History), Q (Agricultural and Natural Resource Economics; Environmental and Ecological Economics), R (Urban, Rural, Regional, Real Estate, and Transport Economics) and Z (Other Special Topics): each has five or fewer observations that are both 100 percent female authored and assigned no more than two distinct *JEL* codes. Results are consistent with those in [](#figure1).
&lt;!--\input{$HOME/Dropbox/Readability/draft/figures/tex/figureD1}--&gt;</Text>
            <Comments>Articles are assigned, on average, two distinct primary *JEL* codes. A quarter of all articles are assigned three or more primary *JEL* codes; eight percent are assigned four or more; two percent are assigned five or more (up to a maximum of eight). Observations assigned five or more *JEL* codes are disproportionately clustered in the excluded codes.</Comments>
            <Notes>&lt;!--\clearpage--&gt;</Notes>
        </Document>
        <Document ID="B30AEF02-62BB-4CAF-A417-1D257EA9867E">
            <Title>Average first, mean and final paper scores</Title>
            <Text>[](#tableB1) displays authors' average readability scores for their first, mean and final papers. Grade-level scores (Flesch-Kincaid, Gunning Fog, SMOG and Dale-Chall) have been multiplied by negative one (see [][MeasuringReadability]). Sample excludes authors with fewer than three publications.
&lt;!--\input{$HOME/Dropbox/Readability/draft/tex/generated/tableB1}--&gt;
&lt;!--\clearpage--&gt;</Text>
        </Document>
        <Document ID="CA03D0ED-0104-442B-9711-F2B3AEB42E3D">
            <Title>Women's papers are more readable</Title>
            <Text>[](#table2) compares textual characteristics between male-authored papers (defined as having a ratio of female authors below 50 percent) and female-authored papers (defined as having a ratio of female authors at or above 50 percent). It suggests women write shorter, simpler sentences: they contain fewer characters, fewer syllables, fewer words and fewer "hard" words. Differences are highly statistically significant.
&lt;!--\input{$HOME/Dropbox/Readability/draft/0-tex/generated/Table-2}--&gt;
[](#table3_FemRatio) presents results from 45 separate ordinary least squares (OLS) regressions of readability scores on the ratio of female authors (papers with fewer than 50 percent female authors are classified as male, see [][Gender]). Column (1) includes journal and editor fixed effects and controls for blind review and its interaction with the ratio of female authors on a paper. Columns (2) and (3) add journal-year interaction dummies. Column (4) introduces controls for paper $j$'s number of co-authors ($N_j$) and the dynamic institution effects described in [][AppendixControls]. Column (5) adds a dummy variable capturing English fluency; it also controls for article quality (citations (asinh)), co-author prominence ($\text{max. }T$) and seniority at the time of publication ($\text{max. }t$). Columns (6)--(9) are estimated on the sample of articles published after 1990. (7) includes fixed effects for primary *JEL* categories. (8) replaces it with three binary variables meant to capture how theoretical vs. empirical a paper is. (9) includes fixed effects for tertiary *JEL* categories.
Results in [](#table3_FemRatio) suggest that abstracts written by women score about 1--2 points higher on the Flesch Reading Ease scale; according to the four grade-level measures, they take about 1--5 fewer months of schooling to understand. Percentage-wise, women write about 1--4 percent better than men.
&lt;!--\input{$HOME/Dropbox/Readability/draft/0-tex/generated/Table-3-FemRatio}--&gt;
[][AppendixJEL] explores field in more detail; [][AppendixAuthorLevel] analyses readability at the author-level. Conditional on other explanatory variables, I find little evidence that field drives results in [](#table3_FemRatio). After accounting for author-specific heterogeneity, the gender gap in readability rises to 2--6 percent.</Text>
            <Comments>The coefficients on the journal dummies in (2) are presented in [][AppendixArticleLevel]. Compared to *AER*, all five scores agree that *Econometrica* is harder to read; four out of five scores suggest *JPE* is, too, while *QJE* is easier.
&lt;!--\label{FootnoteAERpp}--&gt;Due to small sample sizes, column (9) includes 563 articles from *AER P&amp;P*, coded as a separate journal. Papers published in *AER P&amp;P* are selected and edited by the American Economic Association's president-elect with the help of a Program Committee (see [www.aeaweb.org](https://www.aeaweb.org/journals/pandp/about-pandp) for more details). *P&amp;P* does not publish abstracts in its print version; only select years (2003 and 2011--2015) and papers were available online when I collected the data. Excluding these articles does not impact results or conclusions: coefficients are similar to those in column (9), but standard errors are somewhat higher.</Comments>
        </Document>
        <Document ID="C9F7934E-6FC3-4EEC-8282-D201AFCD6308">
            <Title>Results</Title>
        </Document>
        <Document ID="E8DAE991-565B-41FD-BE39-B148D6E56713">
            <Title>Empirical strategy</Title>
            <Text>Creditor returns ($\ol C_0$) determine lending constraints. In countries with dual-chapter bankruptcy, $\ol C_0$ depends on the project-specific distribution of earnings---an unknown function possibly non-linear in both $Y$ and $p$. I rely on a second order Taylor expansion in both variables to approximate it:where $\ol C_{0i_{jt}}$ represents creditor returns when lending money to a firm $i$ operating in the country $j$ during the year $t$; $\vep_{i_{jt}}$ is the residual error term.
In countries with receivership, $\ol C_0=K_0$ in all states. According to [](#cor3), $Y$ and $p$ have no impact. To test this hypothesis, the following first-order expansion is sufficient:
$\ol C_{0i}^B$ and $\ol C_{0i}^R$ are unobserved but represented by the manifest variable $\text{obstacle}_i$. Firm $i$ perceives access to finance as an obstacle only when it has (or has had) difficulty obtaining a loan---which occurs when creditors expect negative returns by making [one](#prop2), *i.e.*,
[](#p2equation5) estimates the impact of $Y$ and $p$ on getting credit by combining [](#p2equation2), [](#p2equation3) and [](#p2equation4):where $B_{jt}$ is an indicator variable equal to one if country $j$ employed dual-chapter bankruptcy in year $t$ and zero, otherwise. $\vect X_{i_{jt}}$ is a vector of [firm](#p2table2) and [country](#p2table3) control variables and country and year fixed effects. $\vep_{i_{jt}}$ is the auxiliary random variable, assumed to follow a standard normal distribution with cumulative distribution function $\Phi$.</Text>
            <Comments>\input{$PPATH/p2/equations/p2equation2}
\input{$PPATH/p2/equations/p2equation3}
\input{$PPATH/p2/equations/p2equation4}
\input{$PPATH/p2/equations/p2equation5}</Comments>
        </Document>
        <Document ID="D381DEFB-0D28-48AE-A520-9886F09628F7">
            <Title>Junior women undergo the longest review</Title>
        </Document>
        <Document ID="98172269-46CB-4743-B9F8-470C64155500">
            <Title>Senior female author, sample of less experienced authors</Title>
            <Text>&lt;!--
\begin{vplace}[0.7]
\input{$HOME/Dropbox/Readability/draft/0-tex/generated/Table-3-FemJunior}
\end{vplace}
\clearpage
\input{$HOME/Dropbox/Readability/draft/0-tex/generated/Table-5-FemJunior}
\clearpage
\input{$HOME/Dropbox/Readability/draft/0-tex/generated/Table-6-FemJunior}
\clearpage
\input{$HOME/Dropbox/Readability/draft/0-tex/generated/Table-7-FemJunior}
\clearpage
--&gt;</Text>
        </Document>
        <Document ID="8B3CC743-4F0F-4EE0-B801-66683C9DF7E7">
            <Title>Contracts and credit</Title>
            <Text>Whatever the interim outcome, entrepreneurs' returns are aggregate earnings, $\ol V_0$, less the amount paid to creditors, $\ol C_0$, making their unconditional expected value
$$!e[p1equation2]\ol E_0=\ol V_0-\ol C_0.e!$$
The optimal contract maximises $\ol E_0$, subject to the constraint that creditors' anticipate at least the principal amount of their loan, *i.e.*, $K_0\le\ol C_0$. Given $\ol V_0$ and $\ol C_0$ are positive, it is obvious from [](#p1equation2) that $\ol E_0$ is increasing in the former and decreasing in the latter; it is less obvious, however, how each reacts to $D$.
Start with viable firms. Viable projects operate both periods whether solvent or not---expected aggregate earnings are constant, making $\p\ol V_0/\p D=0$. Solvent firms satisfy $D\le X_1$ so in *any* outcome creditors receive $D$, meaning $\ol C_1^B=D$. Consequently, $\ol C_0=\ol C_0^B$, where $\ol C_0^B$ is the unconditional expected value of creditor returns in bankruptcy. Since $\ol C_0^B$ is increasing in $D$, $\ol E_0$ must be decreasing in it. Entrepreneurs optimally set $D$ as low as possible: $\ol C_0=K_0$.
For non-viable firms it is no longer true that $\ol V_0$ is unaffected by $D$ or that $\ol C_0$ is always increasing in it. As shown in the proof of [](#lem3), the change in $\ol C_0$ with respect to $D$ is
$$!e[p1equation3]\frac{\p\ol C_0}{\p D} = \int_{\mc L}\!\frac{\p C_1^L}{\p D}\,\dd\hat\Pi+\int_{\mc R}\!\frac{\p\ol C_1^R}{\p D}\dd\hat\Pi + \frac{\p\ol V_0}{\p D},e!$$
 where $\mc L$ and $\mc R$ are the set of outcomes in which creditors accept payoffs consistent with liquidation and reorganisation, respectively, and $\hat\Pi$ their joint cumulative distribution function.
$\p\ol V_0/\p D$ is negative but $\p\ol C_1^R/\p D$ and $\p C_1^L/\p D$ are positive so $\p\ol C_0/\p D$ may be positive or negative. A larger $D$ increases creditors' claims to eventual earnings. This direct effect is the sum of the first two terms in [](#p1equation3). A higher $D$, however, also makes workouts and bankruptcy more attractive, depressing $\ol V_0$ and thereby indirectly reducing $\ol C_0$: the third term of [](#p1equation3).
Combine [](#p1equation3) with the partial derivative of [](#p1equation2) with respect to $D$, and $\p\ol E_0/\p D$ is simply the negative direct effect $D$ has on creditors' returns. Although $D$ affects entrepreneurs' decisions at time 1, and that decision affects expected project value at time 0, its cost is borne only by lenders so entrepreneurs ignore it and opt for the smallest $D$ they can; again, $\ol C_0=K_0$ ([](#lem3)).
LEMMA
The entrepreneur prefers the smallest $D$ such that $\ol C_0=K_0$.
elem
The main feature of the optimal contract is that every entrepreneur wants the cheapest loan he can get his hands on. He may have no choice---for viable projects $\p\ol C_0/\p D$ is positive meaning at most one $D$ exists such that $\ol C_0=K_0$. Possibilities are broader for non-viable projects. $\p\ol C_0/\p D$ is neither definitively positive or negative; it isn't necessarily even monotone. Consequently, the $D$ at which $\ol C_0=K_0$ may be on the upward or downward slope of $\ol C_0$ or even occur at several places ([](#p1figure2)).</Text>
            <Comments>Both $C_1^L$ and $\ol C_1^R$ are non-decreasing in $D$, making $\p \ol C_1^B/\p D$ non-negative.
There is one technical (and rather pedantic) exception: when $\ol C_0=K_0$ on a non-trivial closed interval.
\input{$PPATH/p1/figures/tex/p1figure2}</Comments>
        </Document>
        <Document ID="542D4130-D886-47DB-89FF-0A22139B8F8B">
            <Title>Endogenous collateral.</Title>
            <Text>Machines are generally available in several models, produced by more than one company and sold at various prices. Daily newspapers with circulation between 150,000--200,000 spend anywhere from $4--25 million on a press: cost depends on manufacturer reputation, post-purchase service quality, colour capacity and several additional features that reduce wastage, increase automation and speed up printing. Farm equipment is available in an even more dramatic range. John Deere's 2015 catalogue advertises 20+ tractors, priced $35,000--160,000. Models are differentiated by horsepower, warranty and special add-ons such as air conditioning, instrument panels, automatic transmission, bluetooth and surround sound speakers.
Differentiated products cater to diverse clientele. Larger newspapers need faster presses because they print more copies; farming in extreme heat makes air conditioned tractor cabs (almost) a necessity. It is possible, however, that a spectrum of products sold at various prices also relieves credit rationing caused by bankruptcy.
Assume the machine comes in a variety of models each of which is equally productive when employed by a specific project---utility tractors pick up just as many bales of hay whether equipped with subwoofers or not, and faster presses have little bottom-line impact when used by smaller newspapers. The price of a particular model is $\ul K_0\le K_0$, where $\ul K_0$ corresponds to the cheapest, or "base", model.
Every model retains its full value until time 1. After that, technological change causes the factors which differentiate models of the same machine to depreciate faster than the machine itself. Their value at time 2 is zero; every machine is worth $\ul K_2$ where $\ul K_2$ is the time 2 resale value of the base model.
PROPOSITION
When the product market for machines is sufficiently differentiated, no one is credit rationed. 
eprop
From [](#prop3), every project is funded. A sufficiently differentiated product market ensures a model exists that costs just enough to satisfy $\ol V_1^C&lt;\ol V_1^B$. Per [](#prop2)(ii), the entrepreneur who purchases this model guarantees his project is liquidated at time 1. Loaning him money is riskless, so he always gets credit.
Whether the entrepreneur actually buys an expensive model is a different question. Viable firms able to get credit for $\ul K_0$ choose the base model. Every other firm in every other circumstance, however, buys an expensive machine, *i.e.*, one that costs
$$\ol K_0=\max\l\{\ul K_0,(1-Y)(X_2^H+\ul K_2)\r\},$$
 where $X_2^H$ is the upper limit of $X_2$ ([](#cor2)).
COROLLARY
Viable firms choose $K_0=\ul K_0$ at $K_0\le D$ if creditors are willing to lend; all other firms choose $\ol K_0$ at $\ol K_0=D$.
ecor
Any credit rationed project viable at $\ul K_0$ won't get credit at any other $K_0$ that leaves [](#p1equation1) intact: [](#p1equation4) is decreasing in $K_0$ making loans harder to get as the machine's price rises. Since $\ul K_0$ was already the cheapest model, an entrepreneur's only option is to verifiably grant creditors more bargaining power during insolvency---by buying a machine expensive enough to reverse [](#p1equation1), converting a previously viable project into a newly non-viable one.
By "over-investing"---*i.e.*, purchasing a machine with greater capabilities (and a higher price tag) than what he actually needs---the entrepreneur fundamentally alters the relationship between his project's value in continuation and its assets' piecemeal resale value. Although his project now gets credit, he manages this only by shortening its lifespan.
Every entrepreneur of a non-viable firm---whether non-viable at $\ul K_0$ or because credit is only available if rendered non-viable---wants to subvert entirely his *ex interim* incentive to demand a workout. From [](#prop1), non-viable projects are sometimes operated a second period when time 1 earnings are low. But freedom at time 1 means a higher $D$ at time 0. *Ex ante*, its entrepreneur still prefers $D$ as small as possible ([](#lem3)); by extension he must want liquidation as probable as possible. Purchasing the expensive machine at $\ol K_0$ guarantees it.
$\ol K_0$ assures liquidation much like an exorbitant $D$ would per [](#prop2)(ii) but with one crucial difference: the entrepreneur retains control rights to $X_1$. By setting $D=\ol K_0$ the creditor recovers the loan principal in every state; the entrepreneur keeps all cash flows.
Over-investment by firms not viable at $\ul K_0$ contributes to a more socially desirable outcome. Entrepreneurs exploit the value of the machine at time 0 and its worth at time 2 to induce better behaviour at time 1: increasing $K_0$ reduces their desire to demand workouts or bankruptcy; by buying the model that costs $\ol K_0$ they eliminate it entirely.</Text>
            <Comments>For example, tractors aren't remarkably different from those produced in 1940 but today's audio systems are a big change from earlier gramophones. Similarly, according to [Kelley Blue Book](http://www.kbb.com), a standard equipped 2015 Toyota Camry SLE 4-door sedan currently costs 25.7 percent of the price of a standard equipped 2015 Mercedes-Benz S-Class S550 4-door sedan. That same Camry purchased in 2005, however, currently costs 89.9 percent of a comparable 2005 Mercedes-Benz S-Class. (Assumes all cars are in excellent condition.)
"Sufficient differentiation" in [](#prop3) required only that some $K_0$ exists such that $\ol V_1^C&lt;\ol V_1^B$. [](#cor2) applies only if "sufficient differentiation" includes a model which costs $\ol K_0$.</Comments>
        </Document>
        <Document ID="754D37AA-57F4-4150-AB74-190131E7CE56">
            <Title>Readability data coverage</Title>
            <Text>[](#table1) displays readability data coverage by journal and decade.
&lt;!--
\input{$HOME/Dropbox/Readability/draft/0-tex/generated/Table-B.1}
\clearpage
--&gt;</Text>
        </Document>
        <Document ID="F5274F0F-D718-4CC4-9BE7-CB3C55C847FC">
            <Title>Restud</Title>
            <Text>Although none of the other journals in my dataset print submit-accept times when they publish an article, another highly regarded economics journal does---the *Review of Economic Studies*. To verify that the same trends were present in that data as we see in *Econometrica*, I collected back to 1976---when the journal first began printing this information---and up to 2015. Regression results from this larger sample are shown in [](#table11). Included controls are identical to those shown in [](#table10) with the exception of institution fixed effects, editorial board fixed effects and controls for motherhood and childhood.
&lt;!--\input{$HOME/Dropbox/Readability/draft/tex/generated/table11}--&gt;
Gender gaps in review times among the *Restud* sample of articles only are between two and four months, so they appear to be somewhat smaller than they are at *Econometrica*. The average review time gap when observations from both journals are pooled is four to six months.</Text>
            <Comments>The other four journals either have a single editor in charge of all editorial decisions or are headed by a small editorial board that remains fairly constant over time. *Restud*, however, is headed by an editorial board made up of 7--8 members and its membership changes almost every year. As a result, fixed effects for each distinct editorial board are almost perfectly correlated with year of submission. For that reason, I do not include these controls in [](#table11). Nevertheless, including them leads to similar coefficients as those shown in [](#table11_FemRatio) and somewhat higher standard errors.</Comments>
        </Document>
        <Document ID="A7F250A5-8D54-4506-A83A-8AEAA205E166">
            <Title>dissertation</Title>
            <Text>LaTeX input: aer-style-article-header
Title: &lt;$title&gt;
My Short Title: &lt;$custom:ShortTitle&gt;
Author: &lt;$author&gt;
Affiliation: &lt;$custom:Affiliation&gt;
Email: &lt;$custom:E-mail&gt;
My Abstract: &lt;$synopsis&gt;
Thanks Bitches: &lt;$custom:Thanks&gt;
Base Header Level: 2
LaTeX input: aer-style-article-begin
LaTeX footer: aer-style-article-footer</Text>
        </Document>
        <Document ID="8DE86765-F692-4860-93CB-438B7EB12258">
            <Title>Publishing while Female</Title>
            <Synopsis>Using five well-known ``readability'' tests, I analyse every article abstract published in the top four economics journals since 1950. I) Abstracts written by women are 1--6 percent more readable than those by men. II) The gap is up to three times higher in published articles than in earlier, draft versions of the same papers. III) Women's writing gradually improves but men's does not---meaning the readability gap grows over authors' careers. I explore many interpretations; the simplest and most persuasive is that referees apply higher standards to women's writing, subjecting them to an added time tax. This last hypothesis is confirmed by submit-accept times at *Econometrica*: female-authored papers take six months longer to complete peer review.</Synopsis>
        </Document>
        <Document ID="3DEF56A7-A1F1-4651-8037-35C0D75764FB">
            <Title>Untitled</Title>
        </Document>
        <Document ID="18500215-43BF-4A40-9C02-0E4034ABF985">
            <Title>Senior female author</Title>
            <Text>&lt;!--
\begin{vplace}[0.7]
\input{$HOME/Dropbox/Readability/draft/0-tex/generated/Table-3-FemSenior}
\end{vplace}
\clearpage
\input{$HOME/Dropbox/Readability/draft/0-tex/generated/Table-5-FemSenior}
\clearpage
\input{$HOME/Dropbox/Readability/draft/0-tex/generated/Table-6-FemSenior}
\clearpage
\input{$HOME/Dropbox/Readability/draft/0-tex/generated/Table-7-FemSenior}
\clearpage
\input{$HOME/Dropbox/Readability/draft/0-tex/generated/Table-8-FemSenior}
\clearpage
\input{$HOME/Dropbox/Readability/draft/0-tex/generated/Table-F.2-FemSenior}
\clearpage
--&gt;</Text>
        </Document>
        <Document ID="BBEAE4B9-77E2-4D9A-9159-B37801964FD4">
            <Title>Theoretical strategy</Title>
            <Text>To understand the counterfactual analysis, note that $i$'s optimal choice of readability (defined in [](#eq:EU)) can be formulated as a binary decision problem. Specifically, $i$ initially faces the following choice: he can either ignore the impact readability has on his acceptance rate entirely and simply set $R_{it}$ so that marginal cost equals the marginal benefit of the intrinsic satisfaction he derives from writing readable papers (*i.e.*, $R_{it}=R_i^\star$ where $R_i^\star$ solves $c_i'(R)=\phi_i'(R)$); alternatively, he can chase after higher acceptance rates and set $R_{it}&gt;R_i^\star$. If the latter option is chosen, then $i$ optimally minimises $R$ conditional on acceptance rate---*i.e.*, he sets $R_{it}$ just equal to the readability threshold of the last review group he believes will accept his paper. Thus, $R_{it}=\widetilde R_i^{\bar s} + e_{it}$, where $e_{it}$ is the error in $i$'s time $t$ beliefs about $\widetilde R_i^{\bar s}$ and $\bar s$ is the toughest review group to accept $i$'s papers.
Suppose $i$ satisfies the assumptions and conditions of [](#Theorem1) relative to $k$ and assume that at time $t'$ $i$ and $k$ are sufficiently experienced in peer review to ensure that $e_{it}$ and $e_{kt}$: (i) are on a path converging to zero (*i.e.*, both getting closer and closer to zero each time $i$ and $k$ go through another round of peer review); and (ii) have already converged to one another (*i.e.*, $e_{it}=e_{kt}$). When these assumptions hold, [](#Corollary1) suggests a conservative measure of the impact external factors have on $i$'s time $t'$ readability choice. It is proved in [][AppendixProofs].
&lt;!--\input{$HOME/Dropbox/Readability/draft/0-tex/theorems/corollary}--&gt;
$D_{ik}$ represents a lower bound on the difference between $i$'s time $t'$ optimal readability score given he's held to higher standards than $k$ (*i.e.*, $R_{it'}$) and the readability score he would have chosen had be been subject to the same standards as $k$ (*i.e.*, $R\le\max\{R_{kt'},R_{it''}\}$). The intuition behind it is simple. First note that it is never optimal for $i$ to choose an $R$ less than $R_i^\star$. Since $i$ already chose $R_{it''}&lt;R_{it'}$ ([](#Theorem1)'s Condition 2), that must mean that $R_i^\star\le R_{it''}$. For all $R&gt;R_i^\star$, $i$ prefers to minimise $R$ conditional on acceptance rate, thus, $i$ prefers any $R\in[R_{it''},R_{it'})$ to $R_{it'}$ if it achieves the same acceptance rate as $R_{it'}$. Furthermore, from Condition 3, we know that $i$'s acceptance rate at $R_{it'}$ is identical to $k$'s acceptance rate at $R_{kt'}$. Suppose $R_{kt'}&lt;R_{it''}$. If $i$ and $k$ were subject to identical standards, then $i$'s acceptance rate a $R_{it'}$ would be the same as his acceptance rate at $R_{kt'}$ and therefore also the same as his acceptance rate at $R_{it''}$. Because it may be that $R_{kt'}&lt;R_i^\star\le R_{it''}$, $i$ does not necessarily prefer $R_{kt'}$ to $R_{it''}$, conditional on acceptance rate. We can, however, conclude that $i$ would, at the very least, prefer $R_{it''}$ to $R_{it'}$. Suppose $R_{it''}\le R_{kt'}$. As before, $i$'s acceptance rate at $R_{it'}$ is the same as $k$'s acceptance rate at $R_{kt'}$, but it is no longer necessarily the case that $i$'s acceptance rate at $R_{it''}$ is the same as $k$'s acceptance rate at $R_{kt'}$. Thus, if both were subject to the same standards, then $i$ would, at the very least, prefer $R_{kt'}$ to $R_{it'}$.</Text>
        </Document>
        <Document ID="C1E5A10A-F292-4EE1-A8AA-E29E36962741">
            <Title>Identification</Title>
            <Text>
 $\alpha_i$ represents author-specific time-independent preferences and costs ($u_i$, $\phi_i$, $c_i$); $\mu_{it}$ are the time-dependent confounding factors picked up by $\Pi_{0it}^s$ and $\Pi_{1it}^s$. $\vect X_{it}$ is a vector of observable variables that factor into referees' readability thresholds $\widetilde r_{0i}^s$ and $\widetilde R_i^s$. $\beta_1$ is the impact of taste-based discrimination. $\varepsilon_{it}$ is an ideosyncratic error that reflects the draw of referee group.
If author $i$ knew $(\beta_1,\bm\uptheta)$, then he knows $\widetilde r_{0i}^s$ and $\widetilde R_i^s$, so $\mu_{it}=0$. A fixed effects regression of [](#EquationXX1) would consistently estimate $\beta_1$. Unfortunately, he does not know this parameter vector. However, past some point, he *will*---or at least he's on enough of the right track so that with each subsequent paper the impact of $\mu_{it}$ gets smaller and smaller.
Assume that at time $t'$ $\mu_{it}&lt;\mu_{it'}$ for all $t&gt;t'$. Then the change in readability between times $t''&lt;t'$ is exactly proportional to the initial impact on readability of the time varying confounding factors,
&lt;!--\input{$PPATH/equations/equationXX1.tex}--&gt;
Because $\mu_{it'}&lt;\mu_{it''}$, a positive [](#EquationXX1) implies $\mu_{it''}&lt;0$. Thus, confounding factors at time $t''$ caused author $i$ to underestimate the parameter vector $(\beta_1,\bm\uptheta)$. At time $t'$ she has corrected her mistake so her readability is higher. This also implies that for $i$, acceptance rates are higher at $R_{it'}$ than at $R_{it''}$---if they were the same, then the fact that $i$ chose $R_{it''}$ earlier means she prefers that readability to $R_{it'}$, all thing equal. A higher acceptance rate is the only reason she's motivated to choose the higher readability at time $t''$.
The difference in readability between matched pairs, is the effect of taste-based discrimination ($\beta_1$) plus the degree to which their preferences differ:
&lt;!--\input{$PPATH/equations/equationXX2.tex}--&gt;
 Although it is is impossible to disentangle $\beta_1$ from $\Delta\alpha_{ik}$, as long as [](#EquationXXX1) and [](#EquationXXX2) are both positive then either of the following must be true: $$R_{it'}&gt;R_{kt'}\ge R_{it''}$$ or $$R_{it'}&gt;R_{it''}\ge R_{kt'}.$$ If it's the first case, then $i$ would chose $R_{kt'}$ if she could get the same acceptance probability as she does at $R_{it'}$ because she'd choose $R_{it''}&lt;R_{kt'}$ if she could get the same probability of acceptance at that level as she does at $R_{it'}$. Thus her probability of acceptance must be higher at $R_{it'}$ than it is at $R_{kt'}$. If the probability of acceptance for $k$ at $R_{kt'}$ is at least as high as it is for $i$ at $R_{it'}$ (Condition 1), then $\beta_1$ must be positive.
But not only must $\beta_1$ be positive, $R_{it'}-R_{kt'}$ *underestimates* it. If $R_{kt'}&gt;$R_{it''}$ then $\alpha_i&lt;\alpha_k$. Since $\vect X_i=\vect X_k$, $$R_{kt'}-R_{it''}=-\beta_1+\left(\alpha_k-\alpha_i\right)+\left(\mu_{kt'}-\mu_{it''}\right)+\left(\varepsilon_{kt'}-\varepsilon_{it''}\right).$$ As we've already established, $\mu_{it''}&lt;0$. Assuming $k$ is at least as well informed at time $t'$ than $i$ was at time $t''&lt;t'$ then $\mu_{kt'}$ is smaller in absolute value than $\mu_{it''}$ and their difference must be negative. Because we've already established that $\beta_1$ is positive, $R_{kt}&gt;R_{it''}$ only if $\alpha_i\le\alpha_k$. Thus, $R_{it'}-R_{kt'}$ is a lower bound on $\beta_1$.
On the other hand, if $R_{it'}&gt;R_{it''}\ge R_{kt'}$ then $i$ would choose $R_{it''}$ if she could get the same probability of acceptance at that level as she does at $R_{it'}$. Thus her probability of acceptance must be higher at $R_{it'}$ than it is at $R_{it''}$. If the probability of acceptance for $k$ at $R_{kt'}$, then again, $\beta_1$ must be positive.
=======================================
[](#Theorem1)'s three conditions are sufficient to determine its sign---and therefore if peer review is biased against $i$.
To see this, note that the change in readability between times $t''$ and $t'$ for fixed $i$ is exactly proportional to the initial impact on readability of the time varying confounding factors,
&lt;!--\input{$PPATH/equations/equationXX1.tex}--&gt;
When [](#EquationXX2) is positive (Condition 2), then $\mu_{it''}&lt;0$. At time $t''$ she did not know $\beta_0$, $\beta_1$ or $\theta$ as well as she does at time $t'$
This means that $i$'s increase in readability between times $t''$ and $t'$ was driven by a correction of their earlier mis-information. She writes better at time $t''$ only because earlier she though she could achieve a certain acceptance rate with a lower $R_{it}$; now she knows she needs a higher $R_{it}$ to get that same rate, on average.
$i$ and $k$ are accepted at the same rate at time $t''$ (Condition 1) and 
in [](#EquationXX2). Nevertheless, $i$ preferred the lower $R_{i1}$ when less experienced; she picks $R_{i3}&gt;R_{i1}$ not because she prefers to write more readability, but because experience has taught her this is exactly how readable her papers need to be to achieve the acceptance rate she desires. Assuming $i$ and $k$ are accepted at the same rate with $R_{k3}$ and $R_{i1}$, then if $R_{k3}&gt;R_{i1}$, $\alpha_i&lt;\alpha_k$, so $R_{i3}-R_{k3}$ underestimates $\beta_1$. 
===================================
the change in readability over time for a fixed $i$ is exactly proportional to the initial impact on readability of the time varying confounding factors
&lt;!--\input{$PPATH/equations/equationXX1.tex}--&gt;
 
Although it is impossible to generate an unbiased estimate of $\beta_1$, [](#Theorem1)'s three conditions are sufficient to ensure that $\beta_1$ is strictly positive---and therefore biased against $i$. Assuming $i$ and $k$ are accepted at the same rate, then peer review is biased against $i$ when both [](#EquationXX1) and [](#EquationXX2) are positive.
If [](#EquationXX1) is positive, then $\mu_{i1}&lt;0$---meaning the time-varying confounding factors initially put downward pressure on readability. As the author gained experienced, she realised her readability needed to be better, so that with sufficient experience (*i.e.*, $\mu_{i3]=0$) she writes more clearly than previously.

To control for factors that vary over $t$ in a way that may be correlated with gender---*e.g.*, journal, year, female ratio, *etc.*---I generated author-specific readability scores for fixed values of these variables. Specifically, I estimated [](#EquationXX) on female authors at $t=1$, female authors at $t=3$, male authors at $t=1$ and male authors at $t=3$.
Based the results, I predicted $\varepsilon_{it}$ and with it reconstructed $R_{it}$ at median $t=3$ values of $\text{year}_{it}$, $N_{it}$, $\text{inst. rank}_{it}$ and $\text{max. inst. rank}_{it}$ and $\text{female ratio}_{it}$ equals 0 for men and 1 for women.
</Text>
            <Comments>If her paper was published at $t''$, it was because she got a low draw of $\varepsilon_{it}$---*i.e.*, an $s\in\Sigma$ wish low readability thresholds.</Comments>
        </Document>
        <Document ID="26F43626-DC8D-4D5C-873E-3F3E457A59B2">
            <Title>Validity</Title>
            <Text>Advanced vocabulary and complicated sentences are the two strongest predictors of text difficulty[#Chall1995]. Hundreds of readability formulas exploit this relationship. The five most widely used, tested and reliable formulas for adult reading material are the Flesch Reading Ease, Flesch-Kincaid, Gunning Fog, SMOG (Simple Measure of Gobbledegook) and Dale-Chall[#DuBay2004]. Each are listed in [](#tab:formulas).
These five readability scores generally produce similar rankings: the yellow box plot in [](#figure2) summarises 169 inter-score correlations found in 26 studies; the median is 0.87. Moreover, they tend to correlate with (i) oral reading fluency, (ii) human judgement, (iii) reading comprehension tests and (iv) the cloze procedure. The dark blue box plots in [](#figure2) summarise 167 correlations in 38 published cross-validation studies. (See [][AppendixMetaAnalysis] for a list of studies included in the analysis.)
Furthermore, numerous studies have validated readability scores against surrogate measures of reading comprehension. More readable high school and college-level correspondence courses have higher completion rates[#Klare1973]. More readable academic journals enjoy larger readerships[#Richardson1977,Swanson1948]; their most readable articles win more awards[#Sawyer2008] and are downloaded more often[#Guerini2012]. More readable abstracts are also (generally) cited more frequently (see [#Dowling2018,McCannon2019;] and [](#figure2)). They are also more likely to be published in top-five and other higher ranking journals[#MarinoFages2020]. In a [blog post](http://lukaspuettmann.com/2017/12/09/voxeu-gobbledygook/), Lukas Püttmann compares abstract readability to page views of [VoxEU.org](http://www.voxeu.org) columns: more readable columns are viewed three percent more often[#Puttmann2017]. Evidence from other studies linking readability and citations is, however, weaker[#Lei2016, Berninger2017,Laband1992]. My own data suggest a positive relationship in papers published after 1990---and particularly those published post-2000---but no relationship before [that](#figure2).
Thanks to high predictive power and ease of use, readability formulas are widely employed in education, business and government. The U.S. Securities and Exchange Commission encourages clearer financial disclosure forms benchmarked against the Gunning Fog, Flesch-Kincaid and Flesch Reading Ease scores[#Cox2007]. The formulas have also guided readability assessments of, *inter alia*, standardised test questions[#Chall1977,Chall1983], medical inserts[*e.g.*,\]\[][#Wallace2008], technical manuals[*e.g.*,\]\[][#Hussin2012,Klare1973], health pamphlets[*e.g.*,\]\[][#Foster2002,Meade1989] and data security policies[#Alkhurayyif2017].
In research, readability scores are often used to proxy for "complexity". [#Enke2018;] controls for language sophistication using the Flesch Reading Ease formula in a study of moral values in U.S. presidential elections. [#Spirling2016;] employs the same score to show that British parliamentarians simplified speeches to appeal to less educated voters in the wake of the Great Reform Act. Legal research has found that judges are more reliant on legislative history when interpreting complex legal statutes, as measured by the Flesch-Kincaid formula[#Law2010]. In finance, the scores have linked clarity of financial communication to better firm and market financial health[#Li2008,Biddle2009,Jansen2011], larger investment and trading volume [#Miller2010,Thörnqvist2015,DeFranco2015,Lawrence2013] and lower demand for---albeit higher reliability of---outside research by sell-side analysts[#Lehavy2011]. See also [#Loughran2016;] for a review of the use of readability scores in finance and accounting research.
&lt;!--\input{$HOME/Dropbox/Readability/draft/0-tex/fixed/Figure-D.1}--&gt;</Text>
            <Comments>Oral reading fluency is generally measured as the number of words read aloud correctly per minute. The cloze procedure ranks passages of text according to average readers' ability to correctly guess randomly deleted words.</Comments>
        </Document>
        <Document ID="A93AC44A-82E5-4444-B6C0-9ADC63CB55A5">
            <Title>[](#table8_base), Condition 3</Title>
            <Text>[](#table8_acceptrate) estimates $D_{ik}$ with a rough attempt to control for acceptance rates---it requires $T_{i}\le T_{k}$ or $T_{k}\le T_{i}$ before categorising matched pairs as discrimination against $i$ or $k$, respectively. Conclusions from both tables are are similar to those presented in [][SEUMatching].
&lt;!--
\vfill
\input{$HOME/Dropbox/Readability/draft/tex/generated/table8_acceptrate}
\vfill
\clearpage
--&gt;</Text>
        </Document>
        <Document ID="65CC5F5E-498B-453A-8A20-433A92BAF329">
            <Title>Gender and readability, by *JEL* code</Title>
            <Text>[](#figureE1) displays results from an ordinary least squares regression on the Dale-Chall score; regressors are: (i) ratio of female co-authors (papers with fewer than 50 percent female authors are classified as male, see [][Gender]); (ii) dummies for each primary *JEL* code; (iii) interactions from (i) and (ii); (iv) controls for editor, journal, year, institution and English fluency; and (v) quality controls---citation count, $\text{max. }T$ fixed effects (author prominence) and $\text{max. }t$ (author seniority). Codes A, B, M and P are dropped due to insufficient number of female-authored papers. (Each had fewer than 10 papers authored only by women; no paper is classified under category Y.) Due to small samples---particularly of female authors---[](#figureE1) includes 563 articles from *AER Papers &amp; Proceedings*.
&lt;!--\input{$HOME/Dropbox/Readability/draft/0-tex/fixed/Figure-F.1}--&gt;
Points in [](#figureE1) reflect marginal effects across *JEL* classification; bars represent 90 percent confidence intervals from standard errors clustered by editor. The mean effect at observed *JEL* codes is 0.17 (standard error 0.054). This estimate coincides with results in [](#table3_FemRatio)---women's papers require about two fewer months of schooling to understand---and is highly significant.
Women earn higher marks for clarity in 12 out of 15 categories; only five are at least weakly significant: Q (Agricultural and Natural Resource Economics; Environmental and Ecological Economics), N (Economic History), G (International Economics/Finance), J (Labour Economics) and D (Microeconomics). Men may be better writers in L (Industrial Organisation), O (Economic Development, Innovation, Technological Change, and Growth) and H (Public Economics); none, however, are statistically different from zero. [](#figureE1)'s right-hand graph displays coefficients from interacting the ratio of female co-authors with each *JEL* code. N is (weakly) significantly above the mean; remaining categories are not statistically different from the mean effect.
In general, sample sizes are small and estimates imprecise---only Labour Economics and Microeconomics contain more than 100 papers written only by women (the others average 35). Nevertheless, [](#figureE1) suggests two things. First, the mostly insignificant interaction terms indicate outlier fields are probably not driving journals' gender readability gap---nor is any specific field bucking the trend. Second, the number of women in a field appears to have little effect on the size of the gap: Agriculture/Environment has one of the lowest concentrations of female-authored papers---but Economic History has one of the highest (Labour Economics falls between the two). Of course, Economic History papers are still overwhelmingly---as in 74 percent---penned just by men. But given the readability gap is present in subfields with both above- and below-average rates of sole female authorship, women may need to be better writers even where more of them publish.
&lt;!--\clearpage--&gt;</Text>
            <Comments>See [\]\[pp. 42--43][#Hengel2016;] for a version of  [](#figureE1) excluding *AER Papers &amp; Proceedings* articles.</Comments>
        </Document>
        <Document ID="60D4D28A-20D7-4895-A8AB-649C1D39F501">
            <Title>269 (alastair’s macbook's conflicted copy 2016-05-08)</Title>
            <Text>[](#p3table3) displays average characters, words, syllables, polysyllabic words and difficult words per sentence, broken down by sex. According to all measures, women write shorter, simpler sentences---they contain fewer characters, fewer syllables, fewer words and fewer "hard" words. All differences are highly significant.
[](#p3table4b) presents the estimated coefficients from an ordinary least square regression of an article's ratio of female co-authors on each of the five readability scores. Column (1) includes only the four journal dummies. It indicates abstracts written only by women score 1.27 points higher on the Flesch Reading Ease scale and take between 1--6 fewer months of schooling to understand according to the other four measures. Percentage-wise, women write between 1--3 percent better than men.
Column (2) includes 63 year dummies; column (3) adds another 182 journal and year interaction dummies; column (4) introduces the 98 institutional dummies. Controlling for time reduces gender's impact on the first four scores by 20--40 percent but exacerbates it by a third for the Dale-Chall score. Adding the institution dummies has little to no effect on the estimates. Coefficients and standard errors are very similar across columns (2), (3) and (4).
The coefficients on the journal dummy variables from (2) are presented in [](#p3table5).They contrast the readability of *Econometrica*, *JPE* and *QJE* with *AER* and provide a useful check on the reliability of the five scores. As intuitively anticipated, all five scores agree that *Econometrica* is harder to read; the Flesch Reading Ease and Dale-Chall scores suggest *JPE* and *QJE* are easier.
Columns 4 and 5 add controls for 20 primary *JEL* codes. Articles are usually assigned multiple codes---the average paper in the data has !!AvgJEL linked to it. To avoid subjectively choosing only one or randomly deleting data, each article is duplicated $n_j$ times, where $n_j$ is the number of primary *JEL* codes assigned to article $j$. Observation $j_k\in\{1,\ldots,n_j\}$ is allocated article $j\text{'s }k\text{th}$ *JEL* code and weighted by $1/n_j$.
Since classifications and institutions are only available for a subset of articles, columns 3--5 estimate [](#eq:p3eq1) on just 45, !!JELSubset and 16 percent of the data, respectively. Nevertheless, coefficients are roughly the same as those in column 2, albeit slightly higher---female-authored abstracts may be !!RangeColC percent better written.
[](#p3table5b) displays $\beta_c$ from estimating [](#eq:p3eq2)where $\text{score}_{j_k}^s$ is the $k\text{th}$ copy of readability score $s$ for article $j$, and subscript $c_{j_k}$ denotes article $j\text{'s}$ $k\text{th}$ *JEL* code.
According to at least one readability score, gender differences are less pronounced in subfields B (history of economic thought, methodology and heterodox approaches), F (international economics), H (public economics), M (business administration and business economics; marketing; accounting), O (economic development, technological change and growth) and R (regional, real estate and transportation economics).
Abstracts from subfield O are still better written when authored by women according to three of the five scores; marginal effects for B, F, H and R are not significantly different from zero. Weak evidence points to men as the better writers in subfield M: its Flesch-Kincaid score suggests female-authored abstracts take !!FleschKincaidM more years of schooling to understand (standard error !!FleschKincaidse); effects for the other scores are insignificant.
Papers classified as D (microeconomics), N (economic history) or Q (agricultural and natural resource economics; environmental and ecological economics) have more pronounced gender differences in readability---female-authored abstracts read !!DBetter, !!NBetter and !!QBetter times better than the average, respectively. All readability scores agree, are significant on their own and most are significantly higher than the mean effect.
In contrast to my expectation, more women in a field may not necessarily be an antidote to gender differences in readability. While D, N and Q are about average in terms of total female co-authors, they vary widely in how frequently their papers name only female co-authors. Q is in the !!QFemOnly, D the !!DFemOnly, N the !!NFemOnly. Admittedly, !!NWomen percent is itself pretty low; economic history papers are still overwhelmingly---as in !!NMen percent---penned just by men. But given the readability gap is present in subfields with both above- and below-average rates of sole female authorship, women may need to be better writers even where more of them publish.</Text>
        </Document>
        <Document ID="87359BED-D05B-4E6B-953E-158D57059402">
            <Title>Introduction</Title>
            <Text>In 1986 the U.K. introduced administration: bankruptcy procedures aimed at rehabilitating insolvent debtors. Called reorganisation elsewhere and analogous to U.S. Chapter 11, administration answered political concern that too many financially distressed firms were unnecessarily liquidated---or left to fail---despite a reasonable chance of survival[#Cork1982]. The "rescue culture" it fostered was thought to maximise profits, prevent job loss and uphold creditors' long-term interests.
In a comprehensive debt financing model, I show rescue culture neither maximises profits nor prevents job loss. Instead, it obstructs firm creation in the first place or supplants otherwise certain survival with upfront investment choices that make premature liquidation not just highly likely in financial distress but virtually guaranteed should the firm remain solvent. The only goal rescue culture does serve is creditors' long-term interests---by introducing a profitable niche lending market in an otherwise zero-profit industry.
My starting point is that administration is time-consuming, arduous and expensive. United Airlines spent $250 million on legal and professional services related to its reorganisation; WorldCom topped out at $620 million[#Berk2009]. In New York, procedures last three years[#Weiss1990] and fees eat up 2--4 percent of firm value[#LoPucki2004,Warner1977,Weiss1990]. Customers are reticent to buy goods[#Titman1984], suppliers hesitate to provide inventory and employees leave[#Berk2010]. Hungarian trustees delay liquidation[#Franks2013]; their counterparts in Russia embezzle cash[#Lambert-Mogiliansky2007].
These costs matter: they contribute to expensive debt, delayed liquidation, suboptimal investment and credit rationing. Limited liability makes creditors disproportionately responsible for reorganisation's impact on future earnings. Insolvent borrowers exploit the imbalance by demanding excessive debt write-downs in obvious violation to absolute priority. This cuts creditor earnings, discourages voluntary liquidation and sometimes renders lending unprofitable.
Insolvent firms that ought to continue always do---in settlements known as workouts. Firms with high expected earnings relative to upfront investment have the most to gain from a workout. So do their creditors. Reorganisation is expensive and probable: not only will equity never willingly liquidate, but if put to a judge she will likely agree. Bankruptcy has real consequences and creditors a real desire to avoid it; they quickly acquiesce to intemperate write-downs.
Potential borrowers lose. Workouts avoid wasteful reliance on an ineffective bankruptcy regime but lender write-downs are still proportional to the cost of that regime. As is often the case, however, it is firms' pre-borrowing selves and solvent peers who really pay the cost. For some, credit is merely more expensive. For others, it is rationed or obtained only by inducing premature liquidation. Higher interest rates encourage more workouts so creditors do not lend money no matter how much borrowers are willing to pay. To surmount this, firms over-invest in physical or tradable intangible assets to increase  both judicial likelihood of liquidation and a desire to voluntarily do so themselves.
Insolvent firms that ought to liquidate only sometimes do but nevertheless face fewer upfront repercussions in the lending market. Although they frequently continue when they shouldn't and intermittently even file for bankruptcy, their shorter expected lifespans mean equity usually forgo both to liquidate. From the creditor's perspective, workouts (and bankruptcy) are risker; returns are lower. They clearly prefer lending to borrowers who want them less often.
In fact---and despite a perfectly competitive credit market---lending to firms most prone to bankruptcy can actually be profitable. High interest rates encourage bankruptcy, sure, but when they are very high (and reorganisation very costly) it's no better than shutting down; entrepreneurs are indifferent so creditors liquidate…while pocketing all of the firm's initial earnings. If creditors only recuperate their original investment when liquidation is guaranteed,  an exorbitant rate achieves this and makes them money in the process.
These conclusions suggest radically altered bankruptcy procedures such as immediate auctions or distributed options may miss the point[see\]\[][#Baird1986,Bebchuk1988,Bebchuk2000,Aghion1992a]. All focus on rigidly respecting debt's seniority over equity via a legally structured sale or bargaining process. I show, however, that violations to absolute priority are only harmful once expensive reorganisation forces debt forgiveness beyond that which creditors would otherwise swallow given low initial earnings. Since it is hard to see how auction- or option-based reforms are cheaper than traditional bankruptcy---and their inflexible design may cost more to implement---they are probably not the panacea many hoped them to be.
Far more suitable is the reorganisation law they sought to replace: Chapter 11. I show that any particular bankruptcy regulation that neither implicitly reduces future earnings or explicitly prioritises equity over debt does not constrict lending or distort investment decisions. When it helps quickly resolve reorganisation and doesn't disempower creditors, lending is cheaper and more widespread, investment decisions more efficient.
Most rules in Chapter 11 meet these criteria: allowing management to remain, equity first right to a restructuring plan, super-senior emergency financing and U.S.-style "cramdown"---judicial imposition of a reorganisation plan despite creditor objection. They cost little to implement, do not categorically prioritise equity over debt and their absence (or the alternative) does not strengthen creditors' bargaining position. They do reduce time spent in reorganisation[#Elayan2001], discourage excessive risk-taking and under-investment[#Eberhart1993,Gertner1991] and encourage prompt notification of financial distress[#Povel1999].
Notoriously absent from Chapter 11 are several unambiguously harmful regulations that implicitly cost money. Replacing management with court-appointed administrators---as is done in France---expels specialised skills administrators probably lack. U.K. and German regulations that insist on administrative oversight burden firms with additional salaries that have priority over debt. British advertising requirements publicise financial distress, causing consumers concerned about warranty validity to shop elsewhere[#Titman1984] and employees worried about their jobs to find new ones[#Berk2010].
Chapter 11 may be the best reorganisation law, but its wider bankruptcy system is always inferior to one without an automatic stay. Automatic stays are not only redundant, but weaken creditors' bargaining position. Insolvent borrowers that should continue offer reasonable workout proposals unless creditors are likely the residual claimants in reorganisation. In the latter case, however, lenders' desire to maximise their own recovery impels action that also maximises the insolvent firm's value. Automatic stays therefore anticipate motives that creditors do not have. Eliminating them makes lenders' returns in bankruptcy less risky; insolvent borrowers impose fewer write-downs during workouts.
If the market for equity finance were as competitive as traditional lending is theorised to be, credit-constrained borrowers could turn to family and friends, initial public offerings or angel investors. But not everyone is blessed with wealthy kin, issuing public stock is just not done by tiny enterprises and illiquid venture capital markets favour entrepreneurs with whom investors share social networks and other superficial similarities[#Hochberg2007,Bottazzi2011,Verheul2001]. Thus, eliminating automatic stays---by permitting floating charge liens---is the most straightforward antidote to bankruptcy's unpleasant side effects. Despite claims by the[#Cork1982;], lenders are not prone to asset grabbing and firms are not left to fail[#Franks2005a]. Distressed British companies secured by floating charges were less likely liquidated than their counterparts in countries more committed to their survival[#Davydenko2008]. Bankruptcies involving floating charges keep firms as going concerns far more frequently than reorganisation---and cost significantly less to implement[#Djankov2008].
Yet floating charges are no longer part of British bankruptcy law. The Insolvency Act 1986 weakened them; the Enterprise Act 2002 removed them. Rescue culture won. Paradoxically at the expense of its purported aim: "to recognise that the effects of insolvency are not limited to the private interests of the insolvent, his family, creditors or directors, shareholders and employees, but that other interests of society or other groups in society are vitally affected…and to ensure that these public interests are recognised and secured"[\]\[§198(i)][#Cork1982].</Text>
            <Comments>U.K. law reserves "bankruptcy" for financially distressed individuals and "insolvency" for their corporate counterparts. I use both words per colloquial definitions: bankruptcy is the legal status and insolvency the state of not being able to pay one's debts.
Earlier models by[#Bebchuk2002;] and [#Longhofer1997;] identified deviations to absolute priority as key causes of credit-rationing.
This conclusion mirrors empirical findings that economically viable firms prefer workouts to Chapter 11[#Chatterjee1996].
[#Franks1994;] find that both creditor recovery rates and absolute priority deviations in favour of equity are much higher in distressed exchanges of publicly traded debt than in Chapter 11.
[#Jensen1976;] discussed adjudication costs in bankruptcy as contributing to declining firm value.
Empirical evidence suggests lenders charge higher interest rates, require shorter maturities and demand more collateral when weak creditor protections allow borrowers to extract steep deviations to absolute priority[#Qian2007].
[#Bebchuk2002;] similarly finds violations to absolute priority encourage inefficiently risky investment decisions. 
[#Chatterjee1996;] show empirically that Chapter 11 is dominated by bankrupt firms that should be liquidated.
Although this paper focuses on corporate insolvency, a similar phenomenon may have been partially responsible for excessive home loans made in the U.S. before 2007. Lenders offered mortgages at exorbitant rates to low income borrowers that virtually guaranteed default---but likely not before the borrower made one or more interest payments. Cumbersome personal bankruptcy procedures, low probability of success and high legal fees induce insolvent borrowers to immediately foreclose; the lender resells the home. When housing prices are rising (or at least not falling), this loan is effectively risk free but generates a rate of return above the risk-free interest rate.
Only legal fees incurred in bankruptcy have priority over debt repayment. No other Chapter 11 rule implicitly reduces future earnings or explicitly prioritises equity, suggesting that the structure of Chapter 11 probably isn't the main cause of absolute priority deviations and credit-rationing[#Bebchuk2002]; the massive fees inherent in the U.S. legal system are. Although wages have similar priority, their existence does not depend on bankruptcy. Workers must be paid regardless of solvency, so expected firm value is unaffected by a requirement to pay them first. [][p1AppendixSuperSeniorFinancing] shows that incurring losses (*e.g.*, paying wages in excess of available cash flow) before repaying debt does not affect lending and investment decisions.
An automatic stay is a court order that prevents confiscating collateral or collecting debt payments. It is the defining characteristic of reorganisation, since without it nothing legally prevents creditors from *de facto* liquidating an insolvent firm by seizing the assets it needs to operate.
It's not hard to guess who has the toughest time getting cash---of *Business Insider*'s "[50 Early Stage Investors in Silicon Valley You Need to Know](http://www.businessinsider.com/sv-angel-50-2012-7?op=1&amp;IR=T&amp;IR=T)", only two are female and only one black.
Floating charges are debt secured by an entire business. In the event of default, the creditor with a floating-charge lien is granted control rights of the firm with little judicial interference. An alternative simply removes the specific regulation referring to automatic stays. This may, however, enable borrowers to secure multiple loans with a single asset and exacerbate lender co-ordination problems[#Jackson1986,Baird1986].</Comments>
        </Document>
        <Document ID="489D7173-3DB1-4F9E-9617-148B45E330DC">
            <Title>Recovered Files (3 Dec 2017 at 16:59)</Title>
        </Document>
        <Document ID="99EA8132-17DD-487B-AD1E-32721974321C">
            <Title>269 (erin’s imac's conflicted copy 2016-05-01)</Title>
            <Text>[](#p3table3)displays each gender's average per sentence number of characters, words, syllables, polysyllabic words and difficult words. By all measures, women write shorter, simpler sentences---they contain fewer characters, fewer syllables, fewer words and fewer "hard" words. Differences are highly significant.
[](#p3table4)presents coefficients from an ordinary least squares regression of ratio of female co-authors on the five readability scores. To account for error correlation by editorial policy, observations are grouped by journal editor/editorial board and standard errors are adjusted accordingly.
Column (1) controls for journal: abstracts written only by women score about one point higher on the Flesch Reading Ease scale; according to the four grade-level measures, they take 1--6 fewer months of schooling to understand. Percentage-wise, women write between 1--3 percent better than men.
Column (2) includes 63 year dummies; column (3) adds another 182 journal and year interaction dummies; column (4) introduces the 98 institution dummies. Controlling for time and institution has little effect. Coefficients and standard errors are very similar to those in the first column.
The coefficients on the journal dummies in (2) are presented in [](#p3table5).They contrast the readability of the *American Economic Review* with *Econometrica*, *JPE* and *QJE*, providing a useful check on the reliability of readability formulas in the context of economic writing. As intuitively anticipated, all five scores agree that *Econometrica* is harder to read; the Flesch Reading Ease, Flesch-Kincaid, Gunning Fog and SMOG scores suggest the *JPE* is too; according to the Flesch Reading Ease, Gunning Fog, SMOG and Dale-Chall scores, *QJE* is easier.
Column (5) controls for primary *JEL* classification. Since only post-1990 *JEL* classifications are used, estimates in (5) exclude roughly 40 percent of the data. Nevertheless, coefficients are roughly equivalent---with the exception of the Flesch Reading Ease score which halves and is no longer significant at the 10-percent level.
[](#p3figure4)displays the results from an ordinary least squares regression on the Dale-Chall score; regressors are: (i) ratio of female co-authors; (ii) dummies for each primary *JEL* code, (iii) interactions from (i) and (ii) and (iv) controls for journal, year, institution and editorial board. Due to small sample sizes---particularly of female authors---[](#p3figure4) includes 549 articles from *AER Papers &amp; Proceedings*.
The pink vertical line in the left-hand graph is the mean marginal effect of the ratio of female co-authors: in line with estimates from [](#p3table4), female-authored papers require about six fewer weeks of schooling to understand and the estimate is highly significant (standard error 0.04). Points on the graph are marginal effects of the ratio of female co-authors across *JEL* classification; bars represent 90 percent confidence intervals (standard errors clustered by editorial board). Women ear higher marks for readability in 12 out of 15 categories; but only three are significant on their own: Q (Agricultural and Natural Resource Economics; Environmental and Ecological Economics), N (Economic History), and J (Labour Economics). In no field do men write significantly better than women.
[](#p3table4)'s right-hand graph displays coefficients on the interactions between ratio of female co-authors and each *JEL* code. Q and N are significantly above the mean; O (Economic Development, Innovation, Technological Change, and Growth) and H (Public Economics) are significantly below it. The effect of ratio of female co-authors in the other categories does not differ from the mean effect.
In general, sample sizes are small and estimates are are highly imprecise---only Labour Economics and Microeconomics contain more than 100 papers written only by women; the others average 35. Nevertheless, [](#p3figure4) suggests the gender gap in readability is not being driven by an outlier field.
[](#p3figure4) also suggests that more women in a field is not necessarily an antidote to gender differences in readability. Economic history has one of the highest concentrations of female-authored papers; Agriculture/Environment one of the lowest; Labour Economics falls between the two. Admittedly, Economic History papers are still overwhelmingly---as in 74 percent---penned just by men. But given the readability gap is present in subfields with both above- and below-average rates of sole female authorship, women may need to be better writers even where more of them publish.
To investigate the gender readability gap at the author-level, I disaggregated the data by duplicating each article $n_j$ times, where $n_j$ is article $j$'s number of co-authors, assigned observation $j_k\in\{1,\ldots,n_j\}$ article $j$'s $k\text{th}$ author and estimated [](#p3eqSUP6):where $\text{score}_{j_{it}}^s$ is readability score $s$ for article $j$---the article corresponding to author $i$'s $t$th publication. Gender enters twice via the binary variable $\text{male author}_i$ and $\text{ratio of female co-authors}_{j_{it}}$ to account for author $i$'s sex (zero for women, one for men) as well as the sex of his co-authors, respectively. $\text{No. co-authors}_{j_{it}}$ is article $j$'s total number of co-authors and controls for author $i$'s proportional contribution to his $t$th paper. $\alpha_i$ are author-specific effects, $\vect X_{j_{it}}$ is a vector of editor, journal, year and institution dummies and $\varepsilon_{j_{it}}$ is an i
[](#p3table6)displays results. The first panel estimates [](#p3eqSUP6) using a Thanks to short panels---again, particularly for women---the sample includes 549 articles from *AER Papers &amp; Proceedings* (see [](#fn1)). To account for duplicated observations, the regression is weighted by $1/n_j$. Standard errors adjusted for two-way clustering at the editorial board and author levels.</Text>
        </Document>
        <Document ID="AD3EFF8F-2949-4CF4-ABE8-69514717A082">
            <Title>Citations</Title>
            <Text>I use article citations from [Web of Science](https://login.webofknowledge.com/error/Error?Error=IPError&amp;PathInfo=%2F&amp;RouterURL=https%3A%2F%2Fwww.webofknowledge.com%2F&amp;Domain=.webofknowledge.com&amp;Src=IP&amp;Alias=WOK5). Unless otherwise mentioned, citation counts are transformed using the inverse hyperbolic sine function (asinh).</Text>
        </Document>
        <Document ID="175B78D7-2A1B-4F7D-8780-739D8F2651D6">
            <Title>At least one female author</Title>
            <Text>&lt;!--
\begin{vplace}[0.7]
\input{$HOME/Dropbox/Readability/draft/0-tex/generated/Table-3-Fem1}
\end{vplace}
\clearpage
\input{$HOME/Dropbox/Readability/draft/0-tex/generated/Table-5-Fem1}
\clearpage
\input{$HOME/Dropbox/Readability/draft/0-tex/generated/Table-6-Fem1}
\clearpage
\input{$HOME/Dropbox/Readability/draft/0-tex/generated/Table-7-Fem1}
\clearpage
\input{$HOME/Dropbox/Readability/draft/0-tex/generated/Table-8-Fem1}
\clearpage
\input{$HOME/Dropbox/Readability/draft/0-tex/generated/Table-F.2-Fem1}
\clearpage
--&gt;</Text>
        </Document>
        <Document ID="DAFC1B78-BEAA-4FDB-BCFD-EDC5F3230177">
            <Title>$\widehat R_{it}$ regression output</Title>
            <Text>[](#Rit_regresults) displays output from time- and gender-specific regressions used to generate [$\widehat R_{it}$](#equation14).
&lt;!--
\input{$HOME/Dropbox/Readability/draft/0-tex/generated/Table-J.3}
\clearpage
--&gt;</Text>
        </Document>
        <Document ID="69BF07BE-F69B-411E-AA52-CEA9BE2D00E9">
            <Title>[](#table8), male effects</Title>
            <Text>[](#tableC7) shows male effects from the regressions described and presented in [](#table8). Effects estimated at a female ratio of zero and observed values for other co-variates. Grade-level effects (Flesch-Kincaid, Gunning Fog, SMOG and Dale-Chall) have been multiplied by [negative one][MeasuringReadability].
&lt;!--\input{$HOME/Dropbox/Readability/draft/tex/generated/tableC7}--&gt;</Text>
        </Document>
        <Document ID="21787956-28FB-4804-A99E-9DE66DFC8066">
            <Title>Subfield</Title>
            <Text>[](#p3table5b) displays $\beta_c$ from estimating [](#eq:p3eq2)where $\text{score}_{j_k}^s$ is the $k\text{th}$ copy of readability score $s$ for article $j$, and subscript $c_{j_k}$ denotes article $j\text{'s}$ $k\text{th}$ *JEL* code.
According to at least one readability score, gender differences are less pronounced in subfields B (history of economic thought, methodology and heterodox approaches), F (international economics), H (public economics), M (business administration and business economics; marketing; accounting), O (economic development, technological change and growth) and R (regional, real estate and transportation economics).
Abstracts from subfield O are still better written when authored by women according to three of the five scores; marginal effects for B, F, H and R are not significantly different from zero. Weak evidence points to men as the better writers in subfield M: its Flesch-Kincaid score suggests female-authored abstracts take !!FleschKincaidM more years of schooling to understand (standard error !!FleschKincaidse); effects for the other scores are insignificant.
Papers classified as D (microeconomics), N (economic history) or Q (agricultural and natural resource economics; environmental and ecological economics) have more pronounced gender differences in readability---female-authored abstracts read !!DBetter, !!NBetter and !!QBetter times better than the average, respectively. All readability scores agree, are significant on their own and most are significantly higher than the mean effect.
In contrast to my expectation, more women in a field may not necessarily be an antidote to gender differences in readability. While D, N and Q are about average in terms of total female co-authors, they vary widely in how frequently their papers name only female co-authors. Q is in the !!QFemOnly, D the !!DFemOnly, N the !!NFemOnly. Admittedly, !!NWomen percent is itself pretty low; economic history papers are still overwhelmingly---as in !!NMen percent---penned just by men. But given the readability gap is present in subfields with both above- and below-average rates of sole female authorship, women may need to be better writers even where more of them publish.</Text>
            <Comments>\input{$P3/equations/eqn2.tex}
\input{$P3/tables/tex/table5b.tex}
Due to small sample sizes (particularly of female authors) few *JEL*-specific marginal effects are significant on their own. See the [Appendix](#p3table5a) for full regression output).
Women make up roughly !!DFemRatio of all authors in articles classified as D, !!QFemRatio in articles classified as Q and !!NFemRatio in articles classified as N.</Comments>
        </Document>
        <Document ID="1EDCF80E-2564-4FEF-B28C-EB4E1EBFD44C">
            <Title>Controls</Title>
            <Text>For every article I recorded authors' institutional affiliations. Individual universities in U.S. State University Systems were coded separately (*e.g.*, UCLA and UC Berkeley) but think tanks and research organisations operating under the umbrella of a single university were grouped together with that university (*e.g.*, the Cowles Foundation and Yale University). Institutions linked to multiple universities are coded as separate entities (*e.g.*, École des hautes études en sciences sociales).
In total, 1,039 different institutions were identified. I create 64 dummy variables, each of which represents one or more institution(s); groupings reflect counts of distinct articles in which an institution was listed as an affiliation. Specifically, institutions listed in 59 or fewer articles were grouped in bins of 10 to form six dummy variables: the 751 institutions mentioned in 0--9 articles were grouped to form the first dummy variable, the 92 mentioned in 10--19 articles were grouped to form the second, *etc.* Fifty-eight institutions were affiliated with 60 or more articles; each is assigned its own dummy variable. When multiple institutions are associated with an observation, only the dummy variable with the highest-rank is used, *i.e.*, the highest-ranked institution per author when data is analysed at the author-level and the highest-ranked institution for all authors when data is analysed at the article-level.
I control for article quality and author productivity in several ways. First, I use article citations from the [Web of Science](https://login.webofknowledge.com/error/Error?Error=IPError&amp;PathInfo=%2F&amp;RouterURL=https%3A%2F%2Fwww.webofknowledge.com%2F&amp;Domain=.webofknowledge.com&amp;Src=IP&amp;Alias=WOK5) database. Second, I generate 30 dummy variables that group authors by career-total publication counts in the four journals. For example, Daron Acemoglu and Jean Tirole form one group (each published 45 articles as of December 2015); Alvin Roth, Elhanan Helpman and Gene Grossman form another (27 articles). In [][NBER] and [][Duration], I additionally control for the number of prior top-four papers (at time of publication). For co-authored articles, only the data corresponding to the most prolific author is used.
To account for English fluency, most regressions include a dummy variable equal to one if an article is co-authored by at least one native (or almost native) English speaker. I assume an author is "native" if he: (i) was raised in an English-speaking country; (ii) obtained all post-secondary eduction from English speaking institutions; or (iii) spoke with no discernible (to me) non-native accent. This information was almost always found---by me or a research assistant---in authors' CVs, websites, Wikipedia articles, faculty bios or obituaries. In the few instances where the criteria were ambiguously satisfied---or no information was available---I asked friends and colleagues of the author or inferred English fluency from the author's first name, country of residence or surname (in that order).
I create dummy variables corresponding to the 20 primary and over 700 tertiary *JEL* categories to control for subject matter. The *JEL* system was significantly revised in 1990; because exact mapping from one system to another is not possible, I collected these data only for articles published post-reform---about 60 percent of the dataset. Codes were recorded whenever found in the text of an article or on the websites where bibliographic information was scraped. Remaining articles were classified using codes from the American Economic Association's Econlit database.
To control for editorial policy, I recorded editor/editorial board member names from issue mastheads. *AER* and *Econometrica* employ an individual to oversee policy. *JPE* and *QJE* do not generally name one lead editor and instead rely on boards composed of four to five faculty members at the University of Chicago and Harvard, respectively. Editor controls are based on distinct lead editor/editorial boards---*i.e.*, they differ by at least one member. In total, 74 groups are formed in this manner.
The matching exercise in [][SEUMatching] pairs authors using various factors, including their fraction of first-authored papers. First authors are those identified in the acknowledgements or listed first when authors are not ordered alphabetically.
To control for motherhood's impact on revision times, I recorded children's birth years for women with at least one 100 percent female-authored paper in *Econometrica*. I personally (and, I apologise, rather unsettlingly) gleaned this information from published profiles, CVs, acknowledgements, Wikipedia, personal websites, Facebook pages, [intelius.com](https://www.intelius.com) background checks and local school district/popular extra-curricular activity websites. Exact years were recorded whenever found; otherwise, they were approximated by subtracting a child's actual or estimated age from the date the source material was posted online. If an exhaustive search turned up no reference to children, I assumed the woman in question did not have any.</Text>
            <Comments>[#Blank1991;] ranks institutions by National Academy of Science departmental rankings. Those and similar official rankings are based largely on the number of papers published in the journals analysed here.
&lt;!--\label{fn7}--&gt;This quality/productivity control has several limitations: (i) it relies on publication counts---not necessarily an accurate measure of "quality"; (ii) it discounts current junior economists' productivity; and (iii) it generates somewhat inconsistent groupings---for example, two authors have published 45 articles, but only one author has published 37 (Andrei Shleifer).
In [\]\[p. 42 and p. 44][#Hengel2016;], I experiment with another measure of quality---the order an article appeared in an issue. It has no noticeable impact on the coefficient of interest or its standard error.
Non-native speakers who meet this criteria have been continuously exposed to spoken and written English since age 18. This continuous exposure likely means they write as well as native English speakers. To qualify as an English speaking institution, all courses---not just the course studied by an author---must be primarily taught in English. *E.g.*, McGill University is classified as English-speaking; University of Bonn is not (although most of its graduate economics instruction is in English).
I also conducted a primitive surname analysis [see\]\[pp. 35--36][#Hengel2016]. It suggests that the female authors in my data are no more or less likely to be native English speakers.
In recent years, *JPE* has been published under the aegis of a lead editor.
While the information I found was publicly available, I apologise for the obvious intrusion.
In several instances, I obtained this information from acquaintances, friends and colleagues or by asking the woman directly. Given its sensitive nature, children's birth years are not currently available on my website (unlike other data in this paper).</Comments>
        </Document>
        <Document ID="5195FD07-F29D-42B9-9513-3C1467650578">
            <Title>Theory</Title>
            <Text>
From [][rescue_culture], the degree of credit rationing in any economy is largely determined by how many insolvent firms would be reorganised in bankruptcy and the amount of money wasted if they were. Inefficient reorganisation makes getting a loan unequivocally more difficult---an increase in $Y$ decreases the value of creditors' outside option and weakens their bargaining position during workouts. The cost of lending rises, as does credit-rationing ([](#prop5)(i)).
PROPOSITION
(i) When $Y$ increases, credit-rationing increases. (ii) When $p$ increases, more viable but fewer non-viable firms are credit-rationed.
eprop
The impact of $p$ is not so straightforward. $p$ affects creditor earnings in different ways depending on whether an insolvent project is viable or not. For the non-viable, increasing judicial accuracy makes it more likely bankrupt projects are liquidated. The value of creditors' outside option rises; entrepreneurs accept greater financial responsibility during workouts; credit rationing falls. For viable firms, an increase in $p$ has precisely the opposite effect. Judicial error actually makes it easier for these firms to get credit ([](#prop5)(ii)).
Judicial error simultaneously expands and contracts credit rationing. For non-viable firms the direction is obvious: it increases insolvency costs and stifles lending. When judges make mistakes, however, viable firms are sometimes liquidated. This decision is legally inaccurate and possibly even economically suboptimal. Nonetheless creditors do not lose money in liquidation so their expected returns in insolvency are higher. They are more willing to lend.
COROLLARY
In receivership, $Y$ and $p$ have no impact on credit-rationing.
ecor
But by replacing payments with outcomes---and eliminating judicial interference entirely---a perfectly efficient outcome prevails. Firms liquidate and continue when they should, debt is cheap and credit-rationing unheard of. In its purest form, receivership---*i.e.*, procedures similar to those found in the U.K. prior to 1986---achieves both. As shown in [](#prop4) ([][rescue_culture]), receivership forces entrepreneurs to shoulder maximum financial responsibility during insolvency; every such project is funded. $Y$ and $p$ have no impact on [credit-rationing](#cor3).</Text>
            <Comments>Proved in [][p2Proofs].</Comments>
        </Document>
        <Document ID="3BF9D3D3-7E94-45F7-9978-1132940BDF45">
            <Title>Introduction</Title>
            <Text>Female academics are less likely to make tenure, take longer when they do and earn much less than their male peers[#Bandiera2016,Ceci2014,Ginther2004,Weisshaar2017]. In economics, only a quarter to a third of assistant and associate professors---and no more than 15 percent of professors---are women[#Lundberg2019,Gamage2020,Bateman2021].
There are a number of factors driving these outcomes. Women have smaller research networks[#Ductor2018], make different career choices and face different constraints (*e.g.*, motherhood). They may also be held to tougher standards. For example, evidence suggests that their qualifications and ability are underestimated[#Foschi1996,Grunspan2016,Moss-Racusin2012,Reuben2014]; female-authored papers are evaluated more critically[#Goldberg1968,Krawczyk2016,Paludi1983]; when collaborating with men, women are given less credit[#Heilman2005,Sarsons2020].
In this paper, I investigate whether top economics journals apply similar standards to men's and women's manuscripts---and specifically, similar writing standards. In the English language, clearly written prose is better prose, all things equal. Thoughtful word choice and simple sentence structure make text easier to understand, more interesting to read and expose inconsistencies long-winded writing often hides. Journal editors tend to agree. *Econometrica* asks authors to write "crisply but clearly" and to take "the extra effort involved in revising and reworking the manuscript until it will be clear to most if not all of our readers" ([*Econometrica* submission guidelines](http://www.econometricsociety.org/publications/econometrica/information-authors/instructions-submitting-articles), June 2016).
To measure writing clarity, I apply five highly tested "readability" formulas to 9,117 article abstracts published between 1950--2015 in the *American Economic Review* (*AER*), *Econometrica* (*ECA*), *Journal of Political Economy* (*JPE*) and *Quarterly Journal of Economics* (*QJE*). With these data, I document several stylised facts.
First, women aren't published that often in "top-four" economics journals. The average share of female authors per paper across the entire sample is 7.5 percent. In 2015, that share was still only 15 percent; just eight percent of papers were majority female-authored and only four percent were written entirely by women. Between 2015--2017, *QJE* did not publish a single exclusively female-authored paper. In several recent years, *Econometrica* and *JPE* have not either.
Second, the female-authored abstracts that are published in these journals are 1--6 percent more readable than those by men. Women write better despite adjusting for other factors correlated with quality---including citations, author prominence, seniority and individual fixed effects---accounting for English fluency and adding editor, journal, year and primary and tertiary *JEL* category dummies and roughly controlling for how theoretical vs. empirical a paper is.
Third, the gender gap in readability is 2--3 times larger in the published version of a manuscript compared to its pre-submission version. To arrive at these estimates, I match National Bureau of Economic Research (NBER) working papers with their final published articles. Assuming authors release their manuscripts as NBER working papers at about the same time that they submit them to peer review, these results suggest that female-authored abstracts become 2--5 percent more readable while under review.
Fourth, the portion of the gap formed in peer review reversed direction in journals that blinded referees to authors' identities before the internet. Although standard errors are large and sample sizes small, this evidence tentatively suggests that blind review can mitigate the impact of gender under certain circumstances. It also points to the possibility that editorial/refereeing bias at least partially contributes to women's better writing.
Fifth, I do not find evidence that men compensate for their lower quality writing by raising quality on another dimension. More specifically, better writing by female economists could arguably compensate for some other advantage present in men's papers. But as long as men and women are equally capable researchers and similarly informed conditional on controls, the cost to both genders of implementing their respective publication strategies should be equal---otherwise, women could reduce the cost of producing a paper while holding acceptance rates constant by adopting a strategy marginally closer to men's (or visa versa). A rough test of this hypothesis using submit-accept times from *Econometrica* and the *Review of Economic Studies* (*REStud*) suggests this isn't the case. The cost to men of revising a paper appears to be much lower than the cost to women: female-authored papers spend three to six months longer under review compared to observably equivalent male-authored papers. The effect persists across a range of specifications that account for, among other things, citations, readability, author seniority, motherhood, childbirth and field.
Finally, it does not appear that women are rewarded for their better writing. Recent evidence from a set of comparable journals suggests female-authored papers are not accepted at higher rates after conditioning on similar co-variates[#Card2020].
These stylised facts suggest women spend too much time rewriting old papers and not enough time writing new papers, relative to men. The lack of a gender gap under blind review points to external factors beyond their control, but women's better writing could also be driven by internal factors such as higher risk-aversion [for a review, see\]\[][#Croson2009], lower confidence [see, *e.g.*,\]\[][#Coffman2014,Exley2019], a tendency to update too much when faced with negative signals[#Mobius2014], be more easily swayed by the opinions of others[#Born2019] or exert more effort on low stakes tasks[#Schlosser2019] and those which do not yield obvious benefits[#Babcock2017].
To investigate these mechanisms, I model an author's decision-making process as if it were governed by the rational behaviour of women who update their beliefs about the readability thresholds they are held to as they gain experience in peer review. The intuition of the model is as follows. A gender readability gap that decreases with experience suggests that women initially overestimate referees' and editors' writing thresholds but revise their beliefs downward as they submit more papers to peer review. This pattern indicates that internal factors predominantly drive the gap. On the other hand, a gap that increases with experience suggests that women initially underestimate writing thresholds but revise their beliefs upwards as they gain a better understanding of peer review. In this case, tougher standards probably play a role in women's choices unless their extra effort is rewarded with higher acceptance rates relative to men.
The model identifies three testable conditions which can help establish whether external factors are at all important to the existence and evolution of the gender readability gap: (1) experienced women write better than equivalent men; (2) women improve their writing over time; (3) female-authored papers are accepted no more often than equivalent male-authored papers. Evidence from the pooled sample of authors suggests conditions (1) and (2) hold: on average, women's writing gradually gets better but men's does not; between authors' first and third published articles, the readability gap increases by up to 12 percent. Although my data do not identify Condition (3), female-authored papers are accepted less often than equivalent male-authored papers at a similar set of journals[#Card2020].
The validity and accuracy of these results primarily rely on two critical---and strong---assumptions. First, the more experience a women gains in peer review, the fewer mistakes she makes about referees' and editors' standards. Second, male and female authors write papers that are identical with respect to topic, novelty and quality, conditional on controls. The second assumption is especially likely to be violated. Furthermore, concluding that higher standards are present requires that all three conditions hold for the same author---that is, the same woman must write better than an equivalent man, not be accepted at rates higher than he is *and* raise the quality of her writing over time.
To improve my estimates in both respects, I additionally restrict the sample to authors with three or more top-four publications and match observably similar male and female economists based on characteristics---including citations and field---that predict the topic, novelty and quality of their research. Within-person readability comparisons are used to determine if Condition (2) was satisfied for each author in a matched pair. Between-person comparisons after authors gain experience in peer review are used to establish whether Condition (1) was satisfied for the male or female member.
I find that Conditions (1) and (2) were satisfied for the same author in 65 percent of matched pairs; in two-thirds of those, the member who satisfied them was female. A counterfactual analysis suggests that higher standards mean women write, on average, 5 percent more readably than they otherwise would. I emphasise, however, that the reliability of these estimates depends on the extent to which the observables authors are matched on fully capture differences in the non-readability qualities of papers. Within each matched pair, they also require that Condition (3) is satisfied for the same author who satisfied Conditions (1) and (2), something I cannot directly test with my data. Additionally, the validity of the counterfactual analysis and the precision of its estimates rely on strong assumptions about men’s and women’s beliefs and the impact co-authors have on an article’s readability.
I conclude by showing suggestive evidence that women navigate higher standards by altering their behaviour. Guided by the model, I compare papers pre- and post-review as authors gain experience. This allows me to tease out the direct effect of higher standards---readability changes made *in* peer review---from its ``feedback'' effect---readability changes made *before* peer review in anticipation of those higher standards. I find that the direct effect dominates in authors' earliest papers. In fact, there is no significant gender difference between draft readabilities in men's and women's first top publications; it emerges entirely in peer review. In later papers, however, women write well upfront; the gap chiefly materialises *before* peer review. These results further support the hypothesis that women do not initially expect higher standards and instead learn about them over time. They also suggest that women adapt to higher standards by writing their future papers more readably prior to submission.
This paper contributes to the literature in several ways. First, to the best of my knowledge, I am the first to document empirical evidence that suggests women may be held to higher standards in the peer review process (as opposed to its outcome). Higher standards have recently been established using citations as a proxy for manuscript quality[#Card2020,Moon2020,Grossbard2018]. They also align with research on employee performance reviews, teaching evaluations and online comments: women receive more abusive feedback, less credit for intelligence and creativity and are expected to be more organised, prepared and clear[see, *e.g.*,\]\[][#Boring2017,Mengel2017,Correll2016,Gardiner2016,Wu2019].
Second, this paper proposes a novel explanation for academia's "Publishing Paradox", "Leaky Pipe\\-line" and general promotion gap. Higher standards cause collateral damage to women's productivity: spending more time revising old research means there’s less time for new research; fewer papers results in fewer promotions, possibly driving women into fairer fields. They may also help explain why so few women publish entirely female-authored papers, despite being the work tenure committees give them the most credit for[#Sarsons2020].
Third, my conclusions relate to a more general debate about gender differences in labour market outcomes. Higher standards impose a quantity vs. quality trade-off that characterises female output in many professions---*e.g.*, doctors, real estate agents and airline pilots [for a discussion, see\]\[][#Hengel2017]. Their downstream effects may contribute to several employment phenomena, including women's tendencies to concentrate in certain sectors and occupations[#Blau2016,Cortes2016], under-negotiate pay[#Babcock2003] and apply only to jobs they feel fully qualified for[#Mohr2014]. They may also reinforce work habits---*e.g.*, conscientiousness, tenacity and diligence---that correlate with quality and connote "femininity": for example, female physicians consult longer with patients[#Roter2004], female politicians fundraise more intensely[#Jenkins2007], female faculty commit fewer instances of academic misconduct[#Fang2013] and female lawyers make fewer ethical violations[#Hatamyar2004].
Fourth, this paper joins an emerging body of economic research studying how the experience and anticipation of discrimination affects choices and behaviour. Earlier theoretical work focused on the impact discrimination has on investment in education and occupational choice[see, *e.g.*,\]\[][#Lundberg1983,Coate1993,Goldin2014a]. More recent empirical research explores how stereotypes negatively impact performance[#Coffman2014,Bordalo2016,Lavy2015,Glover2017,Carlana2019]. My results suggest that rational responses to discrimination can distort productivity measurement[see also\]\[][#Parsons2011] and blur the line between biased treatment and voluntary choice.
Finally, this paper makes a related methodological contribution. Discrimination is generally identified from the actions[*e.g.*,\]\[][#Neumark1996,Bertrand2004] and/or learning processes[*e.g.*,\]\[][#Altonji2001,Fryer2013] of those who discriminate. But repeatedly observing individuals' choices can also bring to light the bias they are exposed to. In particular, multiple choices made under changing conditions reveals information about agents' intrinsic preferences and knowledge of underlying processes. Using this information, one can isolate group differences in the observed equilibrium from those that would have occurred in a non-discriminatory counterfactual one. For example, assuming intrinsic preferences are fixed over time, earlier choices provide an upper bound on the impact they play in gender readability gaps; assuming authors update beliefs about the relationship between readability and acceptance rates means later choices are made with more accurate beliefs. Although this strategy relies on several strong assumptions, it may be useful for understanding and bounding the effect discrimination has on the long-term decision-making processes of those who experience it.
The remainder of the paper proceeds in the following order. [][Data] describes the data. In [][StylisedFacts], I present the stylised facts about gender, readability and review times in top economics journals. [][Mechanisms] investigates mechanisms driving these stylised facts. [][Conclusion] concludes.</Text>
            <Comments>[#Auriol2019;] find female economists are slightly better represented at European institutions.
Data from a field journal find female-authored manuscripts are subject to greater scrutiny and spend longer under review[#Alexander2021]. A review-time gap was not, however, present in a set of journals that semi-overlap with those analysed here[#Card2020].
See also[#Bright2017;] for a similar idea in the philosophy literature. This idea has also been informed by extensive research on editorial patterns[#Ellison2002a,Card2013,Clain2017,Casnici2016,Card2020], bias in editorial decisions[#Abrevaya2012,Card2017,Bransch2017,Card2020] and female academics' lagging productivity and underrepresentation[#Ductor2018,Bayer2016,Ginther2004,Teele2017,Chari2017].
Traditional hypotheses focus on obvious discrimination[#Goldin2000], motherhood[#Bertrand2010] and differences in behaviour [*e.g.*,\]\[][#Niederle2010]. Contemporary theories tend to stress inflexible working conditions[#Goldin2014, Goldin2016], preferences[for a review, see\]\[][#Blau2016] and policy design[#Antecol2016].</Comments>
        </Document>
        <Document ID="EC768D57-346B-4795-AFAA-FD86683B8174">
            <Title>Model</Title>
            <Text>An entrepreneur has an idea for a business project. The project lasts two periods and requires a machine. Operating profits each period are $X_1$ and $X_2$, where $X_1$ and $X_2$ are non-negative and independently distributed according to the joint cumulative distribution  function $\Pi$.
The machine costs $K_0$. Up to time 1, it may be resold for its initial value. After time 1, it depreciates; its value at time 2 is $K_2$, where $0&lt;K_2&lt;K_0$. $K_0$ and $K_2$ are non-random and known at time 0.
The entrepreneur has no money to buy the machine. He pitches a creditor the take-it-or-leave-it offer to borrow $K_0$ at time 0 and owe $D$ at time 1. The entrepreneur and creditor are risk neutral, have symmetric information and the risk-free interest rate is zero; credit markets are perfectly competitive.
If the lender accepts his offer, the entrepreneur buys the machine. The project begins and $X_1$ is realised.</Text>
            <Comments>The analysis is applicable to any tangible or intangible depreciable productive asset.
For an extensive form representation of the bankruptcy game, see [][p1appendixextensiveform].
Borrowing more than $K_0$ is a risk-free transfer of wealth from time 1 to time 0. Since the entrepreneur is risk-neutral and the risk-free interest rate is zero, the transfer itself confers no benefit. The fact that loans mature after one period while project returns last for two is fundamental to the model. If loans could last the entire duration of the project, bankruptcy is irrelevant.</Comments>
        </Document>
        <Document ID="D61D4703-AFC6-424B-BADE-B0F242136E96">
            <Title>Summary statistics</Title>
            <Text>&lt;!--\input{$HOME/Dropbox/Readability/draft/tex/generated/table5}--&gt;
Draft abstracts were collected from NBER Technical and Working Paper Series (see [][Data]). NBER series were used as the exclusive data source for two reasons. First, approximately one-fifth of articles in my data were originally part of an NBER series---and one third of papers published since 1990. This makes it the largest single source of draft papers for articles published in top-four journals. Second, authors release their manuscripts as NBER working papers at about the same time they submit them to a journal[see\]\[and [](#figure6)][#Ellison2002a,Goldberg2015].
[](#table5) compares textual characteristics between a paper's draft and final versions. It suggests abstract text is altered during peer review. According to the first panel, draft abstracts are longer---more characters, words and sentences---and denser---more syllables, polysyllabic words and difficult words. The biggest changes are made to female-authored papers: figures in column six are 20--30 percent higher (in absolute value) than those in column three. The second panel of [](#table5) suggests peer review improves readability, although results are less clear for male-authored papers.</Text>
        </Document>
        <Document ID="26299AAC-A568-45A1-8572-2C03439D7261">
            <Title>Insolvency</Title>
            <Text>Suppose now $X_1&lt;D$; the project is insolvent: "unable to pay its debts as they fall due"[\]\[§123(2)][#GreatBritain1986]. No legal restrictions prevent the entrepreneur and creditor from settling matters on their own: renegotiating the terms of their loan contract or jointly agreeing to liquidate. Formal bankruptcy is their fallback when they fail to do so.</Text>
            <Comments>Technically it is "cash flow insolvent". Another test is "balance sheet insolvency" whereby the firm's liabilities exceed its assets. Usually, balance sheet insolvent firms are also cash flow insolvent; rarely, however, a firm is *currently* able to pay its debts but clearly won't be able to in the future. In this scenario, creditors (or shareholders) can petition the court to declare the company insolvent. Since every balance sheet insolvent firm must eventually be cash flow insolvent, the entrepreneur has no opportunity to misuse company funds and the time value of money is zero, I disregard balance sheet insolvent firms (without loss of generality).</Comments>
        </Document>
        <Document ID="10509EE0-ABB1-4A34-A3A1-D0A5E16F82A1">
            <Title>Do referees care about readability?</Title>
            <Text>I scrapped all the posted review comments on the website [Shit My Reviewers Say](https://shitmyreviewerssay.tumblr.com/) posted between 21 October 2014 and 10 February 2018. The entire sample constituted 721 comments from referee reports that academics posted on the website. I then classified these posts as either (a) writing related---*i.e.*, they dealt with writing quality, document structure or word choice/tone---or (b) not writing related. In the end, I classified 180 as writing related and 541 as not writing related [](#smrs).
Because the review comments submitted to Shit My Reviewers Say, the sample is obviously not random. Thus, please interpret as suggestive---but not definitive---evidence that writing is a common element by which referees evaluate manuscripts.
&lt;!--
\input{$HOME/Dropbox/Readability/draft/tex/fixed/smrs}
\clearpage
--&gt;</Text>
        </Document>
        <Document ID="E0973260-0CFF-4E86-9842-51C4C2604B5F">
            <Title>New Folder</Title>
        </Document>
        <Document ID="DF4250CB-567F-4424-BDA7-6CDF0DB4F649">
            <Title>Other data and control variables</Title>
        </Document>
        <Document ID="D83425EA-5CA9-430A-8213-250CBC4D6C67">
            <Title>[](#table5), male effects</Title>
            <Text>[](#tableC4) displays total male effects---*i.e.*, the total effect for men co-authoring only with other men---from the regressions presented in [](#table5). Effects estimated at a female ratio of zero and observed values for other co-variates. Grade-level effects (Flesch-Kincaid, Gunning Fog, SMOG and Dale-Chall) have been multiplied by negative one (see [][MeasuringReadability]).
&lt;!--\input{$HOME/Dropbox/Readability/draft/tex/generated/tableC4}--&gt;
&lt;!--\clearpage--&gt;</Text>
        </Document>
        <Document ID="55655E1F-9EA6-414E-8EE8-2928EDC52CD2">
            <Title>Google</Title>
            <Text>[](#eq:p3eq6) investigates the effect internet search---and Google, specifically---has on readability:where $\text{Google search}_j$ is one of two statistics: a dummy variable equal to 1 for articles published after Google incorporated (September 1998) and the natural log of total annual searches on [Google.com](http://www.google.com).
[](#p3table10b) presents results. It suggests the gender readability gap has grown with the popularity of search engines. The first panel shows the marginal effect for annual search numbers. The ratio of female authors is weakly significant for four out of five scores: each time internet search traffic doubled---which it has 19 times since 1998---women's Flesch Reading Ease score increased by !!GoogleLogFlesch points; their Flesch-Kincaid, Gunning Fog and SMOG scores declined by roughly !!GoogleLogThree points.
Men experienced a muted version of this trend. Doubling the number of Google searches made annually is correlated with a !!GoogleLogFleschMan point rise in their Flesch Reading Ease score and !!GoogleLogKincaidMan point fall in their Flesch-Kincaid score. Differences between the sexes are significant only for the latter.
[](#p3table10b)'s second panel re-estimates [](#eq:p3eq6) with the Google incorporation dummy. Marginal effects are more pronounced---male and female estimates are !!GoogleManPTwo and !!GoogleWomanPTwo times higher, respectively---although the precision of those estimates has not improved. The Flesch Reading Ease suggests Google improved readability of female-authored abstracts by five points; according to the others, it cut schooling needed to understand abstracts by up to a year and a quarter. Men likewise improved, albeit to a lesser degree. Differences between the sexes are newly significant for the Gunning Fog and SMOG scores; the Flesch-Kincaid remains weakly significant. All suggest Google's existence is correlated with a !!GoogleSexCompare percent greater increase in the readability of female-authored abstracts relative to abstracts by men.</Text>
            <Comments>\input{$P3/equations/eqn6.tex}
\input{$P3/tables/tex/table10b.tex}
Ratio of second panel coefficient to first panel coefficient multiplied by the logarithm of $2^{19}$.</Comments>
        </Document>
        <Document ID="F42EBDA9-AE93-46A6-AAA2-4F651BCC8E71">
            <Title>Results</Title>
            <Text>Does the readability gap change as publication counts increase? Yes, it widens---from women writing more clearly and men possibly less so. As their careers evolve, women improve: their average readability scores are 1-5 percent higher than the readability of their first papers; their latest papers 1--7 percent [higher][AppendixFirstLast]. For a man, however, his average and last paper may be more poorly written than the first. [](#figure4) plots mean Flesch Reading Ease scores grouped by authors' $t\text{th}$ article; as the count increases, men and women diverge.
&lt;!--\input{$PPATH/figures/tex/figure4}--&gt;
[](#table10) tests significance of that divergence by FGLS estimation of [](#equation1) (omitting $\text{score}_{it-1}^s$) on subsamples corresponding to authors' first ($t=1$), second ($t=2$), third ($t=3$), fourth and fifth ($t=4\text{--}5$) and sixth and up ($t\ge6$) articles published in the journals and time periods covered by the data. Only marginal effects on co-authoring with women for female authors are shown ($\beta_1$). Final column is a population-averaged estimate on the pooled sample. Regressions in columns ($t=1$) to ($t\ge6$) are weighted by $1/N_j$ (see [][AuthorLevel]), standard errors adjusted for two-way clustering on editor and author and corrected for cross-model correlation. Final column estimates are unweighted, error correlations are specified by an auto-regressive process of order one and standard errors are clustered on author.
&lt;!--\input{$PPATH/tables/tex/table10}--&gt;
All figures agree---women write better---but the magnitude and significance of that difference increases as $t$ increases despite falling numbers of observations. Between columns ($t=1$) and ($t=2$), the gap marginally widens but is not significant; after that, it triples (at least); the increase is significant ($p&lt;0.05$) for all five scores. At higher publication counts, differences in male-female readability remain roughly constant, although estimates are only weakly significant and suffer from small samples of female authors.
Despite having the largest sample, first-time publications are not driving the observed readability gap. [](#figure4) suggests little or no gender difference when $t=1$; [](#table10) backs this up. Coefficients in column ($t=1$) are imprecise, roughly half the size of those from a pooled regression (last column) and a fraction the size of estimates in columns ($t=3$), ($t=4\text{--}5$) and ($t\ge6$). [Wald tests][AppendixEqualityTests] reject equality of $\beta_1$ in the first and third models at $p&lt;0.01$ for the Flesch Reading Ease, Flesch-Kincaid and SMOG scores and $p&lt;0.05$ for the Gunning Fog and Dale-Chall scores.
To generate a slope coefficient measuring the mean additional contribution each paper makes to readability, I estimate:[](#equation9) includes $t$ and its interaction with an article's female ratio and the author's sex; otherwise, it is identical to [](#equation1) and similarly estimated by first-differencing with endogeneity instrumented with earlier lags[#Arellano1995,Bundell1998].
$t$'s (immediate) marginal effect is presented in [](#table11). Male effects estimated for male authors co-authoring with no females $(\beta_3+\beta_4)$; female effects for female authors co-authoring with no males $(\beta_3+\beta_5)$. The coefficients agree: readability scores remain constant as men publish more papers---all are very close to zero and none significantly different from it. For women, however, every additional paper is more readable than the last; the effect is statistically significant for four out of five scores.
&lt;!--\input{$PPATH/tables/tex/table11}--&gt;
[](#figure5) plots mean male and female effects over $t$ using estimates derived from the Flesch Reading Ease regression in [](#table11). It serves as a more precise illustration of the trends depicted in [](#figure4). As before, there is little or no gender difference in readability for the first two publications but thanks to women's self-improvement, the gap widens substantially after that. While [](#figure4) suggested men were also writing more poorly, that effect is not present here.
&lt;!--\input{$PPATH/figures/tex/figure5}--&gt;</Text>
            <Comments>Coefficient equality test statistics are available in [][AppendixEqualityTests].
Note that figures in columns two and three of [](#table10) are roughly in line with third column estimates in [](#table8)---on average, $t=2.7$ for female-authored articles released first as NBER working papers.
\input{$PPATH/equations/equation9}</Comments>
        </Document>
        <Document ID="959D7027-8576-46E3-A7A1-19AD065607C6">
            <Title>Alternative proxies for article gender</Title>
            <Text>In this appendix, I replicate [](#table3_FemRatio), [](#table6_FemRatio), [](#table10_FemRatio), [](#table11_FemRatio), [](#tableH2_FemRatio) and [](#table4_FemRatio) using the alternative proxies for article gender summarised in [](#SampleSummaries).
&lt;!--\input{$HOME/Dropbox/Readability/draft/0-tex/fixed/Table-M.1}--&gt;</Text>
        </Document>
        <Document ID="226CD660-FC2A-44F5-8B0C-3B825C5A41D8">
            <Title>349 (alastair’s macbook's conflicted copy 2016-10-06)</Title>
            <Text>[](#p3table8) suggests the readability gap grew precisely while papers where being reviewed. FGLS estimates suggest female-authored working papers and published articles are better written. The latter's gender readability gap, however, is substantially larger. The Flesch-Kincaid, Gunning Fog and SMOG scores imply immediate peer review accounts for 60--70 percent of the gap; the Flesch Reading Ease and Dale-Chall scores indicate a smaller impact (30--40 percent). OLS estimates on the change in score draw similar conclusions.
Assuming non-peer review factors are independent of its timing---discussed in [][p3Alternatives]---the significant increase in the readability gap *ex post* establishes the desired causal link.
</Text>
        </Document>
        <Document ID="2F7CF3CD-4205-4C06-9BCD-570342589891">
            <Title>Time between working paper release and journal submission</Title>
            <Text>[](#figure6) displays a histogram of the length of time between the date an author releases his draft paper as an NBER working paper and the date he submits it for peer review at *Econometrica*. Pink represents papers with at least one female co-author (41 articles); blue are papers with no female co-authors (187 articles).
[](#figure6) suggests most manuscripts are submitted to peer review at the same time or *before* they are released as NBER Working Papers---and this is especially true for papers with at least one female author. This suggests that the estimates presented in [](#table6_FemRatio) reflect gender differences in changes made to manuscripts while those manuscripts are indeed under review at the journal in which they will be eventually published.
&lt;!--
\input{$HOME/Dropbox/Readability/draft/0-tex/fixed/Figure-G.2}
\clearpage
--&gt;</Text>
        </Document>
        <Document ID="F38B058E-3CC8-4106-B0F0-A623A794901F">
            <Title>[][SEUMatching], co-variate balance when \\(\underline D_{ik}\ne0\\)</Title>
        </Document>
        <Document ID="5ABF8EA6-409A-467F-A815-FC4437CE8E10">
            <Title>Studies included in meta analysis</Title>
            <Text>Below are the studies included in the analysis from [](#figure2), which summarises correlations between readability scores and alternative measures of reading comprehension found in other research. A few notes on the criteria for inclusion and how some correlations were determined:
* I include only documents produced for the U.S. government or published peer reviewed studies---with the exception of the present paper, [#Benoit2017;] and results from dissertations that were presented and discussed in a peer reviewed manuscript.
* I include a small number of studies with correlations between alternative readability measures and the number of words not listed on the Dale-Chall word list. In all other cases, however, correlations with only parts of a score (*e.g.*, syllables per words) are omitted.
* A few earlier studies calculated and listed various readability measures for many passages of text, but did not report coefficients of correlation between them. I manually calculated these correlations myself.
&lt;!--
\begin{refsegment}
\input{/USERS/ERINHENGEL/Dropbox/Readability/draft/0-tex/generated/Figure-D.1-bib}
\end{refsegment}
\printbibliography[segment=1,heading=none]
\clearpage
--&gt;</Text>
        </Document>
        <Document ID="3DA20168-ED59-4A60-BCE5-FF0135174308">
            <Title>Supplemental summary statistics</Title>
        </Document>
        <Document ID="EC2E294F-C3F2-433B-A6E8-370BCF7BF00C">
            <Title>[][SEUEmpirical], supplemental output</Title>
        </Document>
        <Document ID="AEF385F2-9CAF-4121-A023-17E9C5E0F643">
            <Title>[][SEUConsistency]</Title>
        </Document>
        <Document ID="F24572C5-14EA-4BC5-948F-71584CDA9167">
            <Title>[][IndirectEffect], supplemental output</Title>
        </Document>
        <Document ID="87E870E4-D4E5-4835-89D8-06540851AC32">
            <Title>[](#table4), alternative quality/productivity controls</Title>
            <Notes>&lt;!--\clearpage--&gt;</Notes>
        </Document>
        <Document ID="5501BE86-C631-47A1-AF86-70D4FE823133">
            <Title>[](#table4), journal effects</Title>
            <Text>[](#tableC4) shows the coefficients on the journal dummies in column (2), [](#table4). They compare *AER*'s readability to the readability of *Econometrica*, *JPE* and *QJE*.
&lt;!--\input{$PPATH/tables/tex/table5}--&gt;</Text>
        </Document>
        <Document ID="016D7AE3-8D8F-48FB-93DB-A4FF922ABFD4">
            <Title>[](#table4), alternative clustering and quality/productivity controls</Title>
            <Text>The following tables repeat regressions in [](#table4), clustering errors instead on [volume](#tableD8), [issue](#tableD9) and [article](#tableD10). Standard errors vary little.
[](#tableD11) repeats the regressions in [](#table4) using an alternative measure of a paper's quality---the order an article appears in an issue. (For example, the lead article is assigned one, the next article two, *etc.*) It is meant to capture a paper's contribution and importance---its so-called "$q$-quality"[#Ellison2002b]. As [](#tableD11) illustrates, including fixed effects for order has little impact on coefficients or their standard errors.
&lt;!--\input{$HOME/Dropbox/Readability/draft/tables/tex/tableD8}--&gt;
&lt;!--\input{$HOME/Dropbox/Readability/draft/tables/tex/tableD9}--&gt;
&lt;!--\input{$HOME/Dropbox/Readability/draft/tables/tex/tableD10}--&gt;
&lt;!--\input{$HOME/Dropbox/Readability/draft/tables/tex/tableD11}--&gt;
&lt;!--\clearpage--&gt;</Text>
            <Comments>[#Ellison2002a;] showed that papers published earlier in an issue spend less time in peer review. (This is supported by [](#table9).) He attributes this to a "$q$-$r$ trade-off": reviewers demand fewer $r$-quality revisions (robustness checks, clarity, *etc.*) from papers that represent important contributions ($q$-quality).</Comments>
            <Notes>&lt;!--\clearpage--&gt;</Notes>
        </Document>
        <Document ID="780763A2-6960-4C0D-8436-57C20440F28A">
            <Title>Author prominence</Title>
            <Text>I generate 37 dummy variables that group authors by their career-total top-five journal (*AER*, *Econometrica*, *JPE*, *QJE* and *REStud*) publications as of December 2015. For example, Jean Tirole forms one group (59 articles); James Heckman and Gene Grossman form another (34 articles).</Text>
        </Document>
        <Document ID="F0A739EF-20B6-42C6-86F4-2312EA990C73">
            <Title>Majority female-authored</Title>
            <Text>&lt;!--
\begin{vplace}[0.7]
\input{$HOME/Dropbox/Readability/draft/0-tex/generated/Table-3-Fem50}
\end{vplace}
\clearpage
\input{$HOME/Dropbox/Readability/draft/0-tex/generated/Table-5-Fem50}
\clearpage
\input{$HOME/Dropbox/Readability/draft/0-tex/generated/Table-6-Fem50}
\clearpage
\input{$HOME/Dropbox/Readability/draft/0-tex/generated/Table-7-Fem50}
\clearpage
\input{$HOME/Dropbox/Readability/draft/0-tex/generated/Table-8-Fem50}
\clearpage
\input{$HOME/Dropbox/Readability/draft/0-tex/generated/Table-F.2-Fem50}
\clearpage
--&gt;</Text>
        </Document>
        <Document ID="E0CB79A3-753C-441D-8859-6A9F4C90370F">
            <Title>[](#table4), alternative measures of an article's "gender"</Title>
            <Text>[](#tableC4a), [](#tableC4b) and [](#tableC4c) repeat the analysis shown in [](#table4) using three alternative measures of an article's "gender". In [](#tableC4a), papers authored entirely by women are compared to papers authored entirely by men. In [](#tableC4b), papers are considered "female" if at least one author is female. In [](#tableC4c), article gender is represented by a binary variable equal to one if at least half of all authors are female; articles below that threshold with at least one female author are excluded.



&lt;!--\clearpage--&gt;</Text>
        </Document>
        <Document ID="70609C80-475F-481A-83D2-8834708978EB">
            <Title>Policy implications</Title>
            <Text>[](#prop1), [](#prop2) and [](#prop3) imply two fundamental problems caused by bankruptcy: (i) credit rationing and/or decisions that induce premature liquidation *ex ante*; and (ii) inefficient continuation *ex interim*. Problem 1 affects viable firms; Problem 2 affects non-viable firms.
Rescue culture supposedly limits liquidating viable firms. When procedures are expensive, however, the opposite occurs. Minimising Problem 1 means cutting their cost. When $Y=0$, all viable firms get credit. $K_0&lt;\ol V_0^B$; per [](#prop2)(i) and [](#p1equation4) a $D$ exists that satisfies [](#lem3). Since viable firms continue even when insolvent ([](#prop1)(i)), investment decisions are anyway efficient. Meanwhile, [](#prop2)(ii) applies to all non-viable firms; none are excluded from the lending market. With sufficiently differentiated product markets for productive assets, *ex interim* decisions are efficient as well.
The model assumes bankruptcy does not explicitly curtail creditors' right to $D$. Thus, regulations which neither reduce future earnings nor explicitly cut claims do not impact *ex ante* lending decisions. As discussed in the introduction, most rules from Chapter 11 meet these criteria---even super-senior interim financing. As shown in [][p1AppendixSuperSeniorFinancing], emergency finance prevents existing creditors from blocking new loans. Nevertheless, interim loans do not alter underlying firm value; contracts adjust upfront to account for their possibility. Lending conditions and investment decisions remain efficient.
Yet elevating the value of creditors' outside option simultaneously addresses Problems 1 and 2. Several common law countries---most notably the U.K. before 2003---include a form of bankruptcy that does just that: receivership. Receivership applies to a special class of creditor---one that holds a lien on more than just the firm's physical assets.
"Floating charge" liens apply to an entire business. In receivership, a creditor secured by one has full control of the distressed firm. He may operate it himself, sell it to a third party or liquidate its assets piecemeal. Setting $q=0$ simulates receivership: creditor and entrepreneur disagreement implies the creditor supports liquidation ([](#lem1)); without reorganisation, courts uphold his wish.
Contracts based on receivership replace a payment with an outcome precisely when that payment is particularly risky. Safer outside options are more valuable, forcing entrepreneurs to shoulder greater financial responsibility in insolvency. Since $\ol C_1^B=C_1^L$, the creditor always recovers the principal amount of his loan. Lending is riskless. The entrepreneur asks to borrow $K_0$ and repay exactly that one period later. The creditor accepts. Every project is funded. *Ex ante* and *ex interim* investment decisions are efficient ([](#prop4)).
PROPOSITION
In receivership, $D=\ul K_0$. There is no credit-rationing. Non-viable firms liquidate at time 1. Viable firms continue until time 2.
eprop
But receivership is not common and its ability to alleviate credit-rationing frequently undermined by regulation. Reformed U.K. procedures took effect in 2003. The new law limits floating charges to eight exceptional cases.
Rescue culture was the obvious rationale. Nevertheless, 60 percent of small-to-medium-sized firms in British receivership ultimately continued operating as going concerns---and most lenders that eventually liquidated genuinely tried a rescue first[#Davydenko2008,Franks2005a]. Chapter 11, on the other hand, rehabilitates only a third of them; the rest are eventually liquidated[#Baird2007a,Kahl2001].</Text>
            <Comments>As I show in a companion paper, judicial errors may also reduce credit-rationing of and self-induced premature liquidation by viable firms[#Hengel2015].
When the market for machines is insufficiently differentiated, some non-viable firms are always at risk of filing for bankruptcy when initial earnings are very low---and a fraction of those may be inefficiently continued due to judicial error. Nevertheless, sub-optimal interim decisions are infrequent. Firms opt for bankruptcy in only a narrow set of mathematical circumstances predicated mostly by dismal initial earnings. When low earnings are improbable, $D$ will be low squeezing the range of $X_1$ that make bankruptcy attractive. When they are likely, firms may get credit at some $D$ such that $\ol C_0=K_0$. In this case, however, bankruptcy is not much worse than liquidation, so either the difference between returns in liquidation and those in continuation are small or judicial likelihood of liquidation is high. If, on the other hand, no such $D$ is available, the firm only gets credit at an exorbitant rate ([](#prop2)(ii)), thus guaranteeing an efficient outcome at time 1.
Floating charges may also be more narrowly defined or co-exist with traditional secured loans.
They are: 1. capital markets; 2. public-private partnerships; 3. utilities; 4. urban regeneration projects; 5. project finance; 6. financial markets; 7. social landlords; and 8. special administration regimes (notably, transport and water). Each is further limited by several specific provisions in Enterprise Act 2002, *e.g.*, debt exceeding £50 million and contractual "step-in" rights (giving one party rights to "step in" the shoes of another in cases of serious breach of contract).
Findings in[#Davydenko2008;] and [#Franks2005a;] actually apply to U.K. administrative receivership procedures. The Insolvency Act 1986 altered earlier receivership---most notably by outlining statutory responsibilities of the appointed administrator---and renamed it.</Comments>
        </Document>
        <Document ID="098F9E05-1281-4B45-BB3E-77392F6CC463">
            <Title>Lending environment</Title>
            <Text>I also include several controls for a country's general lending environment. Data on inflation, domestic credit to the private sector, GDP per capita and income from abroad per capita are culled from the World Bank's World Development Indicators (WDI) database. The percentage of firms using banks to finance investment is drawn from survey responses in the WES. Data are summarised in [](#p2table2).
Lending environment averages in countries covered by the WES reflect their focus on developing and emerging markets. Mean inflation is 6.2 percent, about 3.8 percentage points above the 2014 world average. Domestic credit to the private sector is considerably lower than the world average: 37.4 percent as opposed to 125.2 percent. GDP per capita is about half: $5,364 versus $10,803. Income from abroad is negative, in contrast to a positive world average of $54.</Text>
            <Comments>&lt;!--\label{p2footnote1}--&gt;Data are originally sourced from national accounts and international financial statistics compiled or estimated by the OECD, World Bank and/or IMF. Uzbek and South Sudanese data on credit to the private sector from [#EBRD2009,EBRD2013;] and [#IMF2014;], respectively. WES data are merged with nearest year for which WDI data exist.
\input{$PPATH/p2/tables/tex/p2table2}
Weighted world averages from the WDI database.</Comments>
        </Document>
        <Document ID="9F847DA0-8962-40D9-BFA9-CA2FF3FD54BD">
            <Title>$\widehat R_{it}$, controlling for *JEL* category</Title>
            <Text>[](#table8_jel) and [](#figure8_jel) replicate the analysis in [][MatchingResults] but [](#equation14) controls for primary *JEL* category. $\widehat R_{it}$ was reconstructed at female ratio equal to 1 for women, 0 for men and for a paper classified in *JEL* categories D (microeconomics) and J (labour and demographic economics).
&lt;!--
\vfill
\input{$HOME/Dropbox/Readability/draft/0-tex/generated/Table-9-jel}
\vfill
\input{$HOME/Dropbox/Readability/draft/0-tex/fixed/Figure-5-jel}
\vfill
\clearpage
--&gt;</Text>
        </Document>
        <Document ID="D06F6684-AD5E-4D28-8703-9F31A3CDC3CE">
            <Title>[](#table6_FemRatio), accounting for field</Title>
            <Text>As argued in [][NBER], if field only impacts the readability of a paper when it is first drafted, then the change in readability between versions should not depend on it. For example, using the change in score as the dependent variable should wash out potential bias from, say, concepts in certain areas being easier to explain. Moreover, because FGLS estimates (shown in the final panel of [](#table6_FemRatio)) are almost identical to estimates using the change in readability as the dependent variable (shown in the second panel of [](#table6_FemRatio)), they may not suffer from substantial bias, either, despite only taking field broadly into account (via the empirical, theory and other dummies).
For added robustness, however, I replicate [](#table6_FemRatio) but also control for primary *JEL* categories. Results are shown in [](#table6_jel). Standard errors on the coefficients on female ratio are slightly higher in [](#table6_jel) compared to [](#table6_FemRatio); coefficients on the interactions between blind review and the ratio of female authors are also slightly higher. Otherwise, results and conclusions are very similar to those presented in [](#table6_FemRatio).
&lt;!--
\input{$HOME/Dropbox/Readability/draft/0-tex/generated/Table-5-jel}
\clearpage
--&gt;</Text>
        </Document>
        <Document ID="65E90209-1D4A-4125-AC52-233E54319A29">
            <Title>Family commitments</Title>
            <Text>To control for motherhood's impact on revision times, I recorded children's birth years for women with at least one entirely female-authored paper in *Econometrica*. I personally (and, I apologise, rather unsettlingly) gleaned this information from published profiles, CVs, acknowledgements, Wikipedia, personal websites, Facebook pages, background checks and local school district/popular extra-curricular activity websites. Exact years were recorded whenever found; otherwise, they were approximated by subtracting a child's actual or estimated age from the date the source material was posted online. In several instances, I obtained this information from acquaintances, friends and colleagues or by asking the woman directly. If an exhaustive search turned up no reference to children, I assumed the woman in question did not have any.
&lt;!--\clearpage--&gt;</Text>
        </Document>
        <Document ID="446183A8-F5D9-4C49-8821-3FF368B70629">
            <Title>Alternative year fixed effects</Title>
            <Text>[](#table10_subyear) and [](#table10_pubyear) replicate [](#table10_FemRatio), replacing acceptance year fixed effects with fixed effects for submission and publication years, respectively.
&lt;!--
\input{$HOME/Dropbox/Readability/draft/0-tex/generated/Table-6-subyear}
\input{$HOME/Dropbox/Readability/draft/0-tex/generated/Table-6-pubyear}
\clearpage
--&gt;</Text>
        </Document>
        <Document ID="E0305587-9547-4338-BCF1-EC4994D166D6">
            <Title>Theoretical framework</Title>
            <Text>The previous section documents several stylised facts which, combined, suggest that women may spend too much time rewriting old papers and not enough time writing new papers relative to men. In this section, I investigate two potential mechanisms that can help explain why: (i) women voluntarily write better papers---*e.g.*, because they’re more sensitive to referee criticism---or (ii) better written papers are women’s response to external factors they do not control---*i.e.*, higher standards imposed by referees and/or editors.
To help distinguish between (i) and (ii), I develop a simple model of an author's decision making process. It follows an author---denoted by $i$---who publishes several articles in prestigious academic journals over the course of his career. Each article is roughly equivalent in terms of topic, novelty and quality, but may vary on readability. Upon submission to a journal, it is refereed by a review group $s\in\Sigma$, where $\Sigma$ is the (finite) set of all potential review groups and $\mu_i$ are strictly positive probability measures on $\Sigma$.
I assume $s$ accepts $i$'s papers if and only if $R_{it}\ge\widetilde R_i^s$ where $R_{it}$ is the readability of $i$'s $t$th paper and $\widetilde R_i^s$ is the threshold that $s$ applies specifically to $i$. $\widetilde R_i^s$ can depend on other qualities of $i$'s papers---*e.g.*, methodological rigour, data, originality or policy relevance. It may also reflect reviewers' objectives, idiosyncratic preferences and relative weight in determining outcomes. For example, an editor who does not care about readability and is willing to override the opinion of referees will implement a lower $\widetilde R_i^s$ (all else equal). I assume $\Sigma$ and $\mu_i$ are known to $i$ but $\widetilde R_i^s$ is not, although the process of peer review provides enough information---*e.g.*, via referee reports---for $i$ to distinguish each $s\in\Sigma$.
$i$ forms expectations about $\widetilde R_i^s$ by assigning subjective probabilities $\pi_{it}^s(R)$ to all $R$. He then regularly updates $\pi_{it}^s$ using relevant information from his own experience in peer review and by observing others' readability choices and publication outcomes. I assume this process of learning is sufficient to ensure that $\pi_{it}^s(R_{it})$ uniformly converges on some closed interval $\mathcal R$ where $R_{it}\in\mathcal R$.
[](#eq:EU) defines $i$'s subjective expected utility at time $t$ from writing a paper as readable as $R_{it}$:
&lt;!--\input{$HOME/Dropbox/Readability/draft/0-tex/equations/equation10}--&gt;
 where $\Pi_{it}^s(R_{it})$ is the probability that $\widetilde R_i^s\le R_{it}$ (*i.e.*, the cumulative sum of $\pi_{it}^s(R)$ for all $R\le R_{it}$), $u_i$ is the utility of having a paper accepted in a prestigious journal, and $\phi_i$ and $c_{i}$ are the satisfaction and cost, respectively, $i$ derives from writing readable papers. $\phi_i$ is increasing and concave in its arguments, $c_{i}$ increasing and convex---marginally higher $R_{it}$ generates proportionally less satisfaction but needs more effort when the paper is already well written.
[](#eq:EU) incorporates a variety of factors that potentially affect authors' readability choices---editorial standards conditional on other qualities in the paper ($\widetilde R_i^s$); ambition ($u_i$); the cost of drafting and revising manuscripts ($c_{i}$); an otherwise unexplained intrinsic satisfaction from writing readable papers ($\phi_i$). Poor information, overconfidence and sensitivity to criticism are not explicitly included, on the assumption that people do not *want* to be poorly informed, overconfident or excessively sensitive. These factors nevertheless enter [](#eq:EU)---and hence influence choices---via the subjective expectations authors form about $\widetilde R_i^s$.
A single $R_{it}$ cannot, therefore, establish if and to what extent $i$'s choices are motivated by (a) intrinsic preferences and costs specific to him ($u_i$, $\phi_i$, $c_{i}$), (b) conditional editorial standards and/or referee assignment outside his control ($\widetilde R_i^s$, $\mu_i$) or (c) miscellaneous confounding factors mopped up by $\Pi_{it}^s$. Because $i$'s intrinsic preference and cost functions are assumed to be time independent, however, observing an increase in his choice of readability at two separate $t$ distinguishes (a) from the combined impact of (b) and (c): $i$ may be more sensitive to criticism and he might prefer writing more clearly; nevertheless, he persistently improves the readability of his future papers relative to his past papers only when he believes that doing so boosts the probability that those future papers will be accepted. Moreover, because (c) does not reflect activities or states the author enjoys, its impact on choices declines with better information---*i.e.*, authors may miscalculate referee expectations and misconstrue their reports, but with experience they correct those mistakes. I capture this idea in [](#Theorem1), which is proved in [][AppendixProofs].
&lt;!--\input{$HOME/Dropbox/Readability/draft/0-tex/theorems/theorem}--&gt;
[](#Theorem1) identifies three testable conditions that, when satisfied, provide suggestive evidence that either editors assign women "tougher" referees---*i.e.*, those with higher $\widetilde R_{i}^s$---or, on average, referees and/or editors apply higher standards to women's writing. The first condition states that the readability of $i$'s and $k$'s papers never converges---*i.e.*, past some point, $i$'s papers are always more readable than $k$'s papers by some fixed amount $K'$. The second condition says that $i$'s future papers are always more readable---this time by some fixed amount $K''$---than at least one paper he wrote in the past. In other words, $i$'s readability does not converge to his most poorly written paper. Finally, the third condition states that $i$'s papers are not accepted more often than $k$'s (on average).
The intuition behind these conditions is simple. Experience can serve as a way to complete information, so observing how women's choices change as it increases can help determine which factors are predominantly motivating those choices. More specifically, information imperfections combined with lower confidence, higher risk aversion or a tendency to update too much when faced with negative signals can mean women write more readably than otherwise equivalent men despite their papers being accepted at similar rates. In this case, the gender readability gap is primarily caused by mis-information; it should therefore decline as women's information gets better. Alternatively, women may simply prefer writing more readably than men. In this case, there should be no obvious change in the gender readability gap as women gain a better understanding of peer review. A final possibility is that the gender gap increases with experience. This pattern of behaviour indicates that women revise their beliefs *upwards* about the standards they are being held to as they learn more about those standards. Assuming women make fewer mistakes about referees' and editors' thresholds as they gain experience in peer review, this would suggest that tougher standards play a role in how they make their choices.
Of course, arriving at this conclusion requires making several strong assumptions. First, it is assumed that [](#eq:EU) defines an author's optimal readability choice. Second, experience must indeed reduce information imperfections and asymmetries between the sexes. That is, [](#Theorem1)'s conclusion only applies if women make fewer mistakes about referees' and editors' standards as they gain more experience in peer review. (Or, at least, this statement become true at some point.) However, if women are unable to obtain the knowledge required to correct mistaken beliefs, then the gaps in women's information relative to men's will not necessarily decline---and could even increase---as women gain experience in peer review. For example, women could systematically mis-perceive a higher threshold, improve readability as a result, get accepted and then have no reason to update their (mistaken) beliefs. Moreover, this could then lead to learning so that future improvements in readability are lower cost for women than they are for men, thereby exacerbating gender differences in readability.
Third---and most critically---$i$'s and $k$'s papers must be identical on every dimension except readability. This assumption applies over $i$ and $k$'s entire lifespan and not just to a single point in time. As a result, it effectively rules out the possibility that $i$ and $k$ specialise over time in different dimensions of quality---*e.g.*, $i$ on readability and $k$ on, say, mathematical rigour---even while the general quality of their work is the same. (See [][Duration] for an indirect test of this hypothesis.)</Text>
            <Comments>See [#Hengel2017;] for a version of the model with a two stage refereeing process, where papers are either rejected or offered a revise and resubmit in the first stage and rejected or accepted in the second.
Effectively, this assumption rules out systematic mistakes in beliefs that are only corrected at the limit.
Authors probably care about getting their papers accepted and they may care about writing well, but their marginal utility from the intersection of the two events---*i.e.*, higher utility from writing well *only* because the paper is published in a top-four journal (as opposed to a top field journal or second-tier general interest journal)---is assumed to be negligible.
See [#Hengel2020;] for a version of the model where $c_{it}$ changes over time, *e.g.*, due to learning-by-doing.</Comments>
        </Document>
        <Document ID="3595D8FE-749A-4AED-BBAF-7FDFA16280A3">
            <Title>List of authors in each matched pair</Title>
            <Text>&lt;!--
\input{$HOME/Dropbox/Readability/draft/0-tex/generated/Table-J.2}
\clearpage
--&gt;</Text>
        </Document>
        <Document ID="A1E6ECD6-F16F-40ED-B48A-7CE5AB1C41BF">
            <Title>Mean \\(\widehat R_{k3}\\) (men)</Title>
        </Document>
        <Document ID="A05C7411-D19E-40DA-9752-0ACD086F6D56">
            <Title>Receivership</Title>
            <Text>Practically, receivership translates to (i) the lack of an automatic stay, (ii) the freedom to pledge an entire business as collateral and (iii) the ability to enforce a claim out-of-court. To identify the presence of each, I again turn to Doing Business data.
Although the definition of a floating charge is relatively straightforward, diagnosing its effective existence is not so. For example, many countries superficially possess legislation that grants this right but simultaneously prevent those securities from automatically including proceeds and inventories. Without that guarantee, other parties may control significant company assets upon default, making it harder to bypass formal bankruptcy. Thus, I assume floating charges are permitted only if the general description of an entire business---including its moveable and future/after-acquired assets---is a permissible form of collateral.
Data were obtained from questionnaire responses used to generate Doing Business's "strength of legal rights index". Columns one and two of [](#p2table4) count countries that do and don't permit the three components of receivership. The fourth column discusses within country variation.</Text>
            <Comments>[](#p2tableB9) ([][p2AppendixB]) lists the specific questions asked. 2012 and 2015 responses were scrapped directly from the Doing Business website using custom perl and python scripts. Because disaggregated data are made available only in the current year, 2010, 2011, 2013 and 2014 responses were comparably scrapped from dated snapshots of the Doing Business website taken by [Internet Archive](https://archive.org). Data between 2006--2009 were based on 2010 data and updated to incorporate reforms mentioned in those years' published reports. Due to methodological changes, only questionnaires pre-2013 asked specifically about the existence of an automatic stay for secured creditors. Those responses were applied to subsequent years and similarly updated for reforms.
\input{$PPATH/p2/tables/tex/p2table4}</Comments>
        </Document>
        <Document ID="2B8F28F0-5F53-4FE6-B84B-F6F3007EECE7">
            <Title>Institutions</Title>
            <Text>For every article I recorded authors' institutional affiliations. Individual universities in U.S. State University Systems were coded separately (*e.g.*, UCLA and UC Berkeley) but think tanks and research organisations operating under the umbrella of a single university were grouped together with that university (*e.g.*, the Cowles Foundation and Yale University). Institutions linked to multiple universities are coded as separate entities (*e.g.*, École des hautes études en sciences sociales).
In total, 1,039 different institutions were identified. For each institution, I count the number of articles in which it was listed as an affiliation in a given year and smooth the average over a five-year period. Institutions are ranked on an annual basis using this figure and then grouped to create fifteen dynamic dummy variables. Institutions ranked in positions 1--9 are assigned individual dummy variables. Those in positions 10--59 are grouped in bins of 10 to form six dummy variables. Institutions ranked 60 or above were collectively grouped to form a final dummy variable. When multiple institutions are associated with an observation, only the dummy variable with the highest rank is used, *i.e.*, the highest-ranked institution per author when data is analysed at the author-level and the highest-ranked institution for all authors when data is analysed at the article-level.</Text>
        </Document>
        <Document ID="325BC625-9B21-4A5F-A4EE-2544A74B6BCB">
            <Title>Solvency</Title>
            <Text>Presume first $D\le X_1$; the project is solvent. The entrepreneur decides whether to operate another period or liquidate. Should he continue, the project generates a second period's cash flow, $X_2$, after which it comes to the natural end of its life. It is shut down and the machine sold for $K_2$. Gross project value is
$$V_2^C=X_1+X_2+K_2.$$
Should he liquidate, all service and employment contracts are voided, business operations cease and the machine is sold for its full market value. Gross project value is instead
$$V_1^L=X_1+K_0.$$
In both scenarios, the creditor is repaid in full; his returns are $D$. The entrepreneur keeps what's left: $E_2^C=V_2^C-D$ if the project is continued and $E_1^L=V_1^L-D$ if it's liquidated.</Text>
        </Document>
        <Document ID="2D388486-9B1B-481D-8613-4488645916E0">
            <Title>Open review</Title>
            <Text>Academia's female productivity gap is as stubborn as the business world's pay gap; yet, if every paper a woman writes needs ***three to six more months*** to finish review, our "Publishing Paradox" seems much less paradoxical.
Is the answer double-blind review? Probably not. Double-blind review cannot stop referees from guessing authors' identities---which they did with surprising accuracy before the internet[#Blank1991], and presumably perfect accuracy after it.
Instead, eliminate single-blind review, too. A randomised controlled trial at the *British Journal of Psychiatry* suggests referee reports are better quality and less abusive when identities are known[#Walsh2000]. Posting them online---as the *British Medical Journal* does---virtually guarantees continuous, independent audits by outside researchers. Worries that reviews are less critical and/or relationships are strained are either unfounded or alleviated by the deep pool of referees common to general interest journals[#vanRooyen1999,vanRooyen2010]. Open review does incur costs---some people refuse to participate and those that don't spend marginally more time drafting reports[#vanRooyen1999,Walsh2000]---but if more accountability promotes fairer outcomes, ethical arguments in its favour should outweigh minor practical concerns.</Text>
            <Comments>Virtually every study on gender differences in scientific publishing rates find men more productive than women[for a list, see\]\[][#Ceci2014]. It's no different in my data: women published on average 1.7 articles; men managed 2.4---and with far more concentration in the distribution's right tail (for example, 56 men have published 16 or more times in the data, but no woman). Women produce fewer papers even when they don't have any children[#Ceci2014]. Appropriate controls for teaching and service do not account for it[#Xie2005], and it isn't a question of time, since female academics work just as many hours as men[#Ceci2014,Ecklund2011].
I find weak evidence suggesting the policy may have been effective before the late [1990s][NBER] but not [afterwards][AppendixDoubleBlind].
The *BMJ* posts reviewers' signed reports, authors' responses and the original manuscript on its website. No documentation is posted for rejected papers, but doing so may be beneficial: (i) A very public review implies a very public rejection; concern for one's reputation could reduce the number of low quality submissions. (ii) The onus of discovering mistakes would be shared with the wider economics community. (iii) Other journals can make publication decisions based on posted reviews---possibly reducing time spent refereeing for the discipline, as a whole. Women may receive greater scrutiny online---as they do at the *Guardian*[#Gardiner2016]---but the difference can be mitigated if comments are non-anonymous, made only by verified members of an appropriate professional society and continuously (and publicly) audited for bias in quantity and quality of feedback.
Each study employed a different research design; nevertheless, both estimate roughly 12 percent of reviewers decline to participate because they oppose open peer review while signing reports increases time spent on the review by 25 minutes. When referees were told their signed reviews might be posted online, time rose by an additional half hour and refusal rates were much higher (55 percent)[#vanRooyen2010].
A decision like this should be carefully considered based on a body of evidence and ideally extensively tested before officially rolling out.</Comments>
        </Document>
        <Document ID="1BD4AF95-951B-446B-8D6F-BA28F0445598">
            <Title>Online appendices</Title>
        </Document>
        <Document ID="E3D6BD99-A9C6-4E73-B83C-5854E795B7EB">
            <Title>Time.</Title>
            <Text>Receivership-like legislation may succumb to lengthy appeals or other delay tactics that restrain creditor action in the event of default. If so, it differs little from traditional bankruptcy. Thus, a final prerequisite is that the procedures themselves not take too much time.
Receivership permits prior contracting of control rights. By design, it should quickly transfer them in the event of default irrespective of the host country's level of development or institutional strength. When it doesn't, bankruptcy involves a significant period during which the firm continues operating under judicial supervision---*i.e.*, reorganisation. Creditors take this into account when issuing and pricing loans.
I define the threshold as two years or less based on Doing Business's "time required to recover debt" indicator. This indicator measures the number of years lenders need to recover their credit from a defaulting firm; it is based on responses to a hypothetical scenario involving a financially distressed hotel unable to make payments on a bank loan collateralised by its entire business.
The nine countries that possess the legislative and time components of receivership are: Albania (2007, 2013), Barbados (2010), Belize (2010), Fiji (2009), Ghana (2013), Jamaica (2010), Montenegro (2013), Nigeria (2007, 2014) and St. Lucia (2010). Each country, the years in which it was surveyed and whether it employed receivership or not are listed in [](#p2table5).</Text>
            <Comments>Even if the firm is shut down during this period, the fact that creditors are delayed in liquidating its assets, selling it to a third party or even operating it themselves reduces the value of their claim in a manner analogous to reorganisation.
If a country records no instance of an official debt enforcement procedure within the previous five years, Doing Business  assumes debt is never recovered (time to recover debt is recorded as "no practice"). Correspondingly, I assume these countries do not meet the two-year threshold.
\input{/Users/erinhengel/Dropbox/Thesis/p2/tables/tex/p2table5}</Comments>
        </Document>
        <Document ID="0F42141D-7EF3-42ED-8A99-7BA16F18625B">
            <Title>Double-blind review</Title>
        </Document>
        <Document ID="3647574A-8EE1-45C3-B993-32CBA73B91B4">
            <Title>Editorial policy</Title>
            <Text>To control for editorial policy, I recorded editor/editorial board member names from issue mastheads. *AER* and *Econometrica* employ an individual to oversee policy. *JPE* and *QJE* do not generally name one lead editor and instead rely on boards composed of four to five faculty members at the University of Chicago and Harvard, respectively. *REStud* is also headed by an editorial board, the size of which has been gradually increasing---from two members in the 1970s to 7--8 members today. Members are also located all over the world.
Editor controls are based on distinct lead editor/editorial boards---*i.e.*, they differ by at least one member. Among top four journals, 74 groups are formed in this manner. *REStud* adds another 34. Given the size of *Restud*'s editorial board and the fact that members serve fixed 3--4 full-year terms, editorial controls are highly correlated with year fixed effects. Moreover, unlike at *JPE* and *QJE*, editors are not located at the same institution. Thus, editor fixed effects may be less informative about editorial policy at *REStud* than they are for the other four journals.</Text>
        </Document>
        <Document ID="7F311019-A771-4CA6-82C1-66C7C5C6EC4F">
            <Title>publishing_female</Title>
            <Text>LaTeX input: draft-header
My Short Title: &lt;$projecttitle&gt;
My Subtitle: &lt;$custom:Subtitle&gt;
Author: &lt;$author&gt;
My Date: &lt;$custom:Date&gt;
Address: &lt;$custom:Address&gt;
Telephone: &lt;$telephone&gt;
Affiliation: &lt;$custom:Affiliation&gt;
Department: &lt;$custom:Department&gt;
Position: &lt;$custom:Position&gt;
Email: &lt;$custom:E-mail&gt;
Thanks Bitches: &lt;$custom:Thanks&gt;
File ID: &lt;$custom:CustomID&gt;
Keywords: &lt;$custom:Keywords&gt;
Subject: &lt;$custom:Subject&gt;
JEL: &lt;$custom:JEL&gt;
My Abstract: &lt;$custom:Abstract&gt;
Word Count: &lt;$wc&gt;
Base Header Level: 3
LaTeX input: draft-begin-single
LaTeX footer: draft-footer</Text>
        </Document>
        <Document ID="91D308F1-FCC7-4479-94C4-1AFE4E664CB2">
            <Title>Stylised facts</Title>
        </Document>
        <Document ID="C091CDEE-11CC-4EAF-92AA-166B5C1CB38D">
            <Title>abstract</Title>
            <Text>&lt;!--begin{abstract}

*Chapter 1*. &lt;$synopsis&gt;

*Chapter 2*. &lt;$synopsis&gt;

*Chapter 3*. &lt;$synopsis&gt;

\end{abstract}--&gt;</Text>
        </Document>
        <Document ID="0E445C7C-DD2B-439E-AF7B-5D6776E40B57">
            <Title>Distribution of female ratio</Title>
            <Text>The figure below [](#femratio_histograms) displays the distribution of female ratio across articles. The graph on the left includes all articles; the one on the right displays the distribution amongst only those articles with at least one female author. As is clear from the graph, the vast majority of papers published in economics journals are entirely male-authored. Conditional on having one female author, however, most articles are roughly 50 percent female-authored. Another concentration exists at 100 percent female-authored---largely reflecting solo-authored papers. A third concentration exists at between a third and a half female authors. Much smaller percentages of papers are roughly 25 percent female-authored and majority (but not entirely) female-authored.
&lt;!--
\input{$HOME/Dropbox/Readability/draft/tex/fixed/femratio_histogram}
\clearpage
--&gt;</Text>
        </Document>
        <Document ID="0E7D4589-611F-442D-850A-17258698B6E2">
            <Title>Suggestive descriptive evidence</Title>
            <Text>In this section, I show suggestive descriptive evidence that, on average, female authors satisfy [](#Theorem1)'s three conditions relative to men.
&lt;!--\input{$HOME/Dropbox/Readability/draft/0-tex/fixed/Figure-4}--&gt;
I first consider whether female-authored papers are accepted more often than male-authored papers (Condition 3). The articles in my data have already been published, so I cannot analyse gender differences in acceptance rates. Nevertheless, the topic has been extensively studied elsewhere. A recent study of four comparable journals suggests that exclusively male- and female-authored manuscripts receive a revise and resubmit decision 8 and 6 percent of the time, respectively[#Card2020]. [#Blank1991;] found that 12.7 and 10.6 percent of male- and female-authored papers were accepted at the *AER*. A study of *JAMA*'s editorial process indicated that 44.8 percent of referees accept male-authored papers as is or if suitably revised; 29.6 percent summarily reject them. Corresponding figures for female-authored papers were 38.3 and 33.3 percent[#Gilbert1994]. Studies from other disciplines find female-authored papers subjected to single-blind peer review are accepted less often than would be expected by chance[#McGillivray2018,Handley2015]. There appear to be no gender differences in acceptance rates to NBER's Summer Institute [#Chari2017]. See [][AppendixAcceptance] for a table summarising these and other studies.
As for Conditions 1 and 2, women write more clearly than men (Condition 1) and their future papers are more readable than their past papers (Condition 2). As shown in [][Underrepresentation], female-authored abstracts are 1–6 percent more readable than those by men. [](#figure7) plots an author's Flesch Reading Ease score against $t$, where $t=1$ for his first top-four publication, $t=2$ for his second, *etc.* As $t$ increases, women's readability improves whereas men's does not.
[](#tableH2_FemRatio) presents the marginal effect on female ratio (papers with fewer than 50 percent female authors are classified as male, see [][Gender]) for female authors ($\beta_1$) from estimating [](#equationX) on subsamples of authors with $t=1$, $t=2$, *etc.*:
&lt;!--\input{$HOME/Dropbox/Readability/draft/0-tex/equations/equation11}--&gt;
 where $R_{it}$ is the readability score for author $i$'s $t$th top-four publication, gender enters twice, $\text{male}_i$ and $\text{female ratio}_{it}$, to account for $i$'s sex and the sex of his co-authors, $\vect X_{it}$ is a vector of observable controls and $\varepsilon_{it}$ is the error term.
&lt;!--\input{$HOME/Dropbox/Readability/draft/0-tex/generated/Table-8-FemRatio}--&gt;
All figures in [](#tableH2_FemRatio) agree---women write better---but the magnitude and significance of that difference increases as $t$ increases. Between $t=1$ and $t=2$, the gap marginally widens but is not significant. After that, it triples (at least); the increase is significant ($p&lt;0.05$) for all five scores ([][AppendixEquality]). At higher publication counts, figures are less precisely estimated and smaller than in column 3, but still noticeably larger than estimates in columns 1 and 2.</Text>
            <Comments>As shown in [][AppendixSEUEmpirical], women's average readability scores are 1-5 percent higher than the readability of their first papers, their latest papers 1--7 percent higher. For a man, however, his average and last papers are more poorly written than his first.
Only 40 female authors have 4--5 publications in the data; just 28 have six or more.</Comments>
        </Document>
        <Document ID="0A2D3174-B366-4B3B-A1D4-777010690665">
            <Title>Indirect effect of higher standards-1</Title>
        </Document>
        <Document ID="15013BDF-CC07-41E3-9FFA-FD763B85A9C2">
            <Title>Untitled</Title>
        </Document>
        <Document ID="B3E6EB0F-D83F-4D2A-878C-6177EF23C8E5">
            <Title>[](#table11NEWXX), including Condition 1</Title>
            <Text>To remain consistent with a similar regression in [](#table11), [](#table6) does not include author productivity effects. [](#tableD12) reproduces [](#table6) including these effects. Coefficients and standard errors vary little from those in [](#table6).
&lt;!--\clearpage--&gt;</Text>
        </Document>
        <Document ID="36E230FA-12D1-411C-A211-AB71950AFFEE">
            <Title>New Folder</Title>
        </Document>
        <Document ID="8173FF92-6FD0-4429-A7DA-457ABF1D6EE8">
            <Title>Condition 2</Title>
        </Document>
        <Document ID="FEECFE95-BE8C-47D4-9AFF-717E472573AE">
            <Title>Rescue Culture</Title>
            <Synopsis>In a comprehensive debt-financing model, I show ``rescue''-centric bankruptcy anticipates motives creditors do not have. Although out-of-court settlements dominate and formal procedures are rarely used, insolvent borrowers nevertheless extract concessions from creditors unless supervised reorganisation is 100 percent free. They pay for it, of course, with inefficient continuation and liquidation, expensive debt and credit-rationing. Ironically, rescue culture hits hardest the firms it purports to save yet benefits creditors, whom it claims to check. Firms with long expected lifespans are either the first denied credit or the first to make investment choices that guarantee premature liquidation. Creditors, on the other hand, enjoy a profitable niche lending market in an otherwise perfectly competitive industry.</Synopsis>
        </Document>
        <Document ID="35E34F9E-C912-4C4C-AF35-EEB8D22B5720">
            <Title>`Textatistic`</Title>
            <Text>I wrote the Python module `Textatistic` to transparently calculate the readability scores in this study. The code and documentation are available on [GitHub](https://github.com/erinhengel/Textatistic); I provide a brief description here.
To determine sentence count, the program replaces common abbreviations with their full text, decimals with a zero and deletes question and exclamation marks used in an obvious, mid-sentence rhetorical manner. The remaining full stops, exclamation and question marks are assumed to end a sentence and counted.
Next, hyphens are deleted from commonly hyphenated single words such as "co-author" and the rest are replaced with spaces, remaining punctuation is removed and words are split into an array based on whitespace. Word count is the length of that array.
An attempt is made to match each word to one on an expanded Dale-Chall list. The count of difficult words is the number that are not found. This expanded list, available on [GitHub](https://github.com/erinhengel/Textatistic), consists of 8,490 words. It is based on the original 3,000 words, but also includes verb tenses, comparative and superlative adjective forms, plural nouns, *etc.* It was created by first adding to the Dale-Chall list every conceivable alternate form of each word using Python's Pattern library. To eliminate nonsense words, the text of 94 English novels published online with Project Gutenberg were matched with words on the expanded list. Words not found in any of the novels were deleted.
Syllable counts are based on the C library `libhyphen`, an implementation of the hyphenation algorithm from[#Liang1983;]. [#Liang1983;]'s algorithm is used by &lt;!--\TeX--&gt;'s typesetting system. `libhyphen` is employed by most open source text processing software, including OpenOffice.</Text>
            <Comments>Abbreviations which do not include full-stops are not altered. I manually replaced common abbreviations, such as "*i.e.*" and "U.S." with their abbreviated versions, sans full stops.
For example, "?)." is replaced with ").".
Per[#Chall1995;], hyphenated words count as two (or more) words.</Comments>
        </Document>
        <Document ID="CACF615D-68B0-4551-8F20-FBCBC9562504">
            <Text>[](#table9)'s first row compares equivalent authors (holding experience constant): senior female economists write more readably than their male counterparts with identical experience. [](#table9)'s last two rows compare authors before and after gaining that experience (holding gender and preferences constant): women write more clearly once they "learn the ropes" in peer review; equivalent men do not. Meanwhile, lifetime publication counts---a crude approximation for acceptance rates---indicate men's more poorly written papers are accepted at least as frequently as women's.
&lt;!--\input{$HOME/Dropbox/Readability/draft/tex/generated/table9}--&gt;
</Text>
            <Comments>See [](#Footnote67) for a discussion of the limitations of using publication counts to proxy for acceptance rates. Please also refer to [][SEUEmpirical] for a more thorough review of the (substantial) prior research on gender neutrality and journals' acceptance rates. It too finds no female advantage in journals' acceptance rates.</Comments>
        </Document>
        <Document ID="E00827F0-A403-45E5-AF50-679ACC49059D">
            <Title>Quantifying the counterfactual</Title>
            <Text>Evidence in the previous section suggests women satisfy [](#Theorem1)'s three conditions relative to men, on average. However, included controls undoubtedly fail to fully account for differences in the non-readability aspects of men's and women's papers (Assumption 2). Furthermore, concluding that higher standards are present actually requires that all three of [](#Theorem1)'s conditions hold for the same author---that is, the same woman must write better than an equivalent man, not be accepted at rates higher than he is *and* raise the quality of her writing over time.
In this section, I attempt to improve my estimates in both respects by restricting the sample to authors with three or more top-four publications. I then match observably similar male and female economists based on characteristics that predict the topic, novelty and quality of their research. Within-person readability comparisons are used to determine if Condition 2 was satisfied for each author in a matched pair. Between-person comparisons after authors gain experience in peer review are used to establish whether Condition 1 was satisfied for the male or female member. I then use these results to construct a counterfactual estimate of the impact higher standards have played in women's readability choices.</Text>
        </Document>
        <Document ID="0B4EE5DC-9826-447C-83FE-33CBE62B4D6C">
            <Title>Exclusively female-authored</Title>
            <Text>&lt;!--
\begin{vplace}[0.7]
\input{$HOME/Dropbox/Readability/draft/0-tex/generated/Table-3-Fem100}
\end{vplace}
\clearpage
\input{$HOME/Dropbox/Readability/draft/0-tex/generated/Table-5-Fem100}
\clearpage
\input{$HOME/Dropbox/Readability/draft/0-tex/generated/Table-6-Fem100}
\clearpage
\input{$HOME/Dropbox/Readability/draft/0-tex/generated/Table-7-Fem100}
\clearpage
\input{$HOME/Dropbox/Readability/draft/0-tex/generated/Table-8-Fem100}
\clearpage
\input{$HOME/Dropbox/Readability/draft/0-tex/generated/Table-F.2-Fem100}
\clearpage
--&gt;</Text>
        </Document>
        <Document ID="186E4648-6F29-477E-B553-86C6CCD7A99D">
            <Title>Does peer review affect readability?</Title>
        </Document>
        <Document ID="EF1B19EC-C260-447E-81A6-5CA09E20A791">
            <Title>rescue_culture</Title>
            <Text>LaTeX input: dissertation-header
Title: &lt;$title&gt;
My Short Title: &lt;$custom:ShortTitle&gt;
Author: &lt;$author&gt;
Affiliation: &lt;$custom:Affiliation&gt;
Email: &lt;$custom:E-mail&gt;
My Abstract: &lt;$synopsis&gt;
Thanks Bitches: &lt;$custom:Thanks&gt;
File ID: &lt;$custom:CustomID&gt;
Keywords: &lt;$custom:Keywords&gt;
Subject: &lt;$custom:Subject&gt;
Base Header Level: 2
LaTeX input: dissertation-begin-single
LaTeX footer: dissertation-footer</Text>
        </Document>
        <Document ID="C9C72080-660C-4D9D-934B-A2CA475674D9">
            <Title>Robustness</Title>
        </Document>
        <Document ID="E8B47CC6-8DB8-4F9F-A5E1-AADFB743A31C">
            <Title>Measuring discrimination</Title>
            <Text>Estimates presented in [](#table11NEW) suggest Conditions 2 and 3 are satisfied *on average*, but [](#Theorem1) technically requires more: *i.e.*, that they are satisfied for the same author, on average. I find Condition 2 and 3 are twice as likely to be simultaneously satisfied for a female author than they are for a male author: in roughly half of all matched pairs, the female author satisfied both Conditions 2 and 3 simultaneously; in another quarter the male author satisfied both conditions simultaneously. In the remaining matched pairs, neither male nor female author simultaneously satisfied both criteria, rendering the test in [](#Theorem1) inconclusive. This suggests female authors are twice as likely to experience discrimination according to [](#Theorem1) than otherwise equivalent male authors.
&lt;!--\input{$PPATH/tables/tex/table11NEWXX}--&gt;
It is also possible to generate a conservative measure of the average effect of discrimination over all matched pairs in which one member satisfies Conditions 2 and 3---and thus, determine whether the average effect leans against men or women. This is the average of the following figure: $$D_i=\bm1_i\big(R_{i3}-\max\left\{R_{i1},R_{k3}\right\}\big),$$ where $\bm1_i$ equals 1 if the author is female and -1 if the author is male.
Consider a matched pair for which Conditions 2 and 3 are satisfied, suggesting discrimination against $i$. The raw estimate of final gender difference in readability is $R_{i3}-R_{k3}$. This reflects actual discrimination only if author $i$ would otherwise voluntarily choose readability $R_{k3}$ if the probability of acceptance at that readability were identical to her probability of acceptance at readability $R_{i3}$. If $R_{k3}&gt;R_{i1}$, then this is true: $R_{i1}$ forms an upper bound on the choice of readability author $i$ would make in the absence of peer review---*i.e.*, left to her own devices, we know author $i$ would choose some $R\le R_{i1}$. If $R_{k3}&lt;R_{i1}$, however, then we do not know if $i$ would otherwise voluntarily choose readability $R_{k3}$ if the probability of acceptance at that readability were identical to her probability of acceptance at readability $R_{i3}$---we only know that if that statement were true, she would choose no higher readability than $R_{i1}$.
The first two columns of [](#table11NEWXX) shows average $D_i$ for male and female authors. Mean $D_f$ is noticeably higher than mean $D_m$ in absolute value (although not always significantly so) in addition to being experienced by twice as many female authors than male authors. The final column of [](#table11NEWXX) averages columns (2) and (3) (weighted by the number of duplicate male observations in the sample). In all five scores it is positive and significant, indicating that, on average, discrimination is more likely to affect female authors; on average, it causes readability scores to rise by about 5--10 percent in female-authored papers.</Text>
        </Document>
        <Document ID="75C4F160-4DF9-41FD-9C13-DA0A6A57037F">
            <Title>Strategic default</Title>
            <Text>In this section, I illustrate that strategic default does not affect lending when reorganisation is costless. In a minority of cases, judicial error may lead to premature liquidation.
Let $Y=0$. Absconding with more than $X_1$ requires surreptitious liquidation that is a serious breech of contract and  probably constitutes fraud. I assume its punishment is enough to deter it entirely. Additionally, $X_1&lt;D$ triggers insolvency and if the creditor desires, court supervision implied by bankruptcy. Thus, strategic default is possible only up to $D\le X_1$.
Consider non-viable firms. Since $Y=0$, $\ol V_1^C&lt;\ol V_1^B$; per [](#prop1), insolvency implies liquidation or bankruptcy. For all $D\le X_1$, however, $\ol E_1^B&lt;E_1^L$. The firm does not strategically default.
Consider now viable firms. As discussed in [][p1Insolvency], for some $X_1$ entrepreneurs are better off in a workout than they would be continuing while solvent---creating motive for strategic default.  Nevertheless, creditors expect at least $K_0$ even in reorganisation when $X_1$ is naught. Since reorganisation is by definition court-supervised and the judiciary assumed ethical, worst-case-scenario creditors turn to it for future oversight. Regardless, viable firms always get credit. If bankruptcy guarantees reorganisation when creditors favour it or if entrepreneurs cannot steal time 2 earnings, firms efficiently continue. Otherwise, judicial error may cause prematurely liquidating a minority of them.</Text>
            <Comments>In an alternative scenario, the entrepreneur could steal all of $X_1$. Using analogous arguments, non-viable firms always obtain credit, whereas viable firms did anyway per [](#prop1)(ii).
$Y=0$ implies no corruption, *i.e.*, an ethical judiciary.</Comments>
        </Document>
        <Document ID="E915A4FA-023A-4914-82FC-7AE0809A6829">
            <Title>[](#table6), equal weighting</Title>
            <Text>In order to create author time series, article-level data were duplicated by article $j$\'s co-author count, $N_j$. Each duplicate observation was assigned a single author. [](#table6) weights authors' observations by $1/N_j$---papers with fewer co-authors are weighted more heavily because they've been duplicated fewer times. [](#tableD13) weights all author-level observations equally. Doing so does not meaningfully affect the analysis.
&lt;!--\input{$HOME/Dropbox/Readability/draft/tables/tex/tableD13}--&gt;</Text>
            <Notes>&lt;!--\clearpage--&gt;</Notes>
        </Document>
        <Document ID="73A9C97A-D69D-4052-A3E9-67629AAC9D50">
            <Title>dissertation</Title>
            <Text>LaTeX input: dissertation-header
Title: &lt;$projecttitle&gt;
Author: &lt;$author&gt;
Email: &lt;$custom:E-mail&gt;
Affiliation: &lt;$custom:Affiliation&gt;
Abstract One: &lt;$synopsis&gt;
Abstract Two: &lt;$synopsis&gt;
Abstract Three: &lt;$synopsis&gt;
Thanks Bitches: I would like to thank Jeremy Edwards, Pramila Krishnan, Sönje Reiche, Melvyn Weeks and, especially, my supervisor Christopher Harris. Their thoughtful input has made this dissertation much better. I am also grateful to my dissertation examination committee, Prof. Hamish Low and Prof. Leonardo Felli for instructive comments.
My Disclaimer: This dissertation is the result of my own work and includes nothing which is the outcome of work done in collaboration. It is not substantially the same as any that I have submitted or is being concurrently submitted for a degree or diploma or other qualification at the University of Cambridge or any other University or similar institution. I further state that no substantial part of my dissertation has already been submitted or is being concurrently submitted for any such degree, diploma or other qualification at the University of Cambridge or any other University or similar institution.
My Word Count: This dissertation does not exceed the prescribed word limit for the Faculty of Economics Degree Committee.
Base Header Level: 2
LaTeX input: dissertation-begin
LaTeX footer: dissertation-footer</Text>
        </Document>
        <Document ID="C68D3686-72FF-47AB-BF44-37CC33B9B14C">
            <Title>Women's papers improve more during peer review</Title>
        </Document>
        <Document ID="2898D675-9504-4E38-8828-5B4B5EED218D">
            <Title>Conclusion</Title>
            <Text>Most raw numerical counts suggest women produce less than men: female real estate agents list fewer homes[#Seagraves2013]; female lawyers bill fewer hours[#Azmat2017]; female physicians see fewer patients[#Bloor2008]; female academics write fewer papers[#Ceci2014]. When evaluated by narrowly defined quality measures, however, women often outperform: houses listed by female real estate agents sell for higher prices[#Salter2012,Seagraves2013]; female lawyers make fewer ethical violations[#Hatamyar2004]; patients treated by female physicians are less likely to die or be readmitted to hospital[#Tsugawa2016].
As I argue in this paper, female economists surpass men on another dimension: writing clarity. Abstracts written by women are 1--6 percent more readable than similar abstracts by men. They also become 2--5 percent more readable while under review when referees aren't blinded to authors' identities. Furthermore, the cost to women of revising their papers appears to be much higher than the cost to men: female-authored papers spend 3--6 months longer under review compared to observably equivalent male-authored papers. Finally, it does not appear that women are rewarded for their better writing: recent evidence from a set of comparable journals suggests female-authored papers are accepted at lower rates, conditional on quality[#Card2020].
To interpret these stylised facts, I model an author's decision-making process as if it were governed by the rational behaviour of women who update their beliefs about the readability thresholds they are held to as they gain experience in peer review. I then derive three testable conditions which can help establish whether higher standards are at all important to the existence and evolution of the gender readability gap: (1) experienced women write better than equivalent men; (2) women improve their writing over time; and (3) female-authored papers are accepted no more often than equivalent male-authored papers.
On average, I find that all three conditions hold: women's writing gradually gets better but men's does not; between authors' first and third published articles, the readability gap increases by up to 12 percent; as already mentioned, female-authored papers are not accepted at higher rates after conditioning on similar co-variates[#Card2020]. I then conduct a counterfactual analysis that exploits within- and between-individual variation in readability among well-published economists. Its results suggest that higher standards lead women to write, on average, 5 percent more readably than they otherwise would.
I emphasis, however, that these conclusions are predicated on several strong assumptions; if any are violated then other hypotheses are also consistent with the data. For example, if matching does not fully account for differences in the non-readability aspects of men's and women's papers, then gender differences in readability may be influenced by gender differences in specialisation over time. Similarly, no control perfectly captures how empirical vs. theoretical a paper is; as a result, the gender gaps I observe may be biased by differences between fields. Additionally, the validity of the counterfactual analysis and the precision of its estimates rely on strong assumptions about men's and women's beliefs and the impact co-authors have on an article's readability.
Higher standards, wherever present, reduce women's labour market opportunities. Work that is evaluated more critically at any point in the production process will be systematically better (holding prices fixed) or systematically cheaper (holding quality fixed). This will reduce women's wages, distort measurements of their productivity and negatively impact their labour market outcomes. For example, if judges require better writing in female-authored briefs, female attorneys must charge lower fees and/or under-report hours to compete with men; billable hours and client revenue will decline, making female lawyers appear less productive than they truly are. In academia, higher standards coupled with longer peer review times likely affect women's probability of obtaining tenure.
Unfortunately, there are no easy solutions for addressing higher standards. But least intrusive---and arguably most effective---is simple awareness and constant supervision. I hope journals are challenged to address the tougher standards they likely impose on women, open to policies that transparently monitor them and supportive of research that helps us better understand them.</Text>
        </Document>
        <Document ID="53EF818D-92E4-43C2-8C5A-A012C5E98E9A">
            <Title>Proofs</Title>
            <Text>&lt;!--
\input{$HOME/Dropbox/Readability/draft/0-tex/theorems/proofs}
\clearpage
--&gt;</Text>
        </Document>
        <Document ID="2CD510B8-DC99-4C58-8939-92490FC7BEE6">
            <Title>Experience and review times</Title>
            <Text>In [][IndirectEffectInterpretation] I find evidence suggesting that inexperienced female economists go through the toughest review, conditional on acceptance. To investigate further, I test the impact of experience on time spent in review by re-estimating [](#equation16) on sub-samples of junior ($t=1$) and senior ($t&gt;1$) authors. Results are displayed in [](#figure11). They suggest papers by junior women do indeed take longer in review; the gender gap is significantly smaller---albeit still positive---for senior women.
&lt;!--
\input{$HOME/Dropbox/Readability/draft/0-tex/fixed/Figure-K.1}
\clearpage--&gt;</Text>
            <Comments>Three notes on estimation. First, in [][Quantification], I define "experienced" as $t=3$. However, most female-authored papers published in *Econometrica* and *REStud* are by women with no (or only one) previous top publication; only 24 have two or more previous papers and were the most senior co-author on a $t&gt;2$ paper. Second, to eliminate confounding by more senior co-authors, I restrict the sample to the senior authors on a paper (*i.e.*, authors satisfying $\text{max. } t=t$). (Including these observations does not substantially impact results or conclusions.) Third, because the sample includes data from *REStud*, readability, motherhood and childbirth controls are not included. See the [August 2018](http://www.erinhengel.com/research/publishing_female20180828.pdf) version of this paper for results that control for these factors (based on data from *Econometrica* alone).</Comments>
        </Document>
        <Document ID="1D724DCE-7C3F-4B21-928C-A6E7D99D4CA0">
            <Title>Results</Title>
            <Text>&lt;!--\input{$HOME/Dropbox/Readability/draft/0-tex/generated/Table-4}--&gt;
[](#table5) compares textual characteristics between a paper's draft and final versions in the samples of male-authored (female ratio below 50 percent) and female-authored manuscripts (female ratio at or above 50 percent). It suggests abstract text is altered during peer review. According to the first panel, draft abstracts are longer---more characters, words and sentences---and denser---more syllables, polysyllabic words and difficult words. The biggest changes are made to female-authored papers: figures in column six are 20--30 percent higher (in absolute value) than those in column three. The second panel of [](#table5) suggests women's papers become more readable during peer review relative to men's. More generally, they also seem to indicate that peer review improves readability, although results are less clear for male-authored papers.
[](#table6_FemRatio)'s first panel displays results from OLS estimation of [](#equation2). Conditional on draft readability, published female-authored papers are more readable than published male-authored papers. Moreover, published article readability positively correlates with draft readability: coefficients on $R_{jW}$ are about 0.8 and consistently highly significant.
&lt;!--\input{$HOME/Dropbox/Readability/draft/0-tex/generated/Table-5-FemRatio}--&gt;
[](#table6_FemRatio)'s remaining columns show results from the two strategies presented in [][NBERIdentification]. The first strategy regresses each paper's change in score on its ratio of female authors (papers with fewer than 50 percent female authors are classified as male, see [][Gender]). As already discussed, an advantage of this strategy is that it more effectively removes the impact of confounding factors---*e.g.*, research field---that are constant between manuscript versions. The FGLS strategy estimates the coefficient on female ratio separately among the sample of working papers and published articles; the impact of gender on the readability gap formed during peer review is the difference between them. The advantage of this strategy is that it allows us to observe an estimate of the gap both before and after peer review.
Results from the first strategy are shown in panel two; results from the second are shown in panel three. Both strategies' estimates of the effect of gender formed during peer review are very similar (columns 4 and 10). They suggest that female-authored abstracts become 2--5 percent more readable while under review. FGLS results further indicate that the gender readability gap is 2--3 times larger in papers' published versions than it was in their pre-print versions.
Interestingly, although citations and abstract readability generally positively correlate with one [another][AppendixReadability], the relationship between citations and the *change* in readability between draft and final versions of a paper is either negative or [zero][AppendixDraftCorr]. Although I do not observe how many citations papers would have received had they not gone through peer review, these results tentatively suggest that the revisions women are asked to make during the process may not improve the general quality of their papers as proxied for by citations.
Also included in [](#table6_FemRatio) are coefficients on the interaction between female ratio and a dummy variable equal to 1 for papers that underwent double-blind review before the internet. These estimates consistently suggest that the gender readability gap reversed direction when papers were subjected to blind review, although none are statistically significant at traditional levels. In [][AppendixEventStudy], I plot average residuals over time for papers published in the *AER* or *QJE* before and after they introduced (or removed) double-blind review from a regression of the differenced readability scores on the ratio of female authors. As the figures illustrates, there is a clear discontinuity in women's average unexplained changes to readability when journals switched to single-blind review (or the internet was introduced). For men, however, unexplained changes do not appear to have been substantially affected by double-blind review, conditional on included controls.
The number of manuscripts---and especially female-authored manuscripts---subjected to double-blind review is small so the coefficients on the interaction between blind review and the ratio of female authors on a paper should be interpreted with caution. Nevertheless, they do provide some (weak) indication that the gender readability gap at least partially results from factors outside of women's control, *e.g.*, editorial and refereeing bias. They may also suggest that masking authors' identities can help reduce peer review's impact on the gender readability gap---but only under certain circumstances. In [][AppendixDoubleBlind], I analyse the policy's post-internet impact. Gender differences are positive regardless of a journal's official review policy, suggesting that double-blind review may be effective only as long as authors are not identifiable by other means.</Text>
        </Document>
        <Document ID="69D1654F-12AF-4843-8260-D48E333BB8F8">
            <Title>\\(Y\\) and \\(p\\)</Title>
            <Text>To measure $Y$ and $p$, I use World Bank Doing Business data. Doing Business indicators cover the cost and complexity of complying with business-related administrative procedures and regulations. They are estimated based on responses to a set of standardised, hypothetical scenarios posed to accountants, lawyers and other relevant local experts. 
The Doing Business's "cost of enforcing contracts" proxies for $Y$. It aggregates attorney, court and enforcement fees needed to resolve a commercial dispute. The final figure reflects those costs as a percentage of the claim in question.
For $p$, I construct an index of the depth, coverage and quality of financial information available on individuals and firms in a country. It is calculated per [](#p2equation1):$\text{Public registry}_{jt}$ is the Doing Business's "credit registry coverage"---the percentage of individuals covered by a public data registry for country $j$ in year $t$. $\text{Private bureau}_{jt}$ is its "credit bureau coverage"---the corresponding coverage of a private data bureau. $\text{Depth of credit info. index}_{jt}$ is the Doing Business's "depth of credit information index". It measures the comprehensiveness of credit-related data in the economy; $S_t$ divides by 6 for years prior to 2013 and 8 thereafter to account for methodological changes.
[](#p2table3)summarises these variables. On average, resolving a financial dispute costs 36.7 percent of the underlying claim in the countries surveyed by the WES, slightly above the 2014 global average of 33.5 percent. Values range between 0 and 1.63. The latter distinction goes to East Timor in 2009; the former to Bhutan that same year.
The credit information index ranges between 0 and 1. In a given year, roughly half of the WES countries have the minimum value. Argentina scored 1 in 2006 and 2010; Honduras and Uruguay in 2010. The mean is 20.6 percent---almost seven points lower than the 2014 global average (27.7 percent).  This difference mainly reflects limited private bureau coverage in WES countries (global average 27.3 percent) and, to a lesser extent, a lower rating on the depth of credit information index (global average 49.0). WES countries surpass the global average in public registry coverage (10.1 percent).</Text>
            <Comments>Doing Business also publishes a measure of the average cost of bankruptcy proceedings. Unfortunately, the data is based on a highly stylised case study of a viable firm and is calculated conditional on  specific bankruptcy procedures without explicitly mentioning what those procedures are. The cost of enforcing contracts, on the other hand, records many of the same fees associated with reorganisation---court costs, attorney fees and eventual sale of the disputed asset---thus proxying for its price while remaining independent of its use. Unlike the average cost of bankruptcy measure, it is also available for all WES countries.
\input{$PPATH/p2/equations/p2equation1}
$t$ corresponds to the year in which data are collected---generally the year prior to their publication.
&lt;!--\label{p2footnote2}--&gt;The depth of credit information index was revised in 2014 but backwardly updated only to a single year. The previous methodology scored countries based on six criteria; the new methodology appends two more features.
\input{$PPATH/p2/tables/tex/p2table3}
Simple average of 189 economies covered in the Doing Business database.
Values above 1 indicate resolving a commercial dispute costs more than the value of the underlying claim.</Comments>
        </Document>
        <Document ID="2B88AA6F-A936-48C0-AA27-E8C6CF055C86">
            <Title>Credit rationing.</Title>
            <Text>Unfortunately, borrowing isn't guaranteed. A higher $D$ affects risk twofold: (i) it increases entrepreneurs' chances of default; and (ii) reduces their desire to liquidate. Both effects result in more workouts and bankruptcy. Making either more probable reduces $\ol C_0$. It is entirely possible $\ol C_0$ does not cover the creditor's initial outlay no matter what $D$ the entrepreneur is willing to pay. Credit is rationed.
PROPOSITION
(i) If $\,\ol V_1^B\le\ol V_1^C$, some firms are credit rationed; the creditor is willing to lend if and only if $K_0\le\,\sup_D\ol C_0$. (ii) If $\,\ol V_1^C&lt;\ol V_1^B$, creditors are willing to lend at any $K_0\le D$ on the extended real number line such that $E_2^R=0$ for all $X_1$, $X_2$.
eprop
Credit rationing per [](#prop2)(i) happens to both viable and non-viable firms. For the former, criteria are straightforward. Because $\ol C_0$ is increasing in $D$, its supremum is its limit as $D$ tends to infinity; viable firms are credit-rationed whenever
$$!e[p1equation4]\lim_{D\ra\iy}\ol C_0=\int\!\lim_{D\ra\iy}\ol C_1^B\,\dd\hat\Pi=\ol V_0^B&lt;K_0.e!$$
Viable projects are funded only when their time 0 expected returns in bankruptcy are enough to cover the loan's principal. When it can't, the firm is credit-rationed.
Credit rationing is actually less of a problem for non-viable projects. For a start, more are liquidated in bankruptcy. As shown in the proof of [](#prop2),
$$!e[p1equation5]\ol C_0=\ol C_0^B+\int_{\mc{L^\star}}\!\l(D-\ol C_1^B\r)\,\dd\hat\Pi,e!$$
 where $\mc{L^\star}$ is the set of outcomes in which the firm is voluntarily liquidated. When $D$ tends to infinity, every firm either engages in a workout or goes bankrupt, making [](#p1equation4) also the limit of [](#p1equation5). Because $q=1-p$, however, non-viable projects more easily attain $K_0\le\ol V_0^B$ than do viable ones for project parameters within a sufficiently small neighbourhood of $\ol V_1^C=V_1^L$. And although those that satisfy [](#prop2)(i) do not go bankrupt, their higher probability of being liquidated if they did increases the value of creditors' outside option during a workout. Entrepreneurs have less scope to extract concessions *ex interim*. They are rewarded with better lending opportunities *ex ante*.
Lending to non-viable firms is safer for another reason: they often forgo workouts to voluntarily liquidate. When loaning money to viable projects, creditors expect $\ol C_0^B$; when lending to non-viable projects they earn $\ol C_0^B$ plus a "bonus"---the integral in [](#p1equation5)---pushing the supremum of [](#p1equation5) beyond [](#p1equation4). Even if judges reorganised non-viable and viable projects at the same rate, the latter would still have a tighter credit market. From the creditor's perspective, liquidation is safer so lending is safer when entrepreneurs want it more often.
Arguably, one group of non-viable firm has no problem ever getting credit---those that sometimes file for bankruptcy ([](#prop2)(ii)). Any firm guaranteed to liquidate at time 1 is guaranteed a loan. With workouts on the table, however, this promise is not credible *ex ante* because lenders capitulate *ex interim*. For those that go bankrupt, however, creditors resist. Couple that resistance with an excessively high $D$ and entrepreneur returns in bankruptcy are exactly what they would be in liquidation: nothing. Assuming firms choose the efficient option when their earnings from doing so are no worse than the alternatives, liquidation at time 1 is guaranteed.
Entrepreneurs take out high interest rate loans per [](#prop2)(ii) only as a last resort. Creditors, on the other hand, love them ([](#cor1)). They are risk-free, yet generate a long-run rate of return above the risk-free rate. Not bad for a zero-profit industry.
COROLLARY
Loans made per \autoref{prop2}(ii) satisfy $\ol E_0=0$ and $K_0&lt;\ol C_0$.
ecor
Creditor profits are a byproduct of guaranteed liquidation. Lending is predicated on avoiding bankruptcy and exorbitant rates do this---but only by conferring the creditor absolute rights to $X_1$. Since creditor profits are a side effect rather than the source of lending, no amount of competition will drive them back to zero.</Text>
            <Comments>$C_2^B\le D$ for all $X_1$, $X_2$ making the integral in [](#p1equation5) non-negative.
The real world relevance of [](#prop2)(ii) depends crucially on whether firms actually do liquidate under such conditions. Evidence in the home mortgage market suggests they do. Lenders are more frequently and rapidly transferred ownership of houses from delinquent borrowers with little to no equity in their homes[#Ambrose1998,Pennington-Cross2010]. Borrowers' quick surrender may be especially likely when bankruptcy incurs a personal cost, such as lost time. See also [](#p1footnote3) for a discussion on the likelihood of voluntary liquidation and the entrepreneur's original intent to liquidate.
Note that $\ol V_1^C&lt;\ol V_1^B$ is not sufficient to guarantee loans are made as in [](#prop2)(ii)---only that such loans exist. Cheaper loans, if obtainable, are preferred ([](#cor1)).</Comments>
        </Document>
        <Document ID="F62E35A4-0A8E-4694-884A-B07FD59166D5">
            <Title>New Folder</Title>
        </Document>
        <Document ID="15388DBA-0041-4429-BB18-6B7FF29D2856">
            <Title>Super-senior financing</Title>
            <Text>Let lower support for $X_1$ be $X_1^L&lt;0$; $X_2$ remains non-negative.  Consider the entrepreneur of a viable firm who obtained a loan at face value $D_1$ and earned $X_1&lt;0$ in time 1.  To continue operating, he must finance first period losses. I assume loans taken out in this manner are legally conferred seniority to all existing debt but obligations in the original debt contract award existing creditors right of first refusal.
Allowing super-senior financing prevents creditors from blocking new loans---should existing debt holders refuse, the borrower can turn to the wider lending market. Thus, creditors evaluate the new loans independent of their previous stake. They extend one if there exists some $D_2$ such that
$$!e[p1equation12]\EE_1\l[\min\{D_2, X_2+K_2\}\r]=-X_1.e!$$
[](#p1equation12) is increasing in $D_2$. $D_2$ exists if and only if $-X_1\le\ol X_2+K_2$, where $\ol X_2$ is the expected value of $X_2$. If $D_2$ does not exist for the original creditor, it does not exist for any creditor. Without necessary financing, the firm is liquidated.
When evaluating the original loan at time 0, the entrepreneur expects $D_1$ when $D_1\le X_1$. If $X_1$ falls below that but above 0, the project is insolvent. In the absence of bankruptcy costs, workout negotiations favour the creditor since reorganisation is equivalent to continuation and liquidation guarantees recovery of his initial investment; without loss of generality I assume both parties agree to continue operating the firm but do not otherwise modify the original loan contract. Creditor returns are
$$\EE_1\l[\min\{D_1,X_1+X_2+K_2\}\r].$$
If $X_1$ is less than 0 but more than $-(\ol X_2+K_2)$, the creditor loans $-X_1$ per [](#p1equation12). His expected returns at time 1 are
$$\EE_1\l[\min\{D_1+D_2,X_2+K_2\}\r]+X_1.$$
When $X_1$ falls below $-(\ol X_2+K_2)$ the project is liquidated. Since the firm is deemed "viable", $K_0&lt;-(\ol X_2+K_2)$. The creditor earns nothing.
In every scenario, creditor returns increase in $D_1$. Their supremum occurs at the limit as $D_1$ tends to infinity, *i.e.*,
$$!e[p1equation13]\ol X_1+\ol X_2+K_2,e!$$
 where $\ol X_1$ is the expected value of $X_1$. Since the firm is viable, [](#p1equation13) is more than $K_0$. Applying similar arguments used to prove [](#prop2), a $D_1$ exists that satisfies $\ol C_0=K_0$. The lending conditions for viable firms are not fundamentally altered when bankruptcy law allows super-senior financing.</Text>
            <Comments>Since neither creditor nor entrepreneur are responsible for time 2 losses, the assumption $0\le X_2$ is made for explanatory ease. If $X_2$ were negative, an equivalent non-negative random variable and corresponding probability mass function exist that assign 0 for all $X_2$ such that $X_2+K_2&lt;0$ and $X_2+K_2$, otherwise.
Non-negative returns at time 1 introduces a nuance to "viability". In fact, the firm was *not* viable---only expected to be. After realisation of time 1 earnings, it is better to liquidate, since project earnings are not high enough to cover the costs required to keep it in operation.
Note that even if the firm were to go bankrupt it would be immediately liquidated. The new loan is required to continue operating (for example, to pay wages), thus even if the judge mandated reorganisation no funding would emerge to make that possible.</Comments>
        </Document>
        <Document ID="4A61D303-F61C-478F-B670-8429D6A8A57F">
            <Title>Results</Title>
            <Text>[](#p2table6)displays results from estimating [](#p2equation5). The second panel presents effects of the variables from [](#p2table1). As before, youth and manufacturing contribute to steeper financing obstacles. Older, service industry companies complain less. Size and foreign-ownership are newly significant---smaller, domestic firms have the toughest time with credit---but state-ownership no longer is. Export status remains close to zero and insignificant.
The third panel in [](#p2table6) lists marginal effects for the lending environment variables from [](#p2table2). None are remotely significant, save GDP---an extra $1,000 per person reduces perceived financing constraints by eight percent.
[](#p2table7)displays the marginal effects of $Y$, $p$ and $B$. The first column shows each at observation-weighted grand means of the covariates. Only credit information is (weakly) significant---a full unit increase reduces perception that finance is an obstacle by about a third. Implementing receivership likewise cuts financing constraints; large contract enforcement costs increase them. Neither effect is significant.
The next two columns in [](#p2table7) break down $Y$ and $p$ by bankruptcy regime. The latter is signifiant only with receivership: a unit increase corresponds to an almost fivefold decrease in reported financing constraints. Without it, the effect is close to zero and not statistically different from it. The difference is both large and significant---confirming [](#hypoth3).
In countries with receivership, the effect on $Y$ is negative, but erratic---in line with [](#hypoth2)'s prediction. [](#hypoth1) appears likewise confirmed: without receivership, $Y$ is positively correlated with financing constraints---as anticipated---but significant only at relatively extreme values of $p$.
[](#p2figure1)maps this relationship, showing $Y$'s marginal effect (fixed at various percentiles) over $p$. $Y$'s positive correlation with $\text{obstacle}_i$ is persistent, yet significant only when information is abundant. Recall that sparse borrower information makes accurate adjudication of viable firms less likely---possibly mitigating the impact of costly reorganisation. As information increases, however, reorganisations do, too---so their impact may be more acutely felt.
Of course, non-viable firms experience the situation in reverse. Poor information makes reorganisation more probable, so $Y$ should have its clearest impact when $p$ is small. This interpretation would be less significant, however, if viable projects dominate the sample---or at least respondents' frame of mind when asked "Is access to financing [an obstacle] to the current operations of this establishment?".</Text>
            <Comments>\input{$PPATH/p2/tables/tex/p2table6}
\input{$PPATH/p2/tables/tex/p2table7}
\input{$PPATH/p2/figures/tex/p2figure1}
"Accurate" is used here in the legal sense: most laws make weak or no reference to bankruptcy's costs, providing judges with little scope to refer to them in rulings. See [][rescue_culture], [](#p1footnote1) for a discussion.
For example, firm owners may benchmark a lending environment based on their most difficult experience obtaining a loan. In the context of the model, this corresponds to requests to fund long-term, "viable" projects. (I frequently use "firm" and "project" interchangeably; in the context of the WES survey, however, they are conceptually separate things: a firm (or its business owner) may have several projects for which it is seeking funding.) An additional consideration is that "viable" projects may sometimes be converted into "non-viable" projects in order to satisfy [lenders](#cor2). Their business owners may nevertheless continue to perceive constraints according to their original conception of a project (for which it was impossible to obtain credit) as opposed to its actual implementation.</Comments>
        </Document>
        <Document ID="EAA52082-F700-45E5-B5C7-E000FBF4FA4A">
            <Title>Data</Title>
            <Text>The data include every English-language article published with an abstract in *AER*, *ECA*, *JPE* and *QJE* between January 1950 and December 2015 (inclusive). The largest sample is from *Econometrica* which consistently published abstracts with its articles prior to 1950. *JPE* added them in the 1960s and *QJE* in 1980. *AER* came last in 1986. Errata and corrigenda are excluded, as are articles from *Papers &amp; Proceedings* (*P&amp;P*) issues of *AER*, unless otherwise mentioned. [][AppendixArticleCount] displays data coverage by journal and decade.
For textual input, I use abstracts. Abstract readability is strongly positively correlated with the readability of other sections of a paper (see [](#figure3) and [#Hartley2003b,Plaven-Sigray2017;]). Its structure is standardised in a manner optimal for computing readability scores. Many abstracts have also been converted to accurate machine readable text therefore curbing errors in transcription.
For the analysis in [][NBER], I collected draft abstracts from NBER Technical and Working Paper Series. To match published articles with their NBER drafts, I used citation data from RePEc and searched NBER's database directly for unmatched papers authored by NBER family members. I eventually matched 1,988 NBER working papers to 1,986 published articles. (The mapping is not one-for-one because a small number of working papers were eventually published as multiple articles or combined into one.) This represents approximately one-fifth of all manuscripts in the data and a third of all manuscripts published between 1990--2015. Descriptive statistics are shown in [][NBERResults].
The analysis in [][Duration] compiles submit-accept times at *Econometrica* (1970--2015) and *REStud* (1976--2015), a fifth highly respected economics journal. (*AER*, *JPE* and *QJE* do not publish the dates manuscripts were submitted and accepted.) I obtained the data from journals' online archives or extracted it from digitised articles using the open source command utility `pdftotext`. [][Duration] displays and discusses basic summary statistics.
In [][Quantification], I analyse readability at the author-level using both the entire sample and the sample of published articles matched with NBER working papers. To generate a panel dataset following author $i$ over the $t\in\{1,\ldots\,T_i\}$ papers he publishes in a top-four journal, I duplicate each article $N_j$ times, where $N_j$ is the number of co-authors on paper $j$. I then assign observation $j_n$ article $j$'s $n\text{th}\in\{1,\ldots,N_j\}$ co-author. To account for duplicate articles, observations in relevant estimates are weighted by $1/N_j$.
To control for the impact of blinded review, I constructed a dummy variable equal to one if a paper underwent double-blind review before the internet at *AER* and *QJE*, the only two journals with an official double-blind review policy in place at some point during the time period covered by the data. *QJE* employed double-blind procedures until 1 June, 2005; *AER* between 1 July, 1989 and 1 July, 2011. From 1 May 1987 to 31 May 1989, the *AER* conducted a randomised controlled trial whereby half of all submitted papers were evaluated by single-blind review; the remaining half were subjected to double-blind review[#Blank1991]. Since referees correctly identified at least one author in 45.6 percent of double-blind reviewed papers, however, only about a quarter of these manuscripts were truly blind reviewed. I therefore classify every paper published during the trial as having undergone single-blind review.
Other control variables used in the analysis include editor fixed effects, dynamic institution fixed effects, primary and tertiary *JEL* fixed effects, controls for author prominence and seniority, English fluency dummies, citation counts (asinh), and controls for motherhood and childbirth ([][Duration], only). I additionally categorised each tertiary *JEL* code as either theory/methodology, empirical or other in order to roughly account for how theoretical vs. empirical a paper is. See [][AppendixControls] for further information on how each of these variables were calculated.</Text>
            <Comments>Double-blind review was likely less effective after the internet was adopted[for anecdotal evidence, see, *e.g.*,\]\[][#Goldberg2014]. I therefore only evaluate the impact of blind review pre-internet, which I define as having been published before Google incorporated in 1998.</Comments>
        </Document>
        <Document ID="1A7A1607-1167-44C7-ABAB-AC899C53BF8D">
            <Title>Robustness</Title>
            <Text>In order to conclude that the results presented in the previous section suggest women's papers become more readable while under review, I assume that NBER working papers are not generally released before their authors submit them for peer review. As [][AppendixTiming] illustrates, this appears to be the case: most manuscripts---and especially most female-authored manuscripts---are submitted to peer review at the same time or before being released as NBER Working Papers.
Another concern is that gender differences in how authors conform to abstract word limits may bias results in [](#table6_FemRatio). To investigate this possibility, I exclude the 642 observations---about 40 percent of the sample---with NBER abstracts longer than the official word limit of the journals in which they were eventually published. Results are presented in [][AppendixWordLimits]. Coefficient magnitudes are similar to those in [](#table6_FemRatio); standard errors are somewhat larger.
Finally, in an effort to maximise sample sizes, I do not control for field. Although estimates in the second panel arguably implicitly account for field already, I additionally replicate [](#table6_FemRatio) with fixed effects for primary *JEL* categories. Results are shown in [][AppendixnBERfield]. Adding *JEL* fixed effects slightly increases standard errors; they otherwise make little difference.</Text>
            <Comments>Concluding that the gender readability gap is *caused* by peer review requires making the additional assumption that authors do not make post-submission changes to their papers unless requested by referees.</Comments>
        </Document>
        <Document ID="6242A060-EDB5-404F-9160-EACCE6C41F6E">
            <Title>Authors' average readability scores for their first, mean and final papers</Title>
            <Text>[](#tableH1) displays authors' average readability scores for their first, mean and final top-four papers. Grade-level scores (Flesch-Kincaid, Gunning Fog, SMOG and Dale-Chall) have been multiplied by negative one (see [][Readability]). Sample excludes authors with fewer than three publications.
As their careers advance, women do write more clearly: their average readability scores are 1-5 percent higher than the readability of their first papers; their latest papers 1--7 percent. For a man, however, his average and last paper are about as well written as his first.
&lt;!--
\input{$HOME/Dropbox/Readability/draft/0-tex/generated/Table-I.2}
\clearpage
--&gt;</Text>
        </Document>
        <Document ID="7E9CC279-AE5B-4ACD-AF4D-A61459822F40">
            <Title>Untitled</Title>
        </Document>
        <Document ID="9EF0B983-B683-4E55-9C24-45C99343DA21">
            <Title>Field</Title>
            <Text>I create dummy variables corresponding to the 20 primary and over 700 tertiary *JEL* categories to control for subject matter. The *JEL* system was significantly revised in 1990; because exact mapping from one system to another is not possible, I collected these data only for articles published post-reform---about 60 percent of the dataset. Codes were recorded whenever found in the text of an article or on the websites where bibliographic information was scraped. Remaining articles were classified using codes from the American Economic Association's Econlit database.
I additionally categorised each tertiary *JEL* code as either theory/methodology, empirical or other. For example, C02 (mathematical methods) and D85 (network formation and analysis: theory) are classified as theory/methodology, whereas D12 (consumer economics: empirical analysis) and F14 (empirical studies of trade) are classified as empirical. Tertiary codes that are not distinctly related to empirical or theory/methodology---*e.g.*, L29 (firm objectives, organisation and behaviour: general) or O10 (economic development: general)---are classified as "other". (Papers published before 1990 are not classified in any category.) [](#jel_list) lists the *JEL* codes assigned to each category.
In total, I classified 99 tertiary *JEL* codes as theory/methodology, four as empirical and the remaining 756 as "other". (Given the small number of distinctly empirical codes, most "other" papers are likely empirical papers.) When combined with my 1990--2015 dataset, there are 1,764 theory/methodology articles (34 percent), 412 empirical articles (8 percent) and 4,608 "other" articles (88 percent). (Given articles can be both theory and empirical, these percentages do not sum to 100 percent.)
&lt;!--\input{$HOME/Dropbox/Readability/draft/0-tex/generated/Table-C.1}--&gt;</Text>
        </Document>
        <Document ID="4D568880-239D-43FC-BB78-5CC6C24B94A4">
            <Title>216 (erin’s imac's conflicted copy 2015-08-30)</Title>
            <Text>In this section, I test the validity of [](#prop5) and [](#prop6). To account for credit-rationing and control for firm-specific characteristics, I use data from the World Enterprise Surveys (WES). The WES are a series of company surveys conducted by the World Bank and European Bank for Reconstruction and Development. They cover business constraints in emerging and developing markets. The data start in 2005 and are generally repeated every three years; in total, they contain 117,105 observations from 135 countries.
One question asks the degree to which access to finance---including interest rates, fees and collateral---is an obstacle to business operations. Answers range from 0 (no obstacle) to 4 (very severe obstacle). I define $\text{Obstacle}_i$ as a binary variable equal to 1 if the respose was moderate (2) or above---roughly 47 percent of firms.
I control for age, size, sector, exporter and ownership to account for firm-specific characteristics. $\text{Age}_i$ is the difference between the year firm $i$ was established and the year in which the survey took place. $\text{Size}_i$ distinguishes between small (fewer than 20 employees), medium and large (more than 100 employees) companies. $\text{Exporter}_i$ is a binary variable equal to 1 if the firm exports all or part of its output; $\text{Foreign-owned}_i$ and $\text{State-owned}_i$ equal 1 if the firm is partly or wholly foreign- or state-owned, respectively. $\text{Manufacturing}_i$ equals 1 if firm $i$ operates in the manufacturing industry.
[](#FirmSpecifics) summarises firm-specific controls. Exporter status, foreign ownership and size are the same for firms reporting finance as an obstacle compared to those that don't. Although state-owned firms are less likely to find financing an obstacle, the difference is slight. Younger firms and those in manufacturing, however, report a tougher time accessing finance than their older, service-oriented peers. Firms which consider finance an obstacle are, on average, 22 years younger than those that don't; 52 percent of them operate in the manufacturing industry compared to only 44 percent of those which do not find finance to be an obstacle.
In addition to firm-specific controls, I include several controls for a country's general lending environment and development; each are culled from the World Bank WDI database and summarised in [](#LendingEnvironment).Inflation, domestic credit to the private sector and GDP and GNI per capita are originally sourced from national accounts and international financial statistics data compiled or estimated by the OECD and World Bank, International Monetary Fund. The percentage of firms using banks to finance investment is drawn from survey responses in the WES. Data are matched to the WES according to the exact country and closest year in which the survey took place.
To measure $p$ and $Y$, I use World Bank Doing Business data. Doing Business indicators cover the cost and complexity of administrative procedures and regulations imposed on firms. They are estimated based on responses to a set of standardised---yet mostly hypothetical---scenarios posed to local accountants, lawyers and other expert practitioners. 
For $p$ I construct an index of the depth, coverage and quality of financial information available on individuals and firms in a country. It is the product of the Doing Business’s *Depth of credit information index* (an index measuring the comprehensiveness of credit-related data in the economy) and *Credit registry coverage* (percentage of individuals and firms covered by a public data registry) or *Credit bureau coverage* (percentage of individuals and firms covered by a private data bureau), whichever is greater.
The *Depth of credit information index* was substantially revised in 2015 but backward revised only until 2013. The old methodology scored countries based on six criteria; the new methodology appends two more features. To account for this difference, data prior to 2013 is normalised by six; data after that is normalised by eight.
In a given year, roughly 60 percent of countries have the minimum value of 0. Argentina had the highest possible value of 1 in 2006 and 2010; Honduras and Uruguay in 2010. The mean value is 0.15 (standard error of 0.01), about 0.05 points lower than the average of all countries covered in the Doing Business (0.2, standard error 0.01).
$Y$ is the Doing Business *Cost of enforcing contracts* measure. It estimates the cost of resolving a commercial dispute as a percentage of the claim in question. It includes attorney and court fees and all costs associated with enforcing the final judicial decision. Values range between 0 and 1.63 (indicating enforcing a contract costs 63 percent more than the value of the claim). The latter distinction goes to East Timor in 2009; the former to Bhutan. The mean is 0.35 (standard error 0.01)---identical to the population mean (standard error 0.006).
As shown in [](#prop4) from [][Rescue Culture] credit rationing---and presumably finance obstacles more generally---vanish in the presence of receivership procedures similar to those present in the U.K. prior to 2003 and especially 1986. Importantly, receivership procedures must allow the creditor full power to bypass bankruptcy entirely and determine for himself the insolvent firm's proper resolution.
To determine whether this is the case, I again rely on Doing Business indicators. Appendix X details my methodology explicitly, but roughly corresponds to whether floating charge liens are legally permissible, have absolute priority in bankruptcy, are not subject to an automatic stay and may be enforced out-of-court.
Fourteen countries employed a floating charge lien in at least one year; 132 did not. In the 187 countries in the Doing Business for which data are available, 15 used a floating charge. That is, the majority of countries with a well-functioning receivership regime are covered by the WES.</Text>
        </Document>
        <Document ID="C33716FF-C8C3-4471-BD56-6FBA82DADAE6">
            <Title>Scratch: Rescue Culture</Title>
            <Text>But inefficient continuation is not a consequence of bankruptcy's design nor is it singularly tied to limited liability; it is a product of poor implementation. Eliminate judicial mistakes *or* wasteful reorganisation and it goes away. When judges identify non-viable firms without error, all are liquidated in bankruptcy. Reorganisation may be expensive, but non-viable firms have no means to take advantage of that. 

When reorganisation isn't wasteful, on the other hand, workouts don't offer enough of a premium to forgo liquidation.  The gains from a possible liquidation are enough to induce the entrepreneur to prefer bankruptcy to a workout. Of course, bankruptcy's only advantage is liquidation, an outcome on which the creditor is anyway keen ([](#lem1)); non-viable projects are consistently liquidated at time 1.

Nevertheless, insolvency provides entrepreneurs the motivation and expensive reorganisation grants them the opportunity to extract excessive deviations from absolute priority. Thus, although the efficient outcome prevails, it does so only inefficiently.

One type of non-viable firm is liquidated regardless of time 1 earnings: those with liquidation values so high that their entrepreneurs would rather file for bankruptcy than engage in a workout, i.e., $V_1^C&lt;V_1^B$, The gains from a possible liquidation outweigh the costs of a conceivably expensive reorganisation making bankruptcy a worthwhile gamble. Of course, bankruptcy's only advantage is liquidation, an outcome on which the creditor is anyway keen ([](#lem1)); these projects are consistently liquidated at time 1.

Official procedures frequently employ regulations which favour the debtor---from giving him first crack at a reorganisation plan. It is rarely the case, however, that reorganisation explicitly transfers wealth from the creditor to the entrepreneur. Instead, it substitutes one outcome for another. 

Violations to absolute priority compound moral hazard. Creditors are forced to accept workouts precisely when the value of their fallback option---bankruptcy---is low. Threatening bankruptcy increases entrepreneurs' earnings by compelling creditors to forfeit some of theirs. 
But inefficient continuation is not a consequence of bankruptcy's design nor is it singularly tied to limited liability; it is a product of poor implementation. Eliminate judicial mistakes *or* wasteful reorganisation and it goes away. When judges identify non-viable firms without error, all are liquidated in bankruptcy. Reorganisation may be expensive, but non-viable firms have no means to take advantage of that. 

When reorganisation isn't wasteful, on the other hand, workouts don't offer enough of a premium to forgo liquidation.  The gains from a possible liquidation are enough to induce the entrepreneur to prefer bankruptcy to a workout. Of course, bankruptcy's only advantage is liquidation, an outcome on which the creditor is anyway keen ([](#lem1)); non-viable projects are consistently liquidated at time 1.

Nevertheless, insolvency provides entrepreneurs the motivation and expensive reorganisation grants them the opportunity to extract excessive deviations from absolute priority. Thus, although the efficient outcome prevails, it does so only inefficiently.

One type of non-viable firm is liquidated regardless of time 1 earnings: those with liquidation values so high that their entrepreneurs would rather file for bankruptcy than engage in a workout, i.e., $V_1^C&lt;V_1^B$, The gains from a possible liquidation outweigh the costs of a conceivably expensive reorganisation making bankruptcy a worthwhile gamble. Of course, bankruptcy's only advantage is liquidation, an outcome on which the creditor is anyway keen ([](#lem1)); these projects are consistently liquidated at time 1.

Official procedures frequently employ regulations which favour the debtor---from giving him first crack at a reorganisation plan. It is rarely the case, however, that reorganisation explicitly transfers wealth from the creditor to the entrepreneur. Instead, it substitutes one outcome for another. 

Violations to absolute priority compound moral hazard. Creditors are forced to accept workouts precisely when the value of their fallback option---bankruptcy---is low. Threatening bankruptcy increases entrepreneurs' earnings by compelling creditors to forfeit some of theirs. 
Finally, to weed out countries prone to excessive appeals or wasteful delays, I further require receivership winds up within two years. 
This paper's goal is to test the validity of [](#prop5) and [](#cor3). The problem and data are conducive to structural estimation via simulated method of moments. For the purposes of this paper, however, I employ a reduced form approach; my aim is simply to determine the average, aggregate sign of $p$ and $Y$ given data on firms in varying bankruptcy regimes.
Yet more nuance is needed to support [](#prop6), which predicts an ambiguous effect in countries without receivership. Security prices in efficient capital markets, stringent disclosure laws and well-developed credit-rating agencies and financial analysis sectors better reveal firm fundamentals. Before loans are made, this added information reduces borrower-lender asymmetry. During bankruptcy, it increases judicial precision. As long as judges systematically underestimate reorganisation's cost, more accurate decisions may mean fewer insolvent firms are liquidated---and so fewer firms get credit in the first place.
</Text>
        </Document>
        <Document ID="8D97AAB2-10CC-429C-9080-177F0B18C0FD">
            <Title>[][NBER], suplemental output</Title>
        </Document>
        <Document ID="D7529450-4313-4DD1-8792-5AD7C387DB51">
            <Title>History, statistical validity and use in research</Title>
        </Document>
        <Document ID="0477AD99-CF5A-40D7-86C0-1AB71F370AFA">
            <Title>Blind review event study</Title>
            <Text>In this section, I show a crude event study illustrating the impact of blind review on the gender readability gap formed during peer review. To implement it, I re-estimate [](#equation3) without controlling for blind review, but otherwise accounting for the same factors in [](#table6_FemRatio). [](#figure-blind-es) plots residuals from this regression for papers published in the *AER* and *QJE* 8--9 years,  6--7 years, *etc.* before they switched to or from single-blind review (or the advent of the internet) and 2--3 years, 4--5 years, *etc.* afterwards.
&lt;!--\input{$HOME/Dropbox/Readability/draft/0-tex/fixed/Figure-G.1}--&gt;
Most graphs in [](#figure-blind-es) suggest a discontinuity in women's unexplained changes in readability at the introduction of single-blind review (or the internet). For men, however, unexplained changes to readability appear largely unaffected by double-blind review, conditional on included controls. [](#figure-blind-es) therefore tentatively suggests that women may benefit (on average) from a policy of double-blind review, while men are less affected by it.
&lt;!--\clearpage--&gt;</Text>
        </Document>
        <Document ID="3F925459-8380-4EC0-991B-304A205B43D8">
            <Title>Scratch: Publishing Female</Title>
            <Text>$i$ satisfies the assumptions and conditions of [](#Theorem1) relative to $k$, but that both were suddenly subject to the same standards. How would $i$'s readability choice change? Suppose first $R_{it''}\le R_{kt}$ where $R_{it''}&lt;R_{it}$ was the readability score of one of $i$'s earlier papers (Condition 2). Since it is never optimal for $i$ to set $R_{it}$ less than $R_i^\star$, Condition 2 implies $R_i^\star&lt;R_{kt}$. Thus, holding acceptance rates constant, $i$ prefers $R_{kt}$ to any readability score greater than $R_{kt}$.
We therefore know that $i$ does not set $R_{it}$ above $R_{kt}$. Now suppose $R_{kt}\le R_{it''}$. so it might be the case that $R_{kt}&lt;R_i^\star\le R_{it''}$
and $k$ were subject to identical standards. What readability score would $i$ optimally chose as an experienced author? 
The rough intuition behind [](#eq:correq1) is simple. First, recall that $R_{it}=\widetilde R_i^{\bar s}+e_{it}$. Second, note that since every review group that accepts $i$'s paper also accepts $k$'s papers, that must mean that the toughest review group to accept $i$'s papers---*i.e.*, $\bar s$---also accepts $k$'s papers. Finally, and without loss of generality, suppose $\bar s$ is also the toughest review group to accept $k$'s paper and that $R_{kt}=\widetilde R_k^{\bar s}+e_{kt}$. Thus $R_{it}-R_{kt}&gt;0$ measures the difference between the readability thresholds applied by group $\bar s$ to $i$ and $k$:
&lt;!--\begin{align*}
R_{it}-R_{kt}&amp;=\widetilde R_i^{\bar s}+e_{it}-\widetilde R_k^{\bar s}-e_{kt}\nonumber\\
&amp;=\widetilde R_i^{\bar s}-\widetilde R_k^{\bar s}.
\end{align}--&gt;
[](#eq:correq1) relaxes the requirement that $\Sigma_{A_{it}}\subset\Sigma_{A_{kt}}$. For example, suppose there exists a particularly tough review group $s'$ that applies an identical readability threshold to $i$ and $k$'s papers and, for whatever reason---*e.g.*, $u_k&lt;u_i$---$i$ is willing to work extra hard to appease its demands whereas $k$ isn't. As a result, $s'$ accepts $i$'s papers but rejects $k$'s papers. From [](#Theorem1)'s third condition, however, $i$ and $k$'s average acceptance rates are equal. Thus, there must also exist at least one other review group, call it $s''$ that accepts $k$'s papers but rejects $i$'s papers. In this case, if $R_{kt}&lt;R_i^\star$, then $R_{it}-R_{kt}$ may over-estimate the impact of tougher standards on his readability choices. Because $R_i^\star\le R_{it''}$, however, 
then $i$ will likely work harder to appease the demands of a particularly tough group of reviewers. That is, there is at least one review group that accepts $i$'s paper but rejects $k$'s paper. Nevertheless, from [](#Theorem1)'s third condition, we know that $i$ and $k$'s average acceptance rates are equal. Thus, there must also exist at least one other review group that accepts $k$'s but rejects $k$'s papers simply because $k
Since every review group that accepts $i$'s paper also accepts $k$'s papers, that must mean that the toughest review group to accept $i$'s papers also accepts $k$'s papers. Thus, $\widetilde R_k^{\bar s}\le\widetilde R_
If every review group that accepts $i$'s paper also accepts $k$'s paper, then that must mean that if $\bar s$ accepts $i$'s paper, it also accepts $k$'s paper.
the toughest review group to accept $i$'s paper Thus, the toughest review group to accept $i$'s paper also accepts $k$'s paper. Sin

Additionally, [](#Theorem1) assumes that the more experience a woman gains in peer review, the fewer mistakes she makes about referees' and editors' standards. (Or at least, at some point this becomes true.)
 the evidence presented in [][Duration] does not support this hypothesis, the test it relies on is very indirect. Moreover, 
that $i$ chooses $R_{it}$ to maximise [](#eq:EU). Second, and more importantly, it assumes that $i$ and $k$ write papers write papers that are identical with respect to topic, novelty and quality but potentially differ on readability. Combined, these two assumptions imply $i$ and $k$ do not specialise in different dimensions of quality such that overall quality remains the  with respect to one another. That is, if $i$'s papers are more readable than $k$'s papers, then 
This admits only a very simplified representation of utility which does not, for example, allow authors to trade-off readability for some other aspect of q
The three assumptions are: [](#eq:EU) defines $i$'s optimal choice of readability (Assumption 1); $i$ and $k$ write papers that are identical with respect to topic, novelty and quality but potentially differ on readability (Assumption 2); and the information $i$ gains during peer review and by observing others' choices and outcomes is sufficient to ensure that mistaken beliefs converge to zero as $t$ tends to infinity---that is, $i$'s beliefs become more accurate as he gains experience in peer review (Assumption 3).
A limitation of the model is that it only admits a very simplified representation of utility and does not, for example, allow authors to trade off readability for some other aspect of quality such that otherwise identical men and women nevertheless choose to specialise in different aspects. As we showed in Section X, however, it is unlikely that male economists compensate for their lower quality writing by raising quality along another dimension. Nevertheless, this form of specialisation may mitigate the bottom line impact of any gender readability gap observed.
I emphasis again that the model fully incorporates the idea that authors can and do make mistakes in their beliefs. It assumes, however, that mistakes result from imperfect information (*e.g.*, risk aversion) and/or information asymmetry, in that if we lived in a world with perfect information and zero information asymmetry between the sexes, such "mistakes" would no longer occur. Thus, since "experience" serves as a way to complete information and make it less asymmetric between the sexes, if the readability gap between genders is being caused by these mistaken believes, then the gap should decline with experience. On the other hand, if we observe the opposite trend---*i.e.*, that women *improve* their readability as the uncertainty and the information gap between men and women declines---then we should be hesitant to conclude that gender differences in mistakes (or other internal factors such as intrinsic preferences), even though they very well may be present, especially initially, are in fact driving this particular gap.
where $q$ maps the strategies $x_m$ and $x_f$ onto the quality space $\mathcal Q$ and $q^{-1}_m$ and $q^{-1}_f$ map them back again is its inverse.

 review. This implies that men and women accrue identical rewards, conditional on $Q$, *i.e.*,

The male researcher is rewarded for his effort with the acceptance rate $a_m(x_m,Q)$; the female researcher is rewarded for her effort with the acceptance rate $a_f(x_f,X)$. The cost to each gender of implementing their strategies is $c_m(x_m)$ and $c_f(x_f)$, respectively.


The latter implies that the reward-cost tradeoff of $x_m$ is identical to the reward-cost tradeoff of $x_f$---since if it weren't, one side would have to exert more effort to achieve $X$ 

 $a_m(x_m,X)=a_f(x_f,X)$. 

If men and women are equally capable researchers, then the reward-cost tradeoff to the male researcher of his strategy must be equal to the reward-cost tradeoff of the female researcher to her strategy, *i.e.*,
&lt;!--\begin{equation}\label{cost-reward}
	\frac{a_m(x_m,X)}{c_m(x_m)}=\frac{a_f(x_f,X)}{c_f(x_f)},
\end{equation}--&gt;
 For if it weren't

the reward-cost tradeoff to the male researcher of his strategy $x_m$ will be equal to the reward-cost tradeoff to the female researcher of her strategy, $x_f$, conditional on both achieving the same quality $X$, *i.e.*,

 since if it weren't, that would imply that one side has to exert more effort for a given level of $X$.
the women's strategy would be costlier to implement even though it generates the same quality as the man's strategy. In other words, she has to exert more effort to produce work that is the same quality as the man's ($c_m(x_m)&lt;c_f(x_f)$)---*i.e.*, she is a less capable researcher---or both.

If men and women are held to identical standards, then 

  [](#cost-reward) implies that  *and* $c_m(x_m)=c_f(x_f)$.
either that the publication strategy women employ generates a lower reward than the publication strategy men employ even though both strategies result in the exact same $X$ ($a_m(x_m,X)&gt;a_f(x_f,X)$) or that the women's strategy is costlier to implement even though it generates the same quality as the man's strategy. In other words, she has to exert more effort to produce work that is the same quality as the man's ($c_m(x_m)&lt;c_f(x_f)$)---*i.e.*, she is a less capable researcher---or both.


In an earlier version of this paper[#Hengel2015], I compared readability gaps in published articles subjected to blind review---*i.e.*, double-blind review before the internet---and non-blind review---*i.e.*, single-blind review or double-blind review after the internet. Blind review appeared to exacerbate the gender readability gap.
These findings, however, were not robust to including fixed effects for year of publication. [](#table7_all) repeats the analysis from [\]\[Table 3.9 (first panel), p. 65][#Hengel2015;] including them. Figures represent the marginal effect of female ratio in non-blind ($\beta_{1P}$) and blind ($\beta_{1P}+\beta_{3P}$) review from OLS estimation of [](#equationF1):
&lt;!--\input{$HOME/Dropbox/Readability/draft/equations/equationF1}--&gt;
The gender readability gap is positive and significant in non-blind review; when papers are blindly evaluated, however, the gap is generally smaller and never significant. Difference-in-difference estimates ([](#table7_all), third row) are mostly positive, indicating---in contrast to results in[#Hengel2015;] but consistent with results in [](#table7)---that the readability gap was smaller under blinded peer review. Nevertheless, standard errors are large relative to the size of the effect; please interpret these results with caution.
&lt;!--\input{$HOME/Dropbox/Readability/draft/tex/generated/table7_all}--&gt;


 Assuming preferences are fixed over time, authors improve readability only when they believe more readable papers are more often accepted. And while oversensitivity and/or poor information may distort their beliefs---and in turn affect readability---with experience, they correct those mistakes. Thus, when an experienced author writes more clearly than her inexperienced self, we can conclude that she did so because it really *does* improve her acceptance rates (and not because she was too sensitive or poorly informed). If that same experienced author writes more clearly than an equivalent, experienced male author, *yet* her papers are no more likely to be accepted his, *then* asymmetric editorial standards and/or biased referee assignment---*i.e.*, discrimination---explains the difference.
The model establishes three conditions sufficient to demonstrate higher standards are present in academic review: (i) experienced women write better than equivalent men; (ii) women improve their writing over time; and (iii) female-authored papers are accepted no more often than equivalent male-authored papers. I empirically test (i) and (ii) among well-published economists. 
As a final exercise, I also document evidence that higher standards increase the cost to women of producing a paper and likely lower their observed productivity. The data also suggest that women respond to higher standards by *conforming* to those standards in ways that may make it difficult to distinguish biased treatment from voluntary choice.

Moreover, higher standards may manifest themselves in more indirect ways. In particular, I find suggestive evidence that women increasingly submit better written papers *ex ante* to offset biased evaluation *ex post*. Thus, the readability gap between senior economists largely forms prior to---therefore appearing independent of---peer review. This pattern of behaviour suggests women internalise tougher standards with strategies that, perversely, may end up disguising them as voluntary choice and may therefore contribute to a host of labour market phenomena: sectoral and occupational concentration, a tendency to under-negotiate pay and greater reluctancy to compete against men.
If $i$ and $k$ are appropriately matched at $t_4=3$, then $\mu_i=\mu_k$ and $\delta_{i3}=\delta_{k3}$. 
As long as $t_4$ is sufficiently large, then the only factors systematically influencing authors' readability choices will those which they correctly anticipate increase their the desire to publish well

, so $\delta_{i3}=\delta_{k3}=0$---then 
 $\delta_{m3}=0$, $m=i,k$---*i.e.*, the only factor influencing experienced authors' readability is the desire to publish well; if $\delta_{i1}\ge0$, then 
 if $t_4$ is appropriately large, then  long individual authors' change in readability is not systematically motivated by anything other than a desire to publish well, $\mathbb{E}[\delta_{i3}-\delta_{i1}]=0$. 
As long as individual authors' papers are roughly equivalent for all $t_4$, well-matched pairs account for remaining differences between $i$ and $k$. They *don't* account for subtle variations over $t_4$ conditional on $i$. Omitting factors exogenous to an author's long-term decision-making process---*e.g.*, year of publication or referee stereotypes about authors' institutions---potentially biases any estimate of $D_{ik}$. But *including* $t_4$-varying factors under $i$'s control---*e.g.*, journal, field and co-author characteristics---could too: the journals one submits to, the fields one gravitates toward and the people with whom one chooses to co-author are all endogenously determined by an author's experience. Higher writing standards for female authors may be met by manipulating any of these variables.
In spite of this, I don't actually find any evidence that observable $t_4$-varying factors---exogenous or not---drive women's increasing readability. For that reason, I opt for a parsimonious [](#equation14). The robustness and validity of this approach are addressed in a following section.
tests the previous section's finding that junior women undergo the toughest peer review. Blue dots are coefficients on female ratio from FGLS estimation of [](#equation16) using data from both journals---and therefore omitting family commitment and institution controls---on sub-samples 
the yellow dot is their difference.
Unfortunately, I cannot test this hypothesis with the data I have collected. In the next section, however, I investigate the former claim, *i.e.*, that accepted manuscripts by junior women undergo the toughest---hence longest---review.





I can only attribute women's behaviour to higher standards if the submission and revision decisions of experienced economists of both sexes adhere to [#Savage1954;]'s axioms of rationality. If they are violated for women---but not men---my results suggest female economists who are very experienced in the peer review process nevertheless engage in actions which limit their career advancement relative to men's (see [][Discussion] for a discussion). As a result, policies which target external (as opposed to internal) factors may still be more effective at increasing the representation of women in top journals [for a discussion in a related context, see\]\[][#Born2019].

 higher standards raise quality at the expense of quantity; performance indicators that weight the latter's fall more heavily than the former's rise will appear artificially low. More
A similar argument was recently made in a study of racial preferences in Major League Baseball[#Parsons2011].
joins newly emerging empirical evidence in focusing on discrimination's impact on the behaviour and choices of people discriminated against. 
higher standards may bias productivity measures 
Instead of suggesting that employers' tastes and beliefs *actually* reduce worker productivity, we suggest instead that th

It is impossible to precisely pin down what might be present in male-authored papers that could perfectly compensate for their lower readability, conditional on quality, so I cannot entirely rule this contingency out. 

These results suggest better writing in female-authored papers probably isn't (i) a response to specific policies in earlier eras; (ii) caused by women writing on topics that are easier to explain; (iii) due to a lopsided concentration of (non-)native English speakers; nor (iv) generated by factors correlated with gender but really related to knowledge, intelligence and creativity.

the *American Economic Review* (*AER*), *Econometrica* (*ECA*), *Journal of Political Economy* (*JPE*) and *Quarterly Journal of Economics* (*QJE*). Because readability might substitute for other aspects of quality, I additionally control for citation counts and various proxies for author productivity.

To test my hypothesis, I analyse the readability of the abstracts of 9,122 articles published 

Although the adage "clear writing is clear thinking" suggests the quality of a written paper is intrinsically tied to how clearly it is written, I additionally control for various measures of article and author quality in order to account for other aspects of quality not picked up by readability.

I use five of the most tested and reliable "readability" formulas for adult reading material: the Flesch Reading Ease, Flesch-Kincaid, Gunning Fog, SMOG and Dale-Chall scores. As textual input, I use abstracts from 9,122 articles published in the *American Economic Review* (*AER*), *Econometrica* (*ECA*), *Journal of Political Economy* (*JPE*) and *Quarterly Journal of Economics* (*QJE*).

a relationship familiar to linguists and educators: simple vocabulary and short sentences are easier to understand. Using the five most widely used, tested and reliable "readability" formulas for adult reading material: the Flesch Reading Ease, Flesch-Kincaid, Gunning Fog, SMOG and Dale-Chall scores. Since a paper's abstract readability is highly correlated with the readability of its other sections (see [][MeasuringReadability] and [](#figure3)), I use abstracts as the textual input.
Although the adage "clear writing is clear thinking" suggests writing well quality are closely related, I also control for citation counts in order to account for other aspects of quality not picked up by readability. Additional controls were obtained by consulting journals' mastheads, authors' CVs and other publicly available information. The total sample includes extensive bibliographic and biographical data on 9,122 academic articles and their 6,875 unique authors published between 1950--2015 in a top-four economics journal---*i.e.*, the *American Economic Review* (*AER*), *Econometrica* (*ECA*), *Journal of Political Economy* (*JPE*) and *Quarterly Journal of Economics* (*QJE*).

In this section, I show that peer review causes the gender readability gap. To do this, I match published articles---which have gone through peer review---to earlier, draft versions of the same papers---which have not. I then compare the change in the gender readability gap between versions.
[][NBERSummary] describes the data used in the analysis and presents basic summary statistics. The identification strategy is discussed in [][NBERIdentification]. Results are shown in [][NBERResults] and are followed by a discussion of their [robustness][NBERRobustness].
There are three steps to my empirical strategy. First, I establish a gap does exist and rule out obvious confounding factors---*e.g.*, women writing on easier topics, editorial policies in earlier eras, *etc.* ([][ArticleLevel]).
Second, I causally link a portion of the gap to the peer review [process][NBER]. To do this, I match published articles---which have gone through peer review---to earlier, draft versions of the same papers---which have not. I then compare the change in the gender readability gap between versions. My results suggest that the peer review process causes about 40 percent of the gap in readability between men and women.
The final step of my empirical strategy suggests the gender readability gap is caused by referees and/or editors applying higher writing standards to female-authored [work][Experience]. There are two write more readably in peer review due to factors that are either (i) within their control---*e.g.*, sensitivity to criticism---or (ii) outside their control---*i.e.*, gender bias by referees and/or editors. To distinguish between (i) and (ii), I develop a dynamic model of an author's decision-making [process][SEUModel]. Based on the model, I find (ii) is predominantly behind women's readability choices. I estimate that it causes senior female economists to write at least seven percent more clearly than they otherwise [would][SEUMatching].
I conclude [][Results] by documenting suggestive evidence that discrimination affects behaviour and lowers productivity. In [][IndirectEffect], I investigate how women react as they update beliefs about referees' expectations. in [][Duration], I test one observable repercussion from subjecting female authors to higher standards---prolonged peer review.
[#Seagraves2013;] find that normal houses (*i.e.* homes not sold under special sales conditions, such as foreclosures, fixer-uppers, corporate-owned properties, transfers and estate sales) sell at a significantly higher price when listed by a female real estate agent. The authors also find buyers pay less if they are represented by a male agent---although the effect is only present for homes sold under special sales conditions. 

 If the male member satisfied both Conditions (1) and (2), I interpret that as evidence of discrimination against men; if the female member satisfied both Conditions (1) and (2), I interpret it as evidence of discrimination against women. All other outcomes are assumed to imply no discrimination.

technically requires comparing a single author to two different versions of himself---before and after gaining
before and after gaining experience in peer review---*i.e.*, Condition (2) is measured using a within-person comparison that's meant to hold intrinsic preferences fixed; and as a hypothetical version of himself if he were a member of the opposite sex 
as the opposite sex after gaining experience in peer review---*i.e.*, Condition (1) is measured with a between-person comparison, holding experience fixed. To account for (i), I restrict the sample to authors with three or more top-four publications. I account for (ii) by matching these experienced authors to members of the opposite sex on characteristics that predict the topic, novelty and quality of their research after they've gained that experience.
The resulting sample includes two different observations
121 matched paris and a single observat
In the present paper, I do not explore the particular relationship between review time and readability. Superficial correlations between the two are almost certainly confounded by unobserved heterogeneity in submission quality. For example, if "referees feel the need to demonstrate their intelligence or industriousness to editors by identifying problems in [otherwise perfectly fine] papers", they may "inflate minor blemishes"---*e.g.*, readability---"to the status of major flaws"[\]\[p. 231][#Berk2017]. It is undoubtedly easier to remedy most issues surrounding readability than it is to fix actual, major flaws. Thus, the unbiased relationship between readability and review time is impossible to obtain without controlling for submission quality or placing significant structure on the data generating process. This is beyond the scope of the present paper.

Although these findings strongly suggest editorial/refereeing bias play a role, they cannot definitively rule out that women simply write better papers---*e.g.*, because they're more sensitive to referee criticism. 
Regardless of the explanation, women spend too much time rewriting old papers and not enough time writing new papers, compared to men. The appropriate policy response, however, depends on the cause. 
Moreover, the lack of a gender gap under double-blind review (as opposed to single-bline
. So any changes in the gender gap in readability before and after the internet should be evidence for the role of referees
Note that identification crucially depends on accounting for all non-readability quality-related characteristics of a paper, *e.g.*, novelty, importance, methodological rigour, *etc.* This is primarily achieve by matching authors on citations. See [][SEUMatching] for more details.

. I assume intrinsic preference parameters are fixed over time but the cost of writing can decline with experience and are free to form misspecified beliefs about 
form (possibly misspecified) beliefs about these thresholds. Based on those beliefs, they

Conditional on non-readability aspects of an author's paper---*e.g.*, topic, novelty and underlying quality---
thresholds of readability for each individual and reject those papers below which they reject papers 
 
allows for one's cost of writing to change with experience and also 
 In order to capture possibly misspecified beliefs, I assume authors choose readability in order to maximise their subjective expected utility. 
A smaller gender difference in review times for senior female authors is likely caused by feedback effects *as well as other factors*. In particular, referees are probably better informed about their quality than they are about junior female authors; this undoubtedly reduces review times as well.

The basic logic, moreover, applies equally well to any situation where people are repeatedly judged on and respond to feedback about some quantifiable component of their output.
Assuming men and women are equally capable researchers, this means women have adopted a strictly dominated publication strategy.
But if men and women were equally capable economists, then why would women adopt this dominated strategy?
Because women *don’t* adopt men's strategy despite its better outcomes suggests the cost of employing it relative to the one they do employ is much higher for them than it would be for men. Meaning women are worse economists than men are.
Women would only opt for this costlier strategy because the one employed by men and achieves similar (or better) outcomes with less work and is, in fact, much costlier for women to employ than it is for men. Implying of course that women are worse economists than men.

The only reason women would choose to employ a costlier strategy that results in worse outcomes is if a less costly option with similar or better outcomes were not available to them---*i.e.*, only if they are worse economists than men are.

The publication strategy men have chosen to employ---whatever it is---results in similar (or higher) acceptance rates and shorter reviews.
Nevertheless, because they have no relative advantage in acceptance rates and longer peer review times this must indirectly imply that women are worse economists than men. The issue is that the writing/math strategy employed by men results in similar (or higher) acceptance rates and shorter review times. Thus, the fact that women don’t adopt this strategy implies the cost of employing it relative to the one they do employ is much higher for women than it is for men. The only reason women would choose to employ a costlier strategy that results in worse outcomes is if a less costly option with similar or better outcomes were not available to them---*i.e.*, only if they are worse economists than men are.
In order to verify that my conclusions weren't specific to one particular journal, I also collected review time data from *Restud*, another highly regarded economics journal that also makes submit-accept times publicly available for every article published after 1975.

Although none of the other journals in my dataset print submit-accept times when they publish an article, another highly regarded economics journal does---the *Review of Economic Studies*. To verify that the same trends were present in that data as we see in *Econometrica*, I collected back to 1976---when the journal first began printing this information---and up to 2015. 
Because the analysis in [][SEUMatching] relies on following women's readability choices over their lifetimes, it is difficult to replicate on a subset of articles satisfying certain criteria. (For that reason, I reproduce [](#table8), instead.) See [](#Footnote103) for further discussion and results from a rough attempt to do so.

Comparing lifetime publication counts between equivalent authors accounts for most confounding factors except individual productivity---especially factors related to household responsibilities. Greater responsibility at home presumably does not affect readability (other than, perhaps, to push women's scores downward), but it may impact the number of papers women can write. As shown in [][Duration], however, motherhood responsibilities after childbirth *do not*, in fact, slow women down during the revision process---at least at *Econometrica*.

Female-authored papers are more likely to improve in writing during the peer review process---but there are two competing hypotheses about why this is so: either women *chose* to write more clearly or editors and referees *demand* they write more clearly. In order to distinguish between them, I adopt a modified outcome test.
This was a major theme in earlier theoretical work. [#Lundberg1983,Lundberg1991;] show that statistical discrimination can lead to suboptimal human capital investment decisions when firms more reliably assess the productivity of members in one group relative to another. POLLUTION THEORY
able to asses the productivity of members of one group less reliable than another, members of one or both groups make suboptimal human capital investment decisions.
 on workers' ability combined with endogenous human capital decisions resulted in suboptimal human capital investment in groups  
Among the earliest work in economics to take this into account 
Earlier theoretical work has proposed a dependence between human capital investment decisions and labour market discrimination. [#Lundberg1983;] modelled this dependence explicitly.
In economics, [#Lundberg1983;] were the first to propose a dependence between human capital investment decisions and labour market discrimination. They develop a model of statistical discrimination that model this dependence explicitly. 
Empirical work is more nascent. 
Most studies that use outcome tests measure racial discrimination assume zero inter-group differences in the counterfactual equilibrium. Because it is more difficult to argue that 
They have been rarely employed to illustrate gender discrimination
For that reason, they are generally more successfully implemented to measure racial discrimination
they are rarely employed to illustrate gender discrimination---is easier to argue that gender differences that do arise are plausibly attributed to individuals' intrinsic preferences, knowledge about the process of allocation or personality traits---such as oversensitivity and risk aversion---present when information is imperfect.
When authors make repeated choices over time in a situation in which they are also *learning* about what the optimal decision should be, the trajectory of their choices reveals information about preferences and initial information, and therefore can be used to differentiate between these two competing explanations.
I show that the trajectory of choices an author makes over time reveals information about his preferences, information and sensitivity
 In order to rule out poor information, oversensitivity or even an individual preference for writing well, I show that if any of those factors *were* driving readability, the difference in men's and women's readability should fall (or remain constant) as women gain experience in peer review. 
I model an author's optimal choice of readability  
Within a subjective expected utility framework, I show that authors' choices also reveal discrimination by editors and/or referees. In order to rule out individual preference potentially driving results, the model exploits 
The model exploits author learning to rule out individual preferences driving 

 reflect a benchmark on her intrinsic preference for writing more readable papers. Assuming authors update beliefs about tany factor which distorts their beliefs and leads to suboptimal readability decisions---*i.e.*, decisions that take more work but don't improve acceptance rates---goes away over time. Thus, 

offer a benchmark with which to distinguish between 
authors improve their readability only if the *believe* doing so really improves their acceptance rate. And although many factors such as bad information and oversensitivity can distorts beliefs, these are ephemeral mistakes that authors' correct over time. Once the learning process is complete, if an author's readability is higher than it was before, then the change actually did improve his acceptance rate. This allows me to reconstruct the equilibrium outcome had discrimination not occured.
authors *know* whether higher readability leads to higher acceptance rates. So if an authors' readability is higher *with* experience than it was *without* experience, then the author's reada
over time, mistakes are corrected so that  
This allows me to reconstruct an equilibrium outcome in the absence of discrimination that takes into account both intial preferences
would have looked like in the absence of discrimination.
Unfortuntately, it's difficult to infer discrimiantion in many situations because it's very hard to determine what the equilibrium allocation would have been had discrimination not occured. 
apply them to may outcomes because pl
Using a subjective expected utility model, I show that an "outcome test"
A limitation of outcome tests is that it's hard to distinguish between two possible explanations for 
if we observe authors' make repeated choices over time allows me to determine whether discrimination 

differentiate between two competing hypotheses to explain the gender readability gap: 

 If editors and referees hold women to higher standards, women's writing will be better. But women's writing might also be better if they just prefer writing more clearly, are oversensitive or have poor information. 
 while they are *learning* about the underlying process. This allows me to differentiate between these two competing explanations and identify the impact of discrimination on women's writing.

[#Lundberg1983;] define human capital investments as not just formal schooling, but also "more subtle types of personal deprivation and deferment of gratification which lead to the habits of action and thought that favour good performance in skilled jobs"[p. 27\]\[][#Arrow1973]. This includes non-obvious factors such as investment in networking skills or the connections themselves. Arguably, it might also encompass any choice one might make that impacts productivity---*e.g.*, when and whether to have children.

[#Knowles2001;] endogenises the behaviour and choices of people discriminated against in a model of racial profiling in police stops. However, they assume that drivers have perfect knowledge of the likelihood of being searched and adjust instantaneously to this fact. Here, I assume instead that people being discriminated against *do not* have perfect knowledge of the extent to which they are discriminated against, but instead *attain* perfect knowledge over time through learning. I then use this learning process to identify discrimination. To the best of my knowledge, this paper is the first to explicitly model the *learning process* of people being discriminated against and use that to identify discrimination. (So others have endogenised the impact of discrimination on behaviour; and used the learning behaviour of those who discriminate; I'm the first to use the impact of discrimination on the behaviour of people being discriminated against to identify discrimination.)

To the best of my knowledge, it is the first to exploit the learning process---and, especially, behaviourial changes---of people being discriminated against to identify the effect of that discrimination.
Predictably, giving birth slows down peer review. The coefficient on motherhood, however, is consistently negative (indicating a productivity boost) and almost always highly significant when subjected to several robustness checks. This result is provocative and discussed in [][Duration]. I encourage interpreting it with caution, however, given (i) counterintuitive results; (ii) the analysis did not intend to measure motherhood’s impact on review times; and especially (iii) only a small number of mothers with young children are published in *Econometrica*.
I also match prolific female authors to similarly productive male authors on characteristics that predict the topic, novelty and quality of research. In addition to explicitly accounting for author equivalence---the (principle) conditional independence assumption behind [](#Theorem1)---matched pair comparisons identify the gender most likely to satisfy [](#Theorem1)'s conditions *simultaneously*; consequently, they can be used to generate a (conservative) causal estimate of the effect of higher standards on authors' [readability](#Corollary1).
we are only 10.6 percent of authors published in top economics journals since 1990. Female economists are less likely to make tenure, take longer when they do and earn much less than their male peers[#Bandiera2016,Ceci2014,Ginther2004,Weisshaar2017].
Results confirm conclusions drawn from [](#figure4) and the first two panels of [](#table10): discrimination by editors and/or referees predominately affects female authors. 
Discrimination by editors and/or referees predominately affects female authors. Mean $\underline D_{ik}$ is positive and significant in both columns for all five scores. 
Moreover, restricting the analysis to solo-authored papers or those co-authored by members of the same sex consistently show that women's readability increases with $t$ (see [](#table8XC) in [][AppendixExclusive]). This suggests that co-author characteristics---whether exogenous or not---are not behind women's increasing readability.
More specifically, I also find fairly solid evidence that co-author characteristics---the vaguest $t$-varying factor---is not motivating women's increasing readability. [](#table8XC) in [][AppendixExclusive] reproduces [](#table8) on the sample of papers exclusively authored by women. Moreover, the most obvious way that co-author characteristics *would* be driving women's increasing readability is if women are more likely to co-author with other women as they gain experience. As I showed in the 2016 version of this papers, however, the reverse is true: as $t$ increases, women are more likely to co-author with men; men are more likely to co-author with women[\]\[Table 12, p. 25][#Hengel2016].
For added robustness, I also control for changes in *JEL* classification [codes][AppendixSEUJEL] and restrict the sample to solo-authored papers: [][AppendixExclusive], [](#table8XC) reproduces [](#table8) on the sample of papers exclusively authored by women. Again, all figure agree: women write better and the magnitude and significance of that difference increases as $t$ increases.

[][MatchingEstimation], [](#equation15) does not include any other $t$-varying factor other than female-ratio. As pointed out earlier, *including* factors under $i$'s control and *excluding* factors outside $i$'s control leads to biased results. However, 
&lt;!--
\iffalse
In the next section, discuss why t-varying factors could distort the selection of pairs that satisfy Condition 2 conditional on pairs that aready satisfy Condition 1.

Perhaps women would rather write more clearly at t=1, but co-author characteristics are holding them back. So, for example, they co-authored their first publications with men, so that's why they were more poorly written. Once they got more experience, they wrote on their own and that's when they wrote more clearly. Yet, women are more likely to solo-author or author with other women at t=1 than they are at t=3. Women are more likely to solo-author---or co-author with women---at t=1 than they are at t=3.

\fi
--&gt;

First, in an earlier version of this paper, I controlled for these factors (as well as the endogenous factors described above) but 
each person in the matched sample would, ideally, solo-author his first and third publication in the same year and publish it in the same journals. Then $R_{i3}-R_{i1}$ would capture the impact $i$'s experience has on readability, conditional on gender; $R_{i3}-R_{k3}$ would reflect the impact of gender, conditional on experience. Unfortunately, the sample is too small to restrict it to just those observations that satisfy these criteria. In order to perform the analysis on a meaningful sample, I therefore created counterfactual observations $\widehat R_{it}$ to account for time varying factors---including co-author characteristics---that might influence readability.
This procedure may have introduced bias in one of two ways. First, if co-author characteristics are not properly accounted for my estimates might be biased by peer effects. Second, if I have not properly accounted for time varying factors, then they , in turn, may be driving the results. This latter concern is a general concern for all of the estimates generated in this paper---*i.e.*, have I properly controlled for all factors that affect readability and correlate with gender---and discuss in more detail in [][Discussion].
The former concern could be tested by looking only at non-co-authored papers or papers predominately co-authored by women. [][AppendixExclusive ], [](#table8XC) reproduces [](#table8) on the sample of papers exclusively authored by women. Again, all figure agree: women write better and the magnitude and significance of that difference increases as $t$ increases.


*e.g.*, journal, field and co-author characteristics.
Nevertheless, the journals one submits to, the fields one gravitates toward and the people with whom one chooses to co-author are all endogenously determined by an author's experience. Higher standards, once uncovered, are potentially met by manipulating any of these variables. As a result, accounting for $t$-varying factors under $i$'s control potentially biases $D_{ik}$.
Yet it is also biased by omitting factors *exogenous* to an author's long-term decision-making process---*e.g.*, year of publication or referee stereotypes about authors' institutions. But 

That is, accounting for $t$-varying factors under $i$'s control potentially biases estimates of Condition 1, thus distorting selection of pairs that satisfy Condition 2 conditional on satisfying Condition 1. See [](#Footnote92) and the following robustness section for further discussion on this point.

Hence, my over all estimate of $D_{ik}$ would be potentially biased by including pairs for which $R_{i3}-R_{k3}$ is positive---*i.e.*, a gender gap between matched pairs exists---but $R_{i3}-R_{i1}$ is positive not because author $i$ learned that readability leads to higher acceptance rates, but because of some exogenously determined factor I do not account for.

: higher standards, once uncovered, may encourage authors to improve on their own, shift toward journals and fields where writing is easier or seek out co-authors who write more clearly
papers; meanwhile, gender differences in $i$ and $k$'s $t=3$ papers aren't constant across pairs. To account for this, I estimate the following equation via OLS in the four distinct time- and gender-specific samples of authors:

R_{it}$.
$$\widehat R_{it}=\beta_{0tg_i}+\beta_{1tg_i}\,\text{female}_i+\varepsilon_{it},$$
 where $text{female}_i$ is 1 if $i$ is female, 0 if male and $\teta_{tg_i}^{\text{\textit{AER}}} is the time- and gender-specific coefficient on the *AER* dummy variable, chosen because the majority of paper are published in that journal.
If $\widehat R_{i3}-\widehat R_{i1}$ is positive, then experience has taught$i$ to write more clearly. Indeed, even if the effect is largely driven by co-author characteristics or the choice of journal or field in which to publish, experience has nevertheless taught $i$ to publish in those journals, concentrate in those fields and co-author with people that demand clearer writing---*because doing so generates a higher acceptance rate*.
Similarly, if $i$ and $k$ are equivalent authors of the opposite gender and $\widehat R_{i3}-\widehat R_{k3}$ are positive, then conditional on journal, $i$'s papers are more readable. But then, if $\widehat R_{i3}-\widehat R_{i1}$ is positive, too, then experience has taught $i$ to generate more readable papers *and* $i$ *does* write more readable papers than author $k$ yet her acceptance rate is no higher than his, then experience has taught her to perform an activity that is not rewraded the same between genders.
Indeed, it could be driven by a multitude of factors---but the fact remains that at $t=3$, $i$ writes more readably than $k$.
denoted by $\widehat R_{it}$ for a paper published in the *AER* and co-authored with people of the same sex---*i.e.*, female ratio equal to 1 for women, 0 for men.
parsemoniously reconstruct $R_{it}$ to account for a subset of these factors by 
I would ideally account for this by restricting the sample to authors to who write their $t=1$ and $t=3$ papers on their own. $R_{i3}-R_{i1}$ then captures experience's impact on readability holding gender and author fixed (Condition 2); $R_{i3}-R_{k3}$ is gender's impact on readability holding experience and author equivalence fixed (Condition 1). Unfortunately, this sample is small: only ten women solo-authored their $t=1$ and $t=3$ papers.
Nevertheless, the most obvious way co-author characteristics could distort the results is if women are more likely to co-author with other women as $t$ increases, so the subsample of authors with $t=1$ and $t=3$ papers at least 50 percent co-authored by women may be the more relevant sample. Results using this subsample are available in [][Appendix50percent] and support the conclusions I present.
I present here a more parsemonious model to account for factors that vary with $t$, which, in addition to co-author characteristics include female ratio, journal, year and steroetypes about authors' institutions.
In order to perform the analysis on a meaningful sample, I therefore created counterfactual observations $\widehat R_{it}$ to account for time varying factors---including co-author characteristics---that might influence readability. Largely in order to preserve clarity, I opt for a parsemonious model when reconstructing $R_{it}$ at female ratio equal to 1 for women, 0 for men and median $t=3$ values of other co-variates---number of co-authors, institutional rank, institutional rank of the highest ranked co-author, $t$ for the most experienced co-author, publication year, dummies for each journal---using relevant coefficients and residuals from four separate time- and gender-specific regressions on readability. (See [][AppendixReconstruction] for regression output.) 
Meanwhile, 
$\widetilde r_{0i}^s$ and $\widetilde R_i^s$ may be influenced by factors that vary with $t$: female ratio, journal, year, field, co-author characteristics and stereotypes about authors' institutions. Nevertheless, the first goal is to isolate the impact of experience on her choice of readability. And indeed, her choice of journal in which to submit her paper, field in which to concentrate and co-authors to publish with are endogenously determined by her experience---she will gravitate towards those values which increase her acceptance rate. Thus, even if the effect is largely driven by co-author characteristics or the choice of journal or field in which to publish, experience has nevertheless taught $i$ to publish in those journals, concentrate in those fields and co-author with people that demand clearer writing---*because doing so generates a higher acceptance rate*.
For that reason, I only account for the subset of time-varying factors which authors must take as given---in this case, year of publication and the female ratio of the paper.
In the interest of clarity, brevity and accuracy, I opt for a parsemonious model that accounts for a subset of these factors, only. It is generated by 

 were at leat 50 percent female-authored at $t=1$ and $t=3$, and this may be a more relevant sample, since 
The latter is ideally accounted for by restricting the sample to I would ideally account for this by considering only the subsample of authors that solo-authored their first and third publications under identical conditions. Then $R_{i3}-R_{i1}$ would capture the impact $i$'s experience has on readability, conditional on gender; $R_{i3}-R_{k3}$ would reflect the impact of gender, conditional on experience.
Unfortunately, the sample is too small to restrict it to just those observations that satisfy these criteria. 
In a dynamic model of authors’ decision-making processes, I show that any gap caused exclu- sively by (i) or (ii) declines with experience. Yet the gap does not decline. It widens. Estimates from pooled subsamples and matching indicate women write more clearly as their publication count increases; men, possibly less so. This pattern of behaviour suggests discrimination—either directly in the form of biased referee scrutiny or indirectly from biased referee assignment (The- orem 1).
Using readability measures, I show that female-authored papers *become* more readable relative to male-authored papers precisely during the time that they are undergoing peer review. Assuming timing independence, this result suggests that the peer review process is responsible for the gap.
Why does peer review cause women to write more clearly? There are two possible explanations. Either women voluntarily write better papers – for example, because they’re more sensitive to referee criticism or overestimate the importance of writing well – or better written papers are women’s response to higher standards imposed by referees and/or editors.
Both explanations imply women spend too much time rewriting old papers and not enough time writing new papers. However, my evidence suggests the latter is primarily to blame. To show this, I model an author's decision-making process over time.


The purpose of this study is to test whether women are held to higher standards in academic peer review using an objective---but narrow---measure of research quality---readability. 
It would be useful to test the particular findings in this paper using 

This paper makes a curious discovery: female-authored articles in top economics journals are better written. This gap widens precisely while the papers are undergoing peer review as well as over a woman's lifetime. A dynamic model of an author's decision-making process shows that tougher editorial standards and/or biased referee assignment are the only explanations consistent with women's choices. Using a conservative measure derived from the model, I estimate that this type of discrimination causes senior female economists to write at least 9 percent more clearly than they otherwise would.
After examining the difference, I conclude that higher standards applied by editors and/or referees are primarily to blame.
No prior study has uncovered convincing evidence of gender bias in journal acceptance rates. It's encouraging that sex is irrelevant to publication outcomes, but that does not mean it has no effect on the process---or on the productivity of female academics. When female authors endure unfair criticism in referee reports, clearer writing and longer review times follow. With less time to spend on new projects, research output slows down.



Measurement error in the dependent variable could partially correlate with a paper's share of female authors due to factors unrelated to higher standards. [][Calculation] outlines principle sources of this error as well as steps I have taken to minimise them. Here I return to one in particular: what if sentence length and vocabulary complexity are not valid stand-ins for reading comprehension in this specific sample of academic abstracts? In this case, interpreting gender differences shifts from "women are better writers" to "women use simpler words and shorter sentences". The conclusions I derive from the analyses mapped out in this paper , however, should remain valid.

Interpreting gender differences imply shifts from "women are better writers" to "women use simpler words and shorter sentences". 

A similar argument can be made about a fourth area of non-classical measurement error: using syllables counts and word lists to determine vocabulary complexity. I raise this issue in [][Discussion].

But why? There is no obvious, non-cognitive reason *why* women should use simpler words, write shorter sentences and, especially, *behave according to the patterns mapped out in this paper*---other than higher writing standards for female-authored work.
the conclusions I derive from the analyses mapped out in this paper should remain valid.
conclusions derived from it remain inescapable
Thus, the inescapable conclusions---women use simpler wrods and write shorter sentencies conditioning on co-variates, *including field*, and that increases over their careers---
Since the analysis mapped out in this paper applies irrespective of one's precise interpretation of the dependent variable, 
the conclusion remains inescapable: women use simpler words and write shorter sentences conditioning on co-variates, *including field*. The analysis mapped out in this paper irrespective of one's precise interpretation of the dependent variable.

That leaves us with a final, straightforward alternative.
But what other reason would induce female economists to employ shorter sentences and simpler vocabulary conditioning for reasons that are independent of a desire to write more readably? Not because they research topics that use simpler words---including field controls has little effect. Maybe men just happened to use simple yet polysyllabic words? But then we should consequently observe no effect using the Dale-Chall score, which measures vocabulary complexity using a predefined word list.
A final alternative is rather uncomfortable. Perhaps female-authored manuscripts just aren't as good? 

Similarly, female research is more provocative, and more provocative work warrants more scrutiny. Nevertheless, if this were true, controlling for *JEL* classification would also reduce (or eliminate) the gap---unless women's work is systematically more provocative even among researchers in very narrow fields. There is some evidence for this hypothesis---provocative work is (presumably) highly cited work and contemporary female-authored work tend to be cited more[#Hengel2018]. Yet more provocative work that is also highly cited should also probably be published at higher rates---and there's no evidence women's papers are accepted more than men's[#Ceci2014]. Alternatively, perhaps the wider public excessively scrutinises female work, and referees respond similarly to minimise blowback. This explanation assumes a wider public capable of discrediting our work---a view many economists would (privately) disagree with. In any case, economics employs advanced mathematics and technical language, making it especially inaccessible to a layperson.
Nevertheless, this *still* implies a systematic inability by editors to appropriately assign relevant referees to female-authored papers. And again, that's bias.

it is still observed that sentences are shorter and vocabulary less complex in female-authored papers, conditional on the co-variates of control. And indeed, the weighted average of these two variables is informative in much the same way that the general "readability inference is. Conditioning on the control variables, there is no obvious reason why 
Indeed, I cannot think of a good reason why, after controlling for all of the factors that I do---including *JEL* controls---female-authored papers should employ shorter sentences and simpler vocabulary for reasons that don't involve higher standards, female irrationality or some explanation involving the general lower quality of female-authored work that somehow isn't picked up by the plethora of co-variates I include to account for it.
Underlying the conclusions presented in this paper is the implicit assumption that if some perfect measure of readability existed, then the difference between that measure and the readability scores I use in the analysis does not partially correlate with gender.
As I describe in [][Calculation], there are plenty of reasons why this might break down. Moreover, I discuss in some detail the effort I have taken to minimise the three primary sources of this error.
In [][Calculation], I describe the effort I have taken to minimise the thre primary sources of non-classical measurement error: (a) grammatical, spelling and transcription errors in the textual input; (b) errors in the estimates of vocabulary complexity and sentence length introduced by automating their calculation; or (c) embodied in the jump from using these two variables to infer readability.

The readability of text may be more important when interest is low than when it is high[#Klare1976,Fass1978]. It has also been shown that prior knowledge and beliefs about a topic improved reading comprehension for a particular text[#Woern1977,Spilich1979,Chiesi1979]. Easier readability of a text has more benefits for those of less knowledge and interest than those of more. Advanced knowledge of a subject can "drown out" the effects of an otherwise difficult text. This study also suggested that when reader interest is high, comprehension is not improved by writing the material below, rather than at, the grade level of the readers. When interest is low, however, comprehension is improved by writing the materials below, rather than at, the reading level of the readers.


 referees' lack of knowledge and interest in female-authored work. But then why aren't editors doing a better job of matching female-authored work to referees that are actually interested in---and knowledgable about---that work?
Nevertheless, the readability gap remains after controlling for detailed *JEL* codes. Thus, this must mean that referees are systematically less interested in women's work even after controlling for highly specific sub-fields. That is, male-authored papers are more appealing to editors and referees even compared to female-authored papers in the exact same field. This suggests a systematic bias against women in peer review that---although not the *fault* of individual referees---nevertheless reflects gender bias stemming from the systematic underrepresentation of women in economics.

&lt;!--
\iffalse
TO ADD:
Readability scores may be capturing something not related to readability.
Thus underlying all of the conclusions presented in this paper is the implicit assumption that if some perfect measure of readability existed that incorporated these factors, it would conclude the same thing. Thus assumption implies that the effects presented in this paper suffer from attenuation bias. given the relatively large textual samples used in the paper, the effects presented in fact suffer from attenuation bias.
Because readability scores omit these factors using them to infer causality between gender and any outcome they attempt to proxy for implicitly assumes that gender impacts these other facets in the same way that gender impacts readability. Or, in other words, if we *did* have a more comprehensive measure of readability, it would show the same thing.
And in any case, readability scores are perfect (or almost perfect) predictors of sentence and word length. Thus, the figures presented in this paper will always capture differences in the weighted averages of these two measures.

\fi
--&gt;

*Discuss classicial measurement error vs. non-classical measurement error*.
Classical measurement error is of most concern in small sample sizes with a high degree of variability between individuals because higher weights are placed on individual results. While I cannot rule out classical measurement error entirely as a factor driving my results, the fact that sample sizes are both relatively large and results do not appear to be driven by any individual results, suggests it is probably not, on its own, a particularly important factor.


Thus, if non-classical measurement error prevents inferring gender differences in *readability* it is still the case that sentence-length and vocabulary complexity in female-authored papers is negatively affected by peer review. 
Thus, should non-classical measurement error prevent inferring readability between genders, one must nevertheless confront the observation that sentence-length and vocabulary complexity in female-authored papers is negatively affected by peer review. 
The weighted average of these two variables is informative in much the same way that the general "readability" inference may be.

Negative values are papers released as working papers first and submitted to peer review second. These are the observations for which timing independence may have been violated. Pink represents papers with at least one female author; blue are papers with no female authors.
Additionally, most drafts have been widely circulated prior to NBER Working Paper release. The average length of the acknowledgement section in NBER Working Papers is 133 words. Most authors thank at least one person for helpful comments---the vast majority thank several---and mention having previously presented the research in conferences and seminars. Combined with evidece from [](#figure7) , this suggests that gender differences in non-peer-review feedback only affect a paper's readability *before* it is released as a working paper; any changes occuring afterward are due to the refereeing process.
even if male and female authors differ in how likely they are to send working papers out for comments, who they send those papers to and how they respond to those comments, gender differences in this respect occur *before* a paper is released as a working paper---and any changes in readability that occur between the NBER Working Paper and the published paper are indeed due to the refereeing process.

If post-submission manuscript changes are only made inside peer review---either because referees actually request them or authors believe (possibly mistakenly) that they will be requested in a future revision-
-- then timing independence will only be violated Fortunately, only a small fraction of observations are exposed to this window.
male and female authors differ in the changes they make to their manuscripts after releasing them as NBER Working Papers.
It is therefore possible that the effects we observe are due to authors compressing the content of their abstracts rather than due to peer review.
is probably only effective when identities cannot be found out via other means.

 suggesting blinding referees to authors' identities may no longer be the anti-bias fix it possibly once was.
Since gender bias is possible only when authors' identities are known or can be reasonably inferred, estimates in [](#table7) exclude the 279 articles in the sample subjected to double-blind review before the internet.
I now consider their impact. 
however, difference-in-difference estimates (reported in the final row) are not statistically significant. Although their consistent direction provides some (weak) evidence that masking authors' identities limits peer review's impact on the gender readability gap, this interpretation should be made with caution given the small number of papers---particularly by women----subjected to double-blind review before the internet.
Consistent with results from [][ArticleLevel], estimates in [](#table7) suggest 
y field, conditional on other explanatory variables. 
 that omitting field controls does not bias results, conditional on the other explanatory variables.
Moreover, (#table7) supports  the findings in [][ArticleLevel] that, conditional on the other explanatory variables, the gender readability gap is basically independent of field.
the similarity between the penultimate and final columns
given non-bias related factors specific to sub-field that may affect readability in a published paper will similarly affect the readability of a working paper, these estimates already implicitly control for sub-field. Thus, the similarity between the two estimation strategies in [] See for evidence further corroborating this finding.
Both strategies show a significant increase in the gender readability gap *ex post*. Assuming non-peer review factors are always independent of either its timing or gender, this establishes the desired causal link. Threats to this identification strategy are considered next.


[](#table4) establishes a gender readability gap for abstracts published in top economics journals. [](#table5) suggests it primarily forms contemporaneously. A possible contemporaneous cause is peer review.


Preliminary regressions suggest that substantial declines in readability are correlated with longer review times, particularly for women. A possible explanation for this inverted relationship is that these papers contained actual, major flaws. Thus, their content meaningfully changed between draft and final versions but ensuring revised text is readable is not a primary concern because more serious issues took precedence. Nevertheless, these results are highly sensitive; further structure is needed to properly interpret them.
in position 1 were grouped to form the first dummy variable, those in position 2 were grouped to form the second dummy variable, *etc
together; in position 2 together, *etc.* to position 10 to form 10 dummy variables; 
2, ..., 10 were grouped together, 
 groupings rank the number of articles in which an institution was listed as an affiliation
I create 64 dummy variables, each of which represents one or more institution(s); groupings reflect counts of distinct articles in which an institution was listed as an affiliation. Specifically, institutions listed in 59 or fewer articles were grouped in bins of 10 to form six dummy variables: the 751 institutions mentioned in 0--9 articles were grouped to form the first dummy variable, the 92 mentioned in 10--19 articles were grouped to form the second, *etc.* Fifty-eight institutions were affiliated with 60 or more articles; each is assigned its own dummy variable. 

As shown in [](#table6), female-authored NBER abstracts tend to be slightly longer (more words) than male-authored abstracts---but then slightly shorter (again, in terms of total words) in published form. [](#figureC7a) shows the distribution of abstract lengths by gender. The graph on the left displays abstract length in NBER working papers; the graph on the right displays the distribution of abstract lengths in the published paper, again broken down by gender. Word counts in NBER drafts are more spread out; they are much more tightly compacted in the final, published version of those same papers. There appears to be no massive gender difference in the distribution of these two figures; also, there appears to be no outlier papers driving the results.
&lt;!--\input{$HOME/Dropbox/Readability/draft/tex/fixed/figureC7a}--&gt;

referees may be more likely to focus on readability issues in papers with few structural issues.
Thus, 

correlations between readability and review time will produce biased results. Indeed, initial regressions suggest this may be the case---papers that *decline* the most in readability spend the *longest* in peer review---suggesting their content has changed substantially between draft and final version but no particular effort has been made to ensure the changed text is readable. Nevertheless, these results are highly sensitive to the particular specification. Further structure is clearly needed in order to properly interpret the results; I feel this is beyond the scope of this paper.
As a result, correlations between 
Papers that improve in readability may contain fewer structural issues 
conditional on experiencing a large improvement in readability, papers may in fact have fewer underlying structural issues and thus possibly experience faster review. 
Unobserved heterogeneity in the underlying quality of submissions should not bias conclusions elsewhere in the paper as long as it does not partially correlate with the share of female authors on a paper. That is, paper quality is similarly distributed across gender after controlling for year, editor, *etc.* Paper quality *may not*, however, be similarly distributed across gender after controlling for review time when estimating the impact gender has on readability (or visa versa)---conditioning on one when estimating the other opens up a backdoor channel to the unobserved heterogeneity[#Pearl2018]. This issue is sometimes referred to as endogenous selection bias or the problem of conditioning on a collider variable.
Conditioning on one when estimating the other opens up a backdoor channel between unobserved heterogeneity and gender if referees are, *e.g.*, more likely to elevate criticism of readability in female-authored papers that are otherwise fine. 
Thus, conditional on readability, female-authored papers are more likely to be "fine" and therefore have 
 otherwise known as endogenous selection bias or the problem of conditioning on a collider variable.
or the other, however, opens up a backdoor channel 
Note that omitting this variable elsewhere in the paper should not bias conclusions in this paper as long as this  is not partially correlated with the share of female authors on a paper.
 conditional on the controls. That is, it is assumed that, after controlling for year, editor, etc. female-authored papers are not more likely to have structural issues than male-authored papers). Conditioning on readability, however, introduces a backdoor channel to 

 structural issues are uncorrelated with the variable of interest (female ratio) conditional on the controls. (I.e., it is assumed that 
the possibility that, e.g., women's papers have *fewer* issues
In this paper, I do not test directly relationship between review time and readability. Conditional on acceptance, correlation between the two is ambiguous even after controlling for gender and author experience as discussed in [][IndirectEffect]. This is due to unobserved heterogeneity in the quality of the underlying submission. For example, lower quality submissions that are nevertheless accepted probably undergo longer review. Yet the fact that they contain more pressing concerns may mean referees pay less attention to readability. Meanwhile, referees may be more concerned with readability in papers with few other errors. Indeed, initial regressions suggest this may be the case---papers that *decline* the most in readability spend the *longest* in peer review---suggesting their content has changed substantially between draft and final version but no particular effort has been made to ensure the changed text is readable. Nevertheless, these results are highly sensitive and change sign (and significance) drastically depending on the specification. Further structure is clearly needed in order to properly interpret the results; I feel this is beyond the scope of this paper.
In the next section, however, I testhe former claim, however, implies that junior wormen---or at least women---undergo the longest peer review.
I have, however, collected review time data for manuscripts published in *Econometrica* in order to tentitively test the former claim
Moreover, if the same is true among rejected manuscripts---which are not included in my dataset---then [](#figure6) and [](#table12) suggest junior female economists experience the highest desk rejection rates, as well.

The direct effect is positive and largest for accepted manuscripts by women with the least experience. This implies that junior women undergo the longest peer review. I investigate this hypothesis in [][Duration]. 
: senior economists of both sexes would rather spend more time rewriting their manuscripts *before* peer review and enjoy higher acceptance rates and/or faster review *during* it

 In any case, [](#figure6) suggests that junior female economists undergo the most arduous peer review process.
Given experienced women prefer, like men at all experience levels, to sacrifice time to increase acceptance rates, the large direct effect in peer review observed for junior women suggests they initially underestimate referees' expectations. 

either by internalising explicit comments on text readability in referee reports from earlier papers or making the (un)conscious connection that acceptance rates are higher (or review times faster) when text is clearer.
 but they learn about them over time and adapt their *ex ante* writing style accordingly.
support's [](#Theorem1)'s implicit conclusion that female authors learn about referees' thresholds over time and adapt accordingly. If the payoff from lucid explanation is high, people will catch on---either by internalising explicit comments on text readability in referee reports from earlier papers or making the (un) conscious connection that acceptance rates are higher---or review times are faster---when text is clearer.
Higher standards affect women's behaviour in ways that are direct---*i.e.*, as an immediate response to referee criticism during peer review---as well as indirect---*i.e.*, as a pre-emptive response in anticipation of criticism. 
2--5 increase in the gender readability gap. The second is the indirect response of anticipating higher standards in peer review by writing more clearly before submitting. Differencing the estimates in [](#table10)---which reflect the sum total of both the direct and indirect effects---from those in [](#table7) suggests the indirect effect increases the gender readability gap by 5--6 percent.
[](#figure6) supports [](#Theorem1)'s implicit assumption that female authors learn about referees' thresholds over time. If the payoff from lucid exposition is high, people will catch on---either by internalising explicit comments on text readability in referee reports from earlier papers or making the (un)conscious connection that acceptance rates are higher---or review times are faster---when text is clearer. Applying that payoff only to women yields a succinct explanation for the gap’s observed growth.
Although [](#table12) concurs, viewing certain estimates in isolation give different, more orthodox impressions. First, panel one suggests that the readability gap declines over increasing $t$. This narrow view favours alternative explanations---*e.g.*, sensitivity, poor information and/or justified statistical discrimination---over bias by referees and/or editors. Only when complemented by [](#figure6) do we fully appreciate that the smaller gap *in* peer review is completely offset by a wider gap *before* peer review. Unfortunately, this reaction poses another identification problem: senior female economists adjust to biased treatment in ways that confuse underlying discrimination with voluntary choice. Both observations suggest that studies must account for all relevant decisions at a single point in time *and* the evolution of those decisions over time. Otherwise, they may underestimate discrimination and misallocate responsibility.
Lack of correlation will not imply lack of causality. As [](#figure6) illustrates, this is particularly true in equilibrium. Just like the path of a boat is not affected by the wind pushing its side when it is helmed by an seasoned skipper. Referee demands do not affect writing in peer review when the paper in question is written by an experienced female economist.
In equilibrium, the response conceals the cause.
In equilibrium, women's response to discrimination conceals it
The equilibrium response to discrimination ends up concealing it.
In equilibrium, discrimination is concealed.
Concealed discrimination in equilibrium.
Like an expert skipper steering his boat straight against the wind 
An expert skipper renders the 
Like an expert skipper steering straight against the wind renders the impact of the wind orthogonal to the boat's direction, 
wind blowing a boat to the side, and the skipper turning the wheel to stay straight yielding no correlation between the wind and the boats direction due to endogenous steering.

Moreover, [](#figure6) emphasises that only *inexperienced women* improve readability during peer review. Assuming choices by senior economists express optimal tradeoffs with full information, this implies that women initially underestimate referees’ expectations. Men, however, do not. Their draft and final readability choices remain relatively stable over increasing $t$.
Are men just better informed? Yes and no. Male and female draft readability scores for first-time publications are exactly the same, suggesting both start out with identical beliefs. Yet identical beliefs do not promise identical information. Men are indeed better informed because the standards they believe apply to them actually do. Female authors make the mistake in assuming those same standards apply to women, too.


If one person with blue hair experiences discrimination, we have no plausible way of identifying the other people with blue hair who had 
would compare a woman plausibly experiencing one particular form of discrimination to a version of herself identical on all other dimensions except that she definitively did not.
We do not have this alternate universe. We are therefore left with the unenviable task of picking apart two separate treatments---gender and discrimination---that are impossible to experimentally randomise.
When studying gender discrimination, we have two treatments---gender and discrimination. Both are impossible to randomise---
Both likely have independent effects on the outcome variable, but both are 
At the heart of every identification strategy is some behaviourial assumption of how the participants have reacted to discrimination. In the Petra Todd study, they assume that agents immediately and optimally adjust the amount of contraband they carry to react to police search rates. In the Kleven study they assume fertility decisions are independent of earnings (and therefore discrimination).

This comparison would then not only recover whether discrimination actually existed, but also the precise magnitude of its effect.
There are two treatments. Peer review and gender. Peer review happens to everyone, but we can compare the within person readability response. So we see everyone's readability pre-treatment 1 and post-treatment 1.
But gender is the second treatment. Being female does not happen to everyone. So we can only compare the non-treated sample with the treated sample.
A finding that minority searches are systematically less productive than white searches is accordingly evidence that police require less probable cause when searching minorities. Such evidence would suggest that if police required the same level of probable cause when searching minorities as when searching whites, there would be fewer minorities searched (or proportionally more whites searched).
Disparate treatment tests are normally tests of decision making on the margin, but real world data at times only allows researchers to assess infra-marginal effects.
Real behavioural change must be accounted for. The difficulty of capture the effect of interventions on real behavioural outcomes (rather than intended or planned behaviour or other proxy measures of behaviour) and there is a 
The way individuals who experience discrimination respond to it distorts test of discrimination at the margin. It may be that earlier experiences that people have and the way they respond to it means that the individual-specific marginal effect we think we are observing is in fact an individual-specific infra-marginal effect.
If either the decision or the outcome is nondichotomous, it is easier for the researcher to identify the marginal effects. If referees are setting continuous readability thresholds, then we can directly test the marginal impact of their decision. The referees' ability to individually vary the threshold in a sense makes every paper marginal, and therefore avoids the infra-marginal problem that has plagued the application of outcome tests to the mortgage context.
The people who are below the margin---and so would get in regardless of affirmative action---are below the margin for historical reasons.
Similarly, if the outcome itself is nondichotomous, it may be easier to identify whether the threshold decisionmaking is discriminatory. Thus, for example, in the citation studies, researchers in measuring the number of citations given to articles written by women and men can assess not just the average level of success but can estimate the entire distribution of sucess. By analysing this distribution, it may be possible to identify whether the editors in making acceptance decisions systematically demand more or fewer expected citations in accepting the marginal (least likely cited) articles of minority authors.

The (false) finding of no racial bias using standard OLS specifications suggests that recent attempts to measure judge decisions using machine learning algorithms could be biased by these issues.
Moreover, our finding that bail judges are monolithic in their treatment of white and black defendants and, as a result, that there is no relative racial bias in bail setting, highlights the importance of developing empriical tests that can detect absolute racial bias.
male and female draft readability scores for first-time publications are exactly the same, suggesting both start out with identical beliefs. Yet identical beliefs to not promise identical information. 
All things equal, economists who anticipate referees' demands are rejected less often; economists who don’t enjoy more free time. [](#figure6) implies little---if any---gender difference in this tradeoff: senior economists of both sexes sacrifice time upfront to increase acceptance rates.

We have to realise two things:
1. Define precisely what exact discrimination we are measuring.
2. 


Becker does imply that market discrimination derived from animus should flourish in less competitive markets.

Market actors respond to the incentives created by discriminatory behaviour, and in doing so they may reduce the ultimate effect of discriminatory behaviour on outcomes such as wages.

Standard regressions recover average differences between blacks and whites conditional on controls, but economic theory tells us we want to know whether there are racial differences for the *marginal* cases. But whether we are looking at the marginal direct or indirect effect obviously also matters. 

In equilibrium, women's response to discrimination conceals it
The equilibrium response to discrimination ends up concealing it.
In equilibrium, discrimination is concealed.
Concealed discrimination in equilibrium.
Like an expert skipper steering his boat straight against the wind 
An expert skipper renders the 
Like an expert skipper steering straight against the wind renders the impact of the wind orthogonal to the boat's direction, 
wind blowing a boat to the side, and the skipper turning the wheel to stay straight yielding no correlation between the wind and the boats direction due to endogenous steering.
one must be aware of the stage at which women 


Yet women, in reality, *are* more poorly informed---because they believe the standards that apply to men also apply to them.
Third, it does not suggest that women have some sort of innate preference for writing well.

This pattern of behaviour is consistent with an explanation of women initially underestimating the standards that apply to them but then learning about those standards as they gain experience in peer review and gradually adapting their behaviour to adjust to those standards.
A naïve estimate of the indirect effect differences these two figures
response increases the gender readability, on average, by 2--5 percent.
Women react to higher standards both immediately in peer review, as they are being experienced, and before peer review in anticipation of experiencing them.
immediately by writing more clearly during peer review (direct effect). They probably also react to those higher standards in peer review by w
Higher standards in peer review cause women to write more clearly during peer review (direct effect)
Higher standards in peer review affect women's writing directly---women improvce clarity during peer review---and indirectly---women anticipate higher standards by writing more readably before submitting their papers to peer review.
e more readably during  as an immediate response to 
in two separate ways. The first represents the *direct effect* of higher standards---*i.e.*, women's immediate response to the higher standards they face in peer review. The second is an *indirect effect* corresponding to how well women choose to write *before* peer review. That is, if women experience bias *in* peer review, then they will adapt how well they write *before* peer review.
The estimates presented in [](#table7) represent the average direct effect of higher standards conditional on NBER affiliation (averaged over experience). They suggest that the size of this effect is 2--5 percent. The estimates in [](#table10), on the other hand, reflect the sum total of the direct and indirect effects conditional on experience (averaged over NBER affiliation). They suggest that these combined effects cause experienced women to write 7--11 percent more clearly than they otherwise would.
The fact that the figures in [](#table10) are larger than those in [](#table7) could reflect the importance of the indirect effect or the importance of experience or both. 
Highlights the importance of accounting for infra-marginality when estimating gender bias.
The inframarginal *is* the marginal.
One of Becker's insights was that in a market setting, we should not expect racial or gender wage differences to derive from how discriminatory employers are on average.

There are two separate effects of higher standards in peer review. The *direct* effect
Asymmetric editorial standards and/or biased referee assignment affect women directly
—as already discussed, women write more readably during and spend longer in peer review. They probably affect women’s behaviour indirectly, too.
The estimates presented in [](#table7) represent the *direct effect* of higher standards---*i.e.*, women's immediate response to the higher standards they face in peer review---averaged over women of all experience levels. The estimates in [](#table10), on the other hand, reflect the sum total of the direct *and* indirect effect---*i.e.*, the change in how well women write *before* peer review as they update beliefs about referees' standards based on previous experience *in* peer review---of higher standards for experienced women, only.
The figures in [](#table10) suggest this combined direct and indirect effect peer review causes *experienced* women to write 9 percent more clearly than they otherwise would. The figures in [](#table7) suggest that the direct effect of peer review causes all women, regardless of experience, to write 2--5 percent more clearly than they otherwise would. 
The fact that the figures in [](#table10) are larger than those in [](#table7) suggests *either* that the indirect effect is larger than the direct effect *or* the combined effect for experienced women is larger than the direct effect for women across all experience levels.

The fact that estimates in [](#table10) are 5--6 times larger than those from [](#table7) suggests *either* that the indirect effect---*i.e.*, the change in how well women write *before* peer review as they update beliefs about referees' standards based on previous experience *in* peer review---is more important than the direct effect *or* the direct effect is m
Estimates in [](#table10) are 5--6 times larger than the direct effect evidence from [](#table7) suggests that the indirect effect---*i.e.*, the change in women's behaviour as they update beliefs about standards---is more important to the gender readability gap than the direct effect---*i.e.*, women's immediate response in peer review to those higher standards.
And indeed, if we look at the change in the readability gap between the NBER working paper and final versions as women gain experience---and therefore update beliefs about referees' expectations---we see suggestive evidence that this is, indeed, the case.

A second issue is that readability scores are only as accurate as the underlying text fed into them. Measurement error obviously increases when they are applied to grammatically incorrect, improperly punctuated or poorly transcribed passages of text. 

Thus, non-classical measurement error from (c) affects the superficial interpretation of a difference: 
instead of identifying a gender difference in *readability* 

 even if non-classical measurement error prevents 
in the absence of errors by (a) and (b), 

A more concerning (and relevant) measurement error is that readability scores systematically mismeasure "readability" that the degree of misreporting or mismeasurement is correlated with the variable of interest (gender) conditional on observable controls. 
is correlated with the variable of interest (gender), then the results presented in this paper 
embodied within the scores themselves
the software used to calculate readability scores *or* the scores themselves intrensically embody measurement error in a way that is 
Either due to mismeasurement instroduced by the software used to calculate readability score *or* from from some inherent 
potentially suffer from a form of mis-measurement

try to discuss the means I have taken to try to reduce this error has 
Nevertheless, inferences made from non-causal predictors are necessarily biased by measurement error. For this reason, I collect a relatively large sample of 
Using the scores in empirical analysis therefore requires confronting the issue as transparently and comprehensively as possible[#Loughran2016]. I try achieve this in several ways.
While all of these components are indeed important for determining readability, the few measures developed to capture them are also highly correlated with readability scores. Nevertheless, as imperfect measures of "readability", to be interpreted as such requires confronting the issue of measurement error.
Indeed, as highlighted in [](#figure0X) are not perfectly correlated with 
Readability scores are a weighted average of vocabulary and sentence length. Inferring "readability" introduces measurement error---as shown in [](#figure0X), the scores are indeed highly correlated with alternative measures of readability, but they aren't perfectly correlated with those measures. 

Using readability scores in this manner has several important advantages. First, researcher subjectivity is avoided. Hundreds of readability scores exist, each employing slightly different methodology to analyse the readability of a piece of text. I simply choose the five most common measures. Moreover, although it is possible (probable?) that one of the underlying components of readability scores is truly driving nay observed gender differences, there is no prior research to pinpoint which one that would be. Using the overarching score, meanwhile, incorporates both measures. This will obviously come at a cost (in terms of noise) if one of the underlying components is truly driving gender differences in readability; but the benefit is reduced researcher subjectivity.

I do not indent to use readability as a proxy for the overall quality of a paper. Instead, I look at readability after controlling for other components---citations, author institution---that reflect (or are a consequence of) the quality of the underlying paper. That is, holding everything constant, what is the marginal difference in readability of male and female-authored papers? 
these readability scales only work as supplementary tools for a text where language proficiency is already confirmed through other means.
These scores are only approximations of reading difficulty and the educational/intellectual level your audience might need to understand your work. They are obviously not indicators of writing quality or how well your writing will be received by your target audience. I would say that a work with such low readability scores and such long sentences (26 for 1200 words?) is likely to come off as too obscure or elitist to the general reader, and would have a very limited audience. But, of course, such things are a matter of taste and convention. So ultimately, this is just my personal opinion.
Of course, readability formuals are not perfect predictors of readability. Evidence suggests they may not be accurate enough to adequately assess or guide development of individual documents, including legal briefs[#Sirico2007], financial disclosure documents[#Loughran2014] or school reading material[#Ardoin2005, Powell-Smith2001]. But any measure used in quantitative analysis to an extent suffers from this criticism. Unemployment status is a noisy predictor of an individual's future employment situtation, but aggregated over many individuals, it more accurately predicts how easy it is to get a job in a particular geographic unit. IQ scores are likely a noisy predictor of one person's intelligence---but are more accurate when aggregated over larger samples. The PISA score is a noisy measure of the educational atainment of a particular country. But---as with readability scores---each of these measures do correlate with the outcome of interest.
However, the use of certain quantitative measures is well established in research. Thus, the process of---and therefore trust in---data collection is more mature. Textual analysis, on the other hand, is still a developing field, with few discipline-specific norms for measuring inputs and selecting methods used to address a given empirical issue.
It has been claimed that traditional tests of reading difficulty "are dubious instruments for adequately assessing the readability of accounting narratives that are adjust oriented and specialist in nature."[p. 172\]\[][#Jones1994]. Yet this criticism was aimed at using readability scores to assess the readability of *specific* texts---*i.e.*, readability scores may not be precise enough to determine if a specific financial disclosure document is sufficiently clear or if a particular book is appropriate for certain grade-levels.
Yet while the margin for error in each individual case may be large, readability scores nevertheless *do* correlated with writing clarity, making them appropriate measures to estimate gender differences when sample sizes are sufficiently large. Just like saying the U.K. has a stronger economy than France because it has a higher estimated GDP per capita is less certain than making the more general claim that countries with higher GDP per capita have, on average, stronger economies than countries with lower GDP per capita. Similarly, the statement "Finland has a better education system because it earns a higher PISA score" is less certain than the statement

I do not indent to use readability as a proxy for the overall quality of a paper. Instead, I look at readability after controlling for other components---citations, author institution---that reflect (or are a consequence of) the quality of the underlying paper. That is, holding everything constant, what is the marginal difference in readability of male and female-authored papers? Yet, although the purpose of this paper is just to show that women are better writing, it would be nice to see that better writing in female-authored papers is associated with other positive quality outcomes. INSERT FIGURES SHOWING THIS.
; mechanically shortening sentences and using simpler words probably won't improve it. This is because the concept of readability is broader than these two components. 
 Interpreting differences in readability scores as actual differences in readability---coupled with the relatively large samples used in this paper---suggests that effects I present suffer from attenuation bias.

As discussed in [][Abstracts]
In X I discuss how remaining measurement bias may affect the results I obtain.
Readability scores omit many other aspects important for readability, such as g

---or anything indirectly correlated with gender---is not affected by 

Specifically, readability scores only measure the "surface features" of text and ignore other features such as content, organisation, coherence and human interest as well as features of the layout such as font, spacing, the prevalence of relevant figures and graphics and reader interest are all important components of reading comprehension ignored by the formulas.
Thus, adaptions and revisions of texts by mechanically lowering readability scores can result in less readable text. Also, reduced cohesion may be brought about by mechanically shortening sentences. Thus, classifying text by readability scores comes with all the same problems of classifying intelligence using IQ tests, or biases using implicit bias tests or really anything using supervised machine learning texts. Thus, applying readability scores in academic research comes with similar cautions.
Yet, despite the success of readability formulas, they remain controversial. Critics of the formulas typically concentrate on three main criticisms. First, one limitation of the formulas is that they do, indeed, contain only “surface features” of text and ignore other features like content and organisation. The research shows, however, that these surface features—the readability variables—with all their limitations have remained the best predictors of text difficulty as measured by comprehension tests (Hunt 1965, Bormuth 1966, Maxwell 1978, Coupland 1978, Kintsch and Miller 1981, Chall 1984, Klare 1984, Davison 1984 and 1986, Carver 1990, Chall and Conard 1991, Chall and Dale 1995).

Second, some critics contend that the readability formulas were developed for children and they never were never formulated or tested with technical documents. This is actually incorrect. All five formulas were originally developed to assess the reading difficulty of adult reading material[#Dubay2004,Vieth1988;]. For example, Klare (1952) tested the Flesch Reading Ease and Dale-Chall formulas against the 16 standardised passages of reading comprehension tests developed for adult readers. Several extensive studies (Klare et al. 1955a, Klare et al. 1957, Klare and Smart 1973, Caylor et al. 1973, Kincaid et al. 1975, Hooke et al. 1979) used materials developed for technical training and regulations in the military to formulate and test several of today’s most popular formulas such as the Flesch-Kincaid grade-level formula. A series of studies in the military (Klare et al. 1955a) examined how prior knowledge as well as the text variables affect the retention and the acceptability of technical documents. more readable versions resulted in: Greater and more complete retention. Greater amount read in a given time. Greater acceptability (preference).
Additionally, as shown in [][Abstracts], NBER abstracts---both those by men and women---change significantly in readability between draft and final version. As I show in [][NBER], drafts are typically released as NBER working papers *after* they were already submitted to peer review (and this is especially true for female-authored papers). Since post-submission, authors probably do not solicit non-referee feedback nor make non-referee requested changes. Thus, these changes in both male and female-authored papers probably do reflect the peer review process.

The Flesch Reading Ease scales from 0 (hard) to 100 (easy). In contrast, the other four scores generate grade levels estimating the minimum years of schooling necessary to confidently understand an evaluated text---and so lower scores indicate easier-to-read text. To minimise confusion, I multiply the four grade-level scores by negative one. Thus, higher numbers universally correspond to clearer writing throughout the paper.
Moreover, some authors have proposed methods they consider to be better estimates of readability. [#Benoit2017;] proposes X; [#Loughran2014;] proposes X, *etc.* While all of these studies criticise one or more traditional measures of readability---and propose new, alternative measures of the readability of complex documents---they also, universally, confirm that traditional metrics of readability do indeed correlate with the actual readability of a particular piece of text.

a number of positive---and a few negative---firm characteristics and financial market outcomes[for a thorough review of this research, see\]\[][#Loughran2016].
The role of readability is also considered important as a central or adjunct variable in assessing financial documents. The empirical results of several studies studies repeatedly document a statistical association between a traditional readability measure and other attributes of the firm. The first paper to examine the link between annual report readability and firm performance for a meaningful sample is [#Li2008;]. He measures the readability of annual reports using the Gunning Fog Index and the number of words contained in the annual report. He finds that firms with lower reported earnings tend to have annual reports that are harder to read (*i.e.*, high Fog Index values or high word counts). He also finds that companies with more readable annual reports have higher earnings persistence. Other researchers have also used the Gunning Fog Index as a measure of annual report readability. [#Biddle2009;] find that firms with high reporting quality (using the Gunning Fog Index and two other variables) are associated with greater capital investment efficiency. [#Guay2015;] find that companies with less readable annual reports (based on six different readability measures, including the Gunning Fog Index) tend to mitigate this negative readability effect by issuing more managerial forecasts of earnings per share, sales and cash flows. [#Miller2010;] finds that small investors trade significantly fewer shares of firms with high Fog Index values and word counts (*i.e.*, less readable annual reports) around the 10-K filing date. Less readable annual reports should be harder to process especially for less-sophisticated investors. Other studies have found that readable business documents are linked to lower analyst dispersion and greater earnings forecast accuracy[#Lehavy2011], higher trading volumn[#DeFranco2015] and stronger analyst reactions to both good and bad news[#Rennekamp2012;]. Other studies have shown that investors are more likely to invest in firms with shorder, more readable 10-Ks[#Lawrence2013], and firms with more readable documents have more pronouced small investor trading activity around the filing date [#Miller2010].
used them to show that ministerial speeches became easier to understand post democratisation likely because of an effort to appeal to new voters, who are poorer and less educated.
Political scientists have used them to positively link clarity of communications with better informed voters
The former tend to use readability scores to link linguistic sophistication to political outcomes[see, *e.g.*,\]\[][#Spriggs1996,Sirling2016,Owens2011,Bischof2018]. The latter use them to link clarity of communication with financial outcomes[for a comprehensive review of this research, see\[\[][#Loughran2016]. Generally, this research has found concrete benefits to clarity.
 They have gotten the widest pickup as a proxy for "complexity" in political discourse and corporate disclosures. Generally, this research has used these measures to proxy for textual complexity with an eye to link linguistic sophistication to outcomes---and a focus on the concrete benefits to clarity. [#Janson2011;], for instance, studies the reading level of communications by four central banks, equating lower reading levels of bank communication with greater clarity, which they link to positive effects on the volatility of returns of financial markets. Likewise, [#Owens2011;] and [#Spriggs1996;] examine the complexity of Supreme Court decisions, pointing to the importance of clarity in court opinions. In the context of the British parliament, [#Spirling2016;] applies readability measures to document the democratising effects of franchise reform on elite speeches. Studying post-war Austrian and German elections, [#Bischof2018;] find that simpler manifestos make for better informed voters.


This decision was made on the assumption that a gender readability gap---if it exists---is function of (i) the probability a particular portion of analysed text was written and/or revised by a female co-author; and (ii) referees' beliefs about female authors' contributions to the writing of a paper. I assume the intersection of (i) and (ii) is positively related to the ratio of female authors on a paper. This assumption is supported by prior research suggesting that co-authors---regardless of seniority---tend to share responsibility for writing and revising collaborative work[#Hart2000,Kumar2016]. [CONFIRM REFERENCE]


more readable text longer in short term memory
studied the reading efficiency and retention of 120 male aviators in a mechanics course at Chanute Air Force Base in Illinois. They used two versions of technical training materisl, hard (13th--15th grade) and easy (7--8th grade). They measured reading efficiency with an eye-movement camera with which they could determine the number of words read per second and the number of words read per fixation. The study showed that the easy text significantly improved both readign efficiency and retention. [Rothkopf1977;] showed the connection between readability and how many words a typist continues to type after the copy page is covered.
A series of studies in the military were conducted at the Sampson Air Force Base in New York and Chanute Air Force Base in illinois using 989 male Air Force enlistees in training with different versions of the same texts. They used the Flesch Reading Ease and Dale-Chall formulas to rate the texts as Easy (grade 7), present (12th grade) and hard (16th grade)[#Klare1955]. The study found that more readable versions resulted in greater and more complete retention, greater amount read in a given time and greater acceptability.

with results from multiple choice reading comprehension tests, cloze tests from 46 published studies 
with results from reading comprehension tests, from multiple choice reading comprehension tests
Early statistical validity of readability scores was established by applying the formulas to passages of text that had previously been ranked according to difficulty based on the average reader's ability to correctly answer several comprehension questions after reading it. All scores correlate highly with these tests: 0.64--0.70. Later studies confirmed their validity using the cloze procedure[for a detailed and extensive overview of this research, see\]\[][#Klare1975]. Other studies have also validated readability scores against the judgement of both expert and non-expert human judgement, oral fluency tests and other readability scores. In general, readability scores are highly correlated with other readability rescores; they also exhibit a strong association with rankings provided by human judgement. Their correlations with measures of oral reading fluency---e.g., the number of words read correctly aloud per minute---are positive, but more moderate[#Begeny2014,Ardoin2005].

 [#Bormuth1966;] found cloze scores correlated highly with all the individual comopnents of readability scores used in this analysis (syllables per word, sentence length, *etc.*). This study also provided evidence that the correlation between formula variables and comprehension do not change as a function of reading ability. These studies continued to show strong statistical correlation between readability scores and writing clarity

The constants in each formula vary widely as do the components used to rank vocabulary. The Flesch Reading Ease and Flesch-Kincaid scores rely on syllable count. Gunning Fog and SMOG total polysyllabic words (words with three or more syllables). Dale-Chall tallies words not on a pre-defined list of 3,000 so-called "easy" words. Because of these differences, the four grade-level scores rarely generate identical figures; nevertheless, all five scores produce roughly equivalent rankings(for a review of literature supporting this conclusion, see, [][AppendixMetaAnalysis]).

---*i.e.*, readers' ease in understanding a passage of text. Hundreds of formulas try to measure readability by exploiting this relationship. I concentrate on the five most widely used, tested and reliable measures for adult reading material: Flesch Reading Ease, Flesch-Kincaid, Gunning Fog, SMOG (Simple Measure of Gobbledegook) and Dale-Chall[#DuBay2004]. Each are listed in [](#table2).

In the wake of serious public concern about adult literacy post-World-War-I, Rudolf Flesch developed one of the first readability scores tailored exclusively for adult reading material. Alternative readability scores for adults soon followed: In 1948, Edgar Dale and Jeanne Chall developed the Dale-Chall formula for advanced readers. In 1952, Robert Gunning published his own formula, the Gunning Fog Index, to assist newspapers and popular magazines to improve their writing. G. Harry McLaughlin published his SMOG formula in 1969. In 1976, a study commissioned by the U.S. Navy modified the Flesch Reading Ease formula to produce a modified score known as the Flesch-Kincaid formula.
In 1953, Wilson Taylor of the University of Illinois published the cloze test. The cloze test is generally considered a better measure of difficulty because it evaluates not the diffiuclty of a wor
In Flesch's original readability study, he found the Flesch Reading Ease correlated with results from the McCall-Crabbs tests was about 70 percent. READABILITY FOR THE OTHER SCORE
CLOZE TEST

Flesch describes the test lessons as a "series of 376 passages for children which had been previously graded on the basis of ten comprehension test questions appended to each passage" (1943b, 6). The tests were supposed to teach children how to comprehend many different kinds of reading materials, how to enjoy reading, and how to be motivated and to improve expressing themselves orally. They were called "standard" test lessons because every test would show "how well the normal or typical pupil would read these same lessons" (McCall and Crabbs 1925,
184).
The estimated coefficient of correlation of his measure, the Flesch Reading Ease score, with a benchmark measure of text comprehension---the McCall-Crabbs *Standard Test Lessons*---was 70 percent.



More recent research using other benchmark measures of text comprehension have backed this earlier evidence up. One of the oldest and most commonly used formulas, the Dale-Chall has consistently proven to be a reliable formula for gauging general text difficulty for higher ability readers[#Begeny2014,Hintz2004]. [#Ardoin2005;] find a modest relationship between reading fluency and the four grade-level readability scores, and especially strong evidence in favour of the Gunning Fog readability measure.

Other studies have shown that the Flesch Reading Ease accurately---albeit noisily---gauges the readability of sophisticated political discourse[#Beniot2017;], although the correlations they reported are more muted than those found in earlier studies.

The most popular reading comprehension test was the McCall-Crabbs *Standard Test Lessons*. The Reading Ease formula correlated .70 with the McCall-Crabbs criterion
Most formulas have also been tested against other reading comprehension tests geared more toward adults.

. These lessons have been convenient statistically because there are a large number of reading passages, covering a
wide range of difficulty, resting upon extensive testing, and providing detailed grade scores.
were based on collections of text passages that had been ranked according to difficulty based on the average reader's ability to correctly answer several questions meant to test his comprehension. The underlying reading comprehension tests---including the McCall-Crabbs *Standard Test Lessons*, the Ojemann tests and the Gray and Leary tests---
standardised reading comprehension tests that had previously been administered to a . That is, readability formulas were applied to a large number of text passages which had previously been ranked according to difficulty based on the average number of correct responses 

That, a large number of readers were given a passage of text and then asked various questions about that text to determine how well it was understood. Passages of text in which a high percentage of test takers got most of the questions correct are assumed to be "easier"; passages with fewer right answers are assumed to be harder. Readability formulas were then applied to that text and 
The earliest studies showing correlation between readability scores and text comprehension were based on 
McCall-Crabbs *Standard Test Lessons*. These tests involved a series of 376 passages for children, each of which included ten comprehension test questions to evaluate how well the test-taker had understood the piece of text. Grade-level reading ability was calibrated by how many questions were answered correctly---*i.e.*, more correct answers indicated higher reading ability. Klare tested the Flesch Reading Ease and Dale-Chall formulas against 16 standardised passages of the Ojemann tests and the 48 passages of the Gray and Leary tests, all developed for adult readers.




After testing several components against benchmark measures of text comprehension, he eventually settled on a weighted average of sentence length and syllables per word.



figure out the components of writing that contributed to readability. Flesch picked twenty-one magazines and divided them into five different levels of reading difficulty: Level A was the easiest and contained *Romantic Story*; level B (*e.g.*, *Redbook*); Level C (*e.g.*, *Reader's Digest*); Level E (*e.g.*, *Fortune*); and Level E (*e.g.*, *The American Scholar* and *The Yale Review*. He then counted the number of times several components of writing which had previously been found to correlate with reading comprehension in children were found in the different texts. He found that sentence length, the number of abstract words and the number of affixes had a high predictive value.
Flesch connected several follow-up analyses using the McCall-Crabbs standardised test questions on reading materials
In response to those concerns, public librarians, sought to recommend reading material of the appropriate grade level to adults. In an effort to help the large numbers of adults who were unable to read adn to provide the needed research on the area of writing readable books, the American Association for Adult Education formed the Readability Laboratory in 1936.
In order to determine a way to produce readable material, Rudolf Flesch and the Laboratory's other researchers evaluated the exist
The earliest measures of readability were simple counts of difficult words.
Unfortunately, formulas using word lists had some definite problems when applied to adults. Most word lists had been developed for children. Adults, on the other hand, read more, had more experiences and just knew too many words. Moreover, even if a perfect and accurate word list exists, it still would not tell us which words were difficult to understand because of how they were used or because they were abstract to begin with.
In response, [#Gray1935;] statistically tested numerous additional components that could be added to word lists to improve readability measures. 
were developed for this purpose. Unfortunately, most of these measures 

Though this is not quite the same as a journal’s peer-review process, it’s not quite a personal Word document uploaded to the internet, either. Because of that, economics working papers are regularly discussed by journalists, academics, and even policymakers before they’re formally published in a journal.
Although this might be an issue if a non-standardised repository of draft papers were used, this is not likely to have a huge confounding effect on papers released as NBER drafts. NBER Working Papers are high profile releases of draft papers before being published in order to encourage discussion and suggestions for revision before publications. However, the long list of acknowledgements on NBER working papers---and, in particular, that overlap with the acknowlegements in the actual paper---suggests that the papers have undergone a thorough form of internal peer review---by friends and colleagues of the authors---before it is released as an NBER working paper.


I worry that we cannot conclusively attribute changes in readability that occur between the NBER working paper and the published paper to the referee process. Some plausible alternative explanations are that male and female authors differ in the following ways: (i) how likely it is that they send the working paper out for comments, (ii) who they send the paper to for comments (might women send the paper to women more often, who perhaps are more likely to
focus on writing quality if they are ex-ante better writers?), and (iii) how they respond to those comments that they do receive.

long before they were released as NBER Working Papers and, combined with the evidence presented in [](#figure7), long before they were submitted to a journal. In additiona, most papers thank several economic congerences and their participants
The other thing critics brought up is that Doleac and Mukherjee’s article is not yet peer reviewed—they plan to submit it to journals soon. But this, too, is an important difference between economics and some other types of health research. Economists tend to put out working papers and circulate them among colleagues long before they submit to journals. For example, Doleac and Mukherjee’s paper thanks several economic conferences and their participants in its acknowledgments. These colleagues—sometimes in pressure-cooker-esque seminars—ask questions and make suggestions, after which the paper is revised, then submitted.

Thus, if the data from *Econometrica* is indicative of all papers, and we assume that people no longer solicit external feedback---*e.g.*, by sending the drafter version out for comments---after submitting to the journal in which it is eventually published, then [](#figure7) is strong evidence that the changes authors---and especially female authors---make to their papers is in response to referee feedback.

One could also include in that designation papers with a female first author. I therefore designate the lead author has he who was listed first in a paper in which authors were listed non-alphabetically or he was listed as the "corresponding author" or "lead author" in the acknowledgements. Unfortunately, however, this added only another 35 (0.38 percent) "female" observations.

Moreover, conducting the analysis in [][Matching] would mean restricting the sample to only those women with at least three 100% female-authored papers; there are only 22, and only an additional 26 women have two. Indeed, 26% of female authors with at least three top publications have always authored with at least one man.

with the introduction of the internet it seems unreasonable to assume that its prior success can be replicated currently. Indeed, if we drop the variables subjected to double-blind review pre-internet and include instead a dummy variable for double-blind review *post* internet, we find similar (positive) effects on gender in both the single- and double-blind samples. This evidence is consistent with the evidence presented in [](#table8a)---*i.e.*, there is less (or at least no more) evidence of gender bias under double-blind review than under single-blind review.

Single-blind review combined with wide dissemination---particularly on the internet---of NBER working papers make it likely that . 
 NBER working papers high profile (and wide dissemination)
at most journals Double-blind review---particularly before the internet---may have curbed that. To deal with the issue, [](#table7) excludes the 279 affected articles. Here, I consider discrimination in the review process through the lens of the shift from double-blind to single-blind review (or the introduction of the internet). 

, I found that the readability gap increased during the period that
papers underwent double-blind review in an analysis that only looked at the readability of published papers (and did not consider the readability of draft versions of those same periods as is done in this analysis). As shown in ??, however, those results are not robust to including time trends.


As can be seen from [](#figure0), *QJE* and, especially, *AER* implemented double-blind review over a period during which both journals experienced there was a rapid expansion in female authors published in top economics journals. Moreover, *AER* and *QJE* tend to be better written than *Econometrica* and *JPE*, according to [](#tableC3). Thus, the significantly larger effect from [#Hengel2015;] was likely due to confounding 

The gender of an author can be usually inferred from authors' given names. In the case of unisex and certain foreign first names, the high profile nature of NBER researchers and their articles probably mean most referees of these papers are aware of the gender of an author.
The one thing that might thwart this assumption is double-blind review, particularly before the internet. Two journals---*AER* and *QJE* employed double-blind review at some point during the time period covered by the data. *QJE* used double-blind procedures until 1 June, 2005. *AER*'s spell began 1 July, 1989 and ended 1 July, 2011.

---*i.e.*, the probability 
there's a 100 percent chance that solo-female-authored papers were written and/or revised by women; there's a 0 percent chance that a solo-male-authored paper was written and/or revised by women; 
the paper's "female intensity"---*i.e.*, it's proportion of female authors.
positively related to the proportion of female authors on a paper.

Yet the proportion of papers with at least one female author is much higher---18.8 percent since 1990. Because the sample sizes for 100 percent female-authored papers was so small, [#Blank1990;] used this classification strategy to define "female" papers.


 two alternative definitions---only female authors (compared to only male authors) and at least one fema

Prior papers indicate that two-thirds of co-authors describe their working relationships as "collegial"---*i.e.*, authors sharing the work as colleagues. Only a quarter of working relationships were described as "mentoring"---a senior author mentors junior authors---or "directing"---one author has primary responsibility and takes the lead in organising the research.
co-authors of the same rank tend to share the task of writing and revising the paper 

(ii) referees' beliefs about the probability papers are written by female co-authors. I assume the probability both are true is a linear function of the ratio of female co-authors.
Nevertheless
That is, if a paper's female ratio is 25 percent, then 25 percent of the time, (i) and (ii) are true. Thus, gender impacts readability in 25 percent of these papers but has no effect in the remaining 75 percent.
In mentor-mentee relationships, the tasks are predominately on the shoulders of mentees. Less likely to contribute to the writing of a paper than they would be if co-authoring with a colleague.
Significant difference between the importance of tasks performed in producing a research paper as a mentor and as a colleague.
Very important, important, less important
In mentor-mentee relationships, more important to contribute to writing and revising the paper; in colleague relationships, less important.
According to the International Committee for Medical Journal Editors, authorship credit should be based only on substantial contributions to all three of the following components: (a) conception and design, or analysis and interpretation of data; and to (b) drafting the article or revising it critically for important intellectual content; and on (c) final revision of the version to be published.
I assume authors are granted authorship credit if and only if they have participated sufficiently in the work to take responsibility for the content.
Unlike the more inclusive measure (at least one female author), female ratio captures the inherent uncertainty from both events: the probability a paper was written by a female co-author is zero or one---and referees' beliefs are presumably correct---when papers are either entirely written by men or women, respectively. Mixed gendered papers, however, are assigned a probability that (i) and (ii) are true based on the proportion of women listed as authors.

25 percent of the time when there is one female author amongst four, 50 percent of the time when half of the authors are female, *etc.* Thus, 
This strategy obviously assumes that 
 about whether a woman wrote the paper; (iii) the probability a female co-author is responsible for making editorial changes to the paper during the peer review process; and (iv) the referee's belief about whether a female co-author is responsible for 
the referee’s perception of the contribution of a woman to a particular paper. To capture the uncertainty of his view when both men and women co-author together, I opted for the less inclusive continuous measure.


We are left with two alternative measures of a paper's gender: consider a paper "female" when at least one author is female or use the ratio of female authors to total authors as a continuous measure of a paper's gender.
Much of the analysis has been duplicated using the more inclusive measure (at least one female author) and least inclusive measure (only female-authored). The results are generally similar to those presented here.
Because the sample sizes for 100 percent female-authored papers was so small, [#Blank1990;] used this classification strategy to define "female" papers.
with only female names on the paper would be compared to papers 
The most straightforward way to deal with the gender of a co-authored paper is gender in these papers is 
In the case of single-authored papers, it is obvious how to code the gender of the paper---but 56 percent of papers are co-authored. A variety of metrics potentially deal with the gender of these papers: *e.g.*, if at least one female name is on the paper, if the primary author is female, or if only female names are on the paper.
The most straightforward way to 
according to the latter strategy would be compared to papers classified as "male. Unfortunately, however, only 4 percent of papers are predominately female-authored; even fewer (3.5 percent) are authored entirely by women. Moreover, when restricting the sample to papers after 1990 or 2000, the figures only slightly improve. Of papers published post-1990, only 5.4 percent are predominantly female-authored; even fewer (4.5 percent) are authored entirely by women. The figures hardly budge when restricting samples to papers published after 2000: 5.7 percent are predominantly female-authored; again, 4.5 percent are authored entirely by women. (See [](#figure0) for a breakdown by journal.)


Thus, higher standards in narrow dimensions that fail to contribute to the observed value of output will lower most measures of female productivity and confound gender differences in labour market outcomes. For similar reasons, common performance controls may discount discrimination in equations that relate wages (and other labour market outcomes) to gender.

For similar reasons, common performance controls may discount discrimination in equations that relate wages (and other labour market outcomes) to gender. Controlling for performance is undeniably import; yet just as important is our judgement and measurement of that performance. Higher standards in narrow dimensions that fail to contribute to the observed value of output will lower most measures of female productivity and confound gender differences in labour market outcomes.

The second panel of [](#tableAX1) displays the marginal effect of female ratio in draft and published papers. The total gap in the published article is relatively stable---between 2--3 Flesch Reading Ease points


Consistent with [](#table7), readability may actually decline during peer review. As discussed in [][NBER], this may be an artifact specific to abstracts, which are edited for length in addition to readability. Alternatively, writing (too) well upfront satisfies the review group with the highest initial readability threshold. Because referee reports reveal $s$ (and therefore $\widetilde R_i^s$), a readability decline after receiving an R&amp;R indicates that a majority of groups have laxer standards. This explanation is consistent with the theoretical model in [][SEUModel].

[](#figureA1) suggests female economists initially underestimate referees' expectations: without experience, their writing improves during peer review; with experience, they write more clearly before peer review. Women's draft readability increases between $t=1$ and $t=2$---and then again between $t=2$ and $t=3$. Consequently, women make fewer changes during peer review in $t=2$ than in $t=1$; changes shrink further by $t=3$.
Women's pattern of behaviour both resembles and differs from men's. Draft and final readability scores for male-authored papers remain relatively constant over increasing $t$. Unlike women's, men's approach does not radically change with experience: they consistently overestimate referee demands pre-peer review to minimise changes made in peer review.
This strategy mirrors women's at later $t$. Economists who anticipate demands are desk rejected less often; economists who don't enjoy more free time, all things equal. [](#figureA1) implies little---if any---gender difference in this tradeoff. Decisions by junior economists may reflect inexperience, but decisions by senior economists should not. Senior economists are familiar with peer review; their choices express optimal tradeoffs with full information (for discussion and justification, see [][SEUModel]). [](#figureA1) suggests both men and women sacrifice time to increase acceptance rates.


Matches are obviously sensitive to the choice and construction of variables in the first panel, the model used to estimate propensity scores and whether matches are made with or without replacement. Outcomes are not. After controlling for $T_i$, decade, journal and *JEL* code, matches using alternative variables (*e.g.*, minimum citation counts and minimum institutional rank) and specifications (logit and no replacement) generate similar figures and identical conclusions to those presented in [][SEUMatching]. (Not shown; analysis available on request.)


Moreover, the extent that higher standards require time investments and induce broader behavioural adjustments that reduce the quantity of female output, they may even produce superficial productivity measures that mask their effect. 
This conclusion suggests that controlling for productivity in estimates of gender earnings and promotion gaps may underestimate labour market discrimination unless those measures adjust for differences in standards.
Moreover, the extent that higher standards require time investments and induce broader behavioural adjustments that reduce the quantity of female output, they may even produce superficial productivity measures that mask their effect. 
Performance is important, 
Given women are prone to adjust behaviour in anticipation of these higher standards, even measures that appear highly objective are likely affected by them; estimates of gender gaps in career outcomes that control for performance may therefore underestimate labour market discrimination.

More broadly, these findings suggest that tougher standards in arbitrary dimensions reduce female productivity. They highlight that using performance measures to understand gender gaps in career outcomes likely underestimates---or even altogether misses---instances of gender discrimination in the labour market. Thus narrowly focusing public policies on increasing female productivity---e.g. by encouraging men to shoulder a larger portion of childcare responsibilities or promoting flexible work schedules---may not be sufficient to achieve gender equality in the workforce.
By meeting discriminatory editorial standards before submission, the readability gap between senior, productive economists could appear independent of peer review in naive estimations.

Earlier in their careers, women know less about referee expectations. [](#figureA1) suggests they *underestimate* those expectations, and, consequently, revise more during peer review. Draft readability at $t=2$ suggests 
make greater revisions during the process. suggesting women adapt behaviour and choices earlier in their career, where the learning curve is steeper. Later in their careers, when they're better informed, women's choices settle into a more stable pattern.

The difference is not significant and the sample of women with 4 or more publications is very small (see [][SEUEmpirical]). Nevertheless, a fall in readability (on average) during peer review is consistent with [](#Theorem1). Writing (too) well upfront satisfies a minority of review groups who discriminate by desk rejecting (relatively) poorly written female-authored papers. Once past desk rejection, however, the review group is "revealed" (recall that the author recognises groups that reviewed his earlier papers even though the identities of the groups' individual members remain unknown to him). In the majority of instances, this reveals that the group is non-discriminatory, so women relax their writing when making revisions. In this case, women's decline in readability between draft and published versions at $t=4\text{--}5$ and $t=6+$ could indicate that only a minority of reviewers discriminate.


implies that referees and/or editors were less inclined to demand readability edits in $t=1$ papers authored by men.
Data for publications three and up further reinforce the idea that women receive a message in peer review---write well---that men do not. 

will be desk rejected less often---and therefore accepted more frequently---than the economist who doesn't.
In the absence of bias---and assuming identical acceptance rates---senior economists would make identical decisions.
Specifically,  by female authors coping with the higher standards editors and/or referees impose on their writing.
Higher standards lead to more changes in peer review[][NBER]. They affect readability before peer review, too[][SEUModel]. 
Since women cope with demands
Having already established that women face greater referee demands than men, I now investigate how women cope with those demands.
 As a final exercise, I investigate gender differences in how this tradeoff is made.
---risking desk rejection---or anticipate demands
can choose to make changes directly in repose to referee demands *during* peer review. Or they can make changes in anticipation of those demands *before* peer review.
If I have a high probability of drawing a review group that desk rejects my paper unless it's well written, and initially laxer review groups are just as demanding during the review phase, 
 has exacting standards and an author had an especially high probability of drawing that review group, it makes more sense to meet its standards prior to peer review.
authors face a As a final exercise, I compare NBER working paper draft readability to the readability of 


The idea that women are told to write more clearly but men aren't is further reinforced at the e publications three and up, women mostly write well upfront; like men, they make very few changes during peer review. Indeed, for publications four and up, their writing gets slightly worse during peer review. Although this difference is not significant and includes only a small number of authors, this fall is consistent with [](#Theorem1). Effectively, these women write *too* well upfront because they anticipate some non-zero probability of drawing a discriminatory reviewer who desk rejects their papers if not well written. Once past desk rejection, the referee group is revealed. If the review group is not discriminatory, women relax their writing when making revisions. Thus, women's decline in readability between draft and published versions at $t=4\text{--}5$ and $t=6+$ could indicate that only a minority of reviewers discriminate.

[](#tableAX1) displays the contemporaneous marginal effect of peer review ($R_{jP}-R_{jW}$) estimated separately for men and women (first panel) and the marginal effect of female ratio in the draft paper and then in the published paper (second panel). Interestingly, without the previous context provided by [](#figureA1), the first panel of [](#tableAX1) would suggest that the readability gap is declining over increasing $t$---



Other fields---most notably social psychology---are more active on this topic[see, *e.g.*,\]\[][#Steele1995]. Economics has occasionally broached this issue, although generally within the context of identity formation[for a review of existing research, see\]\[][#Dee2014].

$e_{nit}^s-e_{nkt}^s$ converges to 0, so for large enough $t$ [](#EquationCorollary1) and/or [](#EquationCorollary2) predict the direction of $D_{ik}$ even when errors remain gender-specific. (See discussions in [][SEUModel] and the next section.)

This paper makes a curious discovery: female-authored articles in top economics journals are better written. After examining the difference, I conclude that higher standards applied by editors and/or referees are primarily to blame.

Moreover, if this effect were due entirely to women continuing to make mistakes about referee expectations at $t=3$, we should expect that a minority of men are also as sensitive as women---and so the mistakes they make are of a similar magnitude. That is, the mistakes made by the men in panel two fo [](#table10X) would be of a similar magnitude to those in panel one---just that they are made by fewer men. Yet they are not---on average, men that are discriminated against write only 20 percent more clearly than they otherwise would. This is a difference of nine percent. It seems highly unlikely that the women listed in [][AppendixMatchingNames]---a higher proportion of whom are editors and/or assistant editors at the journals in question---would persist in making errors of this magnitude relative to the men listed in [][AppendixMatchingNames].

This paper also suggests a novel technical instrument that may be used to overcome bias inherent in measures of subjective evaluation. In the presence of higher standards, subjective quality measures are themselves biased. I use readability formulas to overcome this problem. Readability formulas are transparent, objective and accurate measures of one component on which papers are evaluated during peer review--writing clarity. Similar tests to expose higher quality work may uncover higher standards in a variety of situations: successful business proposals, published "letters to the editor" or annual report introductions. Readability scores could also be employed to produce more accurate measures of labour market wage gaps---by instrumenting for unobserved bias in the subjective productivity measures themselves. Readability scores have their limitations [see][MeasuringReadability] and their use in this manner applies to just a narrow set of questions. Nevertheless, they appear to be a largely ignored, naturally occurring source of pseudo-experiments relevant to research on gender or racial bias---and differential group treatment, more generally.



- Convergence if and only if signal is informative and agent uses signal to update posterior belief. Asymptotic learning fails if the correct action is not chosen (irrationality).
- Smith and Sorenson (2000): when individuals observe all past actions and private beliefs are unbounded, information will be aggregated and the correct action will be chosen asymptotically.
- Bounded beliefs: individuals copy past actions and/or completely ignore their own signals. (Why would men's beliefs be unbounded but women's beliefs bounded?)
- Agents cannot (a) begin acting identically; (b) be subject to the same costs/rewards, (c) process signals in the same way (rationality), (d) yet end up on diverging paths. Even if we relax (a), and assume agents do not act identically---which does not correspond with what I find---assuming (b) and (c) means agents' paths remain a constant distance from one another.
- Because it does not appear that women are rewarded differently for better writing (women's acceptance rates are identical to men's and their review times longer), it must be that they have higher costs of writing poorly (again, because acceptance rates are identical to men's, the cost likely comes in more length peer review). This is equivalent to so-called "higher standards".
- There are two reasonable rewards from writing well (or, conversely, costs from writing poorly). Acceptance rates are higher and/or review times are faster. Compared to men, women do not benefit from either: numerous studies show women are not published at higher rates; my own analysis suggests women's review times are substantially longer. This latter point, especially, indicates female authors are punished more for poorer writing and/or male authors are rewarded more for clearer writing. I find some evidence of both effects. Male-authored papers that are better written do indeed enjoy faster peer review times. Given fewer changes are made to their papers during peer review, this provides a rough (although not causal) indication that better initial writing in male-authored papers shortens the time men spend in peer review. It appears to have much less of an effect for women.
Women are not rewarded for better writing's acceptance rates are identical to men's but their review times are longer. Thus suggests women are not rewarded for better writing; instead, they are punished for poorer writing.
- There is no asymptotic learning in networks with non-expanding observations (observe actions of finite subset of actors).
- When private beliefs are unbounded and the network topology is expanding, then there will be asymptotic learning.
- Note that female readability starts out identical to men's but then diverges.







++++++++++++++++++++++++++++++++++++++++++++++++
With experience, authors' beliefs about the impact of readability on acceptance rates converges to reality, a widening readability gap post experience without higher acceptance rates implies women are held to higher standards in peer review.
implies women's mistake was actually *not* in writing too clearly, but having earlier not written clear enough. When observed acceptance rates are otherwise identical between similar authors, then a widening readability gap can only be attributed to higher standards.
and with experience, authors' beliefs about the impact of readability on acceptance rates converges to reality.
if acceptance rates between men and women are identical

 writing clear enough earlier in their careers
If, with experience, the gap has widened, it must be caused by higher standards
So with experience, the readability gap should go away; if it doesn't, it must be caused by higher standards.
National Bureau of Economic Research (NBER) working papers to their final, published versions; the gap is three times larger for the latter. While both papers are exposed to many factors that impact readability, only published articles are subject to peer review. By comparing the two, influences unrelated to immediate peer review are isolated from those that are; assuming the former are not correlated with the latter's timing, a widening gap suggests a causal link---and therefore rules out some constant difference between the sexes in how well they write (holding all else equal), it would arise only during the drafting phase---that is, the time before papers are submitted to peer review. I find that this is not the case.
Second, women do not derive any apparent benefit to writing well. There are two reasonable rewards from writing well (or, conversely, costs from writing poorly). Acceptance rates are higher and/or review times are faster. Compared to men, women do not benefit from either: prior research does not suggest that journal acceptance rates are higher for women [see, *e.g.*,\]\[][#Abrevaya2012,Blank1991,Borsuk2009,Gilbert1994,Lloyd1990] nor do women enjoy speedier review times. In fact, my own analysis suggests women's review times are, on average ***six months longer***. This estimate is based on submit-accept times at *Econometrica*, is consistent across a range of specifications and, in addition to other factors, controls for motherhood and giving birth.
Third, women's writing gradually gets better but men's does not. Between authors' first and third published articles, the gap grows by 12 percent. Evidence does not suggest senior female economists co-author with more women. Nor are initially bad female writers leaving academia. Instead, women appear to get better at writing as their careers progress, whereas men donot . Given that we've already established a causal link between the readability gap and peer review, then this indicates women are receiving a form of positive reinforcement than men are not receiving. That is, men and women are receiving separate signals in peer review.
Combined with point two, this suggests gender differences in (i) biology/behaviour that manifests itself in peer review---for example, women are more sensitive to referee criticism---as well as (ii) knowledge about the advantages of writing well cannot resolve the gap. In a simple model of Bayesian updating, I show Agents cannot (a) be subject to the same costs/rewards, (b) be rational (i.e., both learn rationally and start off with the same belief sets---either both have bounded beliefs or both have unbounded beliefs) and (c) end up on paths that diverge over time. 
Thus, if we assume women are no more or less likely to systematically misinterpret signals, then observing (c) means (a) must be violated. That is, women and men are subject to different costs and rewards of writing well---i.e., they are subject to different standards. Conversely, insisting (a) is true implies women act irrationally.
Instead, women---and only women---seem to figure out that writing well makes peer review smoother; they write subsequent papers clearer from the start. 

Higher standards impose quantity vs. quality tradeoffs that characterise many instances of female output. 
undoubtedly hurting women’s productivity and probably, as a consequence, promotion rates, too.
 explain academia “Publishing Paradox”: 
Higher standards impose quantity vs. quality tradeoffs that may may explain lagging female wages and productivity, more generally.

Peer review is not immune. Using five reliable measures of writing clarity, I show that female-authored articles published in top economics journals are better written than equivalent papers by men.
Academia's publishing paradox. Justifies lower pay and promotion rates. May account for small fraction of women among assistant, associate and full professors.
In this paper, I explore a possible explanation for this: women exert more effort writing their papers more clearly, and in general spend longer in peer review.
Using five reliable formulas to exploit this, I analyse 9,123 article abstracts published in the *American Economic Review* (*AER*), *Econometrica* (*ECA*), *Journal of Political Economy* (*JPE*) and *Quarterly Journal of Economics* (*QJE*).
I find female-authored abstracts are 1--6 percent more readable than those by men. Women write better despite controls for editor, journal, year and primary and tertiary *JEL* classification; that remains unchanged when proxying for article and author quality. This means the readability gap probably wasn't (i) a response to specific policies in earlier eras; (ii) caused by women writing on topics that are easier to explain; nor (iii) generated by factors correlated with gender but really related to knowledge, intelligence and creativity.
Combined with point two, this suggests gender differences in (i) biology/behaviour that manifests itself in peer review---for example, women are more sensitive to referee criticism---as well as (ii) knowledge about the advantages of writing well cannot resolve the gap. According to both interpretations, readability essentially diverges because women voluntarily accept the costs of good writing (longer drafting and revision stages) despite receiving no tangible benefit (acceptance rates do not change). Initially, this explanation is perfectly plausible; indefinitely, it is not---unless we assume senior female economists (and only senior female economists) are basically acting irrationally.
Instead, they increase. Between authors' first and third published articles, the gap grows by 12 percent. Evidence does not suggest senior female economists co-author with more women. Nor are initially bad female writers leaving academia. Instead, women---and only women---seem to figure out that writing well makes peer review smoother; they write subsequent papers clearer from the start.


Clearer sentences, less jargon and more scrutiny aren’t bad things. Added attention reduces discriminatory outcomes[#Bartos2016]. It doubtless leads to a better final product: papers that are easier to understand enjoy wider, more diverse readership; closer review catches logical mistakes and leads to fewer factual errors.
Still, adding robustness checks, clarifying proofs and making sentences even marginally more readable takes time. [][AppendixModel] illustrates this idea using [#Bartos2016;]’s formative model linking discrimination and attention. The major change is a simple extension that assumes referees pass on to authors some of the cost of evaluating their papers. This prolongs peer review directly---referees spend more time evaluating women’s papers and women spend more time responding---and indirectly---female authors take longer drafting future papers. 
I estimate the direct effect results in female-authored papers spending ***six months longer*** in peer review. This estimate is based on submit-accept times at *Econometrica*, is consistent across a range of specifications and, in addition to other factors, controls for motherhood and giving birth. It appears to dominate in papers written by early career academics. In fact, there is very little gender difference in readability in the draft version of an authors' first publication---although a large difference does emerge by the time those same articles are actually published. The indirect effect---i.e., women spending more time upfront drafting their papers---increasingly dominates as women gain experience in peer review.
In [][xxxx], I look at the indirect effect. I find the direct effect dominates among early career academics; the indirect effect dominates later in their careers. There is very little gender difference in readability in the draft version of authors' first publication---although a significant difference does emerge by the time those articles are eventually published. Initial drafts of women's second publications, however, are already much better written than men's---evidence of an indirect effect---although the still further widens during peer review.



*******************************************

Additionally, this readability gap widens precisely while papers undergo peer review.
Because better writing takes effort to compose, it likely prolongs female review time---indeed, I find female female-authored papers spend six months longer in peer review---and probably contributes to lower publishing rates.
I explore many interpretations---*e.g.*, Are women more sensitive to criticism? Is better writing due to risk aversion? Do women's papers deserve more scrutiny because they aren't as good? The only straightforward explanation consistent with the data, however, is that referees apply higher standards to women’s writing. Because better writing takes effort to compose, higher standards prolong female review time---by six months at *Econometrica*---and likely contribute to lower publishing rates. Tougher standards applied more broadly reduce women’s output; ignoring them undervalues female labour and may confound estimates of gender discrimination.
PARAGRAPH ON SPENDING 6 MONTHS LONGER IN PEER REVIEW
Two possible explanations can account for this gap: either referees subject female-authored papers to added scrutiny, or women choose themselves to exert more effort. Both possibilities imply women spend more time than men on a given task. On the positive side, it increases quality; on the other hand, it reduces quantity. Either could explain academia's "Publishing Paradox". Unequal time spent making revisions leads to unequal time conducting new research; as a result, women write fewer papers.
Nevertheless, I find the more plausible explanation is indeed because referees subject female-authored papers to greater scrutiny. First, women's writing gradually gets better but men's does not---meaning gender differences in (i) biology/behaviour and/or (ii) knowledge about the advantages of writing well cannot resolve the gap. According to both interpretations, readability essentially diverges because women voluntarily accept the costs of good writing (longer drafting and revision stages) despite receiving no tangible benefit (acceptance rates do not change). Initially, this explanation is perfectly plausible; indefinitely, it is not: wouldn't individual women eventually figure out there's nothing to gain by writing well---and then do it less? If so, gender differences in readability should decline as authors gain experience in peer review.
Instead, they increase. Between authors' first and third published articles, the gap grows by 12 percent. Evidence does not suggest senior female economists co-author with more women. Nor are initially bad female writers leaving academia. Instead, women---and only women---seem to figure out that writing well makes peer review smoother; they write subsequent papers clearer from the start.
Points one to three provide strong evidence that peer review is at least partially responsible for better writing in female-authored papers. But is it given that female-authored papers invite undue scrutiny? In [][alternatives], I explore several alternative hypotheses---many of which have nothing to do with bias and some that exonerate peer review, too. Among them: Are female economists disproportionately native English speakers? Are female referees the toughest critics, and, if so, are they more likely to review female-authored papers? Do manuscripts written by women deserve more criticism because they aren’t as good?
As [][Alternatives] also illustrates, however, the only straightforward explanation consistent with the data is that referees are disproportionately critical of female-authored papers. Clearer sentences, less jargon and more scrutiny aren’t bad things. Added attention reduces discriminatory outcomes[#Bartos2016]. It doubtless leads to a better final product: papers that are easier to understand enjoy wider, more diverse readership; closer review catches logical mistakes and leads to fewer factual errors.


Still, adding robustness checks, clarifying proofs and making sentences even marginally more readable takes time. [][AppendixModel] illustrates this idea using [#Bartos2016;]’s formative model linking discrimination and attention. The major change is a simple extension that assumes referees pass on to authors some of the cost of evaluating their papers. This prolongs peer review directly---referees spend more time evaluating women’s papers and women spend more time responding---and indirectly---female authors take longer drafting future papers. 
I estimate the direct effect results in female-authored papers spending ***six months longer*** in peer review. This estimate is based on submit-accept times at *Econometrica*, is consistent across a range of specifications and, in addition to other factors, controls for motherhood and giving birth. It appears to dominate in papers written by early career academics. In fact, there is very little gender difference in readability in the draft version of an authors' first publication---although a large difference does emerge by the time those same articles are actually published. The indirect effect---i.e., women spending more time upfront drafting their papers---increasingly dominates as women gain experience in peer review.
In [][xxxx], I look at the indirect effect. I find the direct effect dominates among early career academics; the indirect effect dominates later in their careers. There is very little gender difference in readability in the draft version of authors' first publication---although a significant difference does emerge by the time those articles are eventually published. Initial drafts of women's second publications, however, are already much better written than men's---evidence of an indirect effect---although the still further widens during peer review.
Spending six more months in peer review may explain academia's "Publishing Paradox". Unequal time spent making revisions leads to unequal time conducting new research; as a result, women write fewer papers. 
undoubtedly hurting women’s productivity and probably, as a consequence, promotion rates, too.
 explain academia “Publishing Paradox”: 
Higher standards impose quantity vs. quality tradeoffs that may may explain lagging female wages and productivity, more generally. Work that is evaluated more critically *at any point in the production process* will be systematically better (holding prices fixed) or systematically cheaper (holding quality fixed). This reduces women's wages---for example, if judges require better writing in female-authored briefs, female attorneys must charge lower fees and/or under-report hours to compete with men---and distorts measurements of female productivity---billable hours and client revenue decline; female lawyers appear less productive than they truly are.

Yet higher standards impose a quantity vs. quality tradeoff that may contribute to women's lower wages and promotion rates. Work that is evaluated more critically at any point in the production process will be systematically better (holding prices fixed) or systematically cheaper (holding quality fixed). This reduces women's wages and distorts their promotion rates. Because discrimination affects behaviour and choices, it necessarily impacts how accurately we measure their productivity. 


Stereotypes form for a variety of reasons, and discrimination exists in a plethora of forms---all of which presumably affect how victims behave. Their 

It suggests that some "feminine" traits may be more context specific than previously thought. 

This quantity vs. quality tradeoff characterises many instances of female output. According to raw numerical counts, women produce less than men. Female academics publish fewer academic journals[#Ceci2014]; female physicians see fewer patients[#Bloor2008] and submit fewer grant proposals[#Gordon2009,Klos2013]; female reporters write fewer front-page bylines[#Crozier1999]; female novelists produce less nonfiction output[#Waisbren2008]; female real estate agents list fewer homes[#Trulia2011].
When ranked by narrowly defined outcome measures, however, women generally outperform men. Female students earn better grades[#Voyer2014]; female auditors are more accurate and efficient[#Chung2001,ODonnell2001,Ittonen2013,Niskanen2011]; congresswomen secure more federal funding for their districts, sponsor more legislation and score higher on a composite measure of legislative effectiveness[#Anzia2011,Volden2013]; houses listed by female real estate agents sell for higher prices[#Seagraves2013,Salter2012]; patients treated by female physicians are less likely to die or be readmitted to hospital[#Tsugawa2016]; female pilots are involved in fewer fatal accidents[#Vail1986,Bazargan2011].
To the extent that higher standards require time investments and induce similar behavioural adjustments that reduce the quantity of female output, they may even produce superficial productivity measures that mask their effect. 

women appear to have figured out how well they need to write---and they are doing all the hard work of writing well upfront. This suggests female economists gradually anticipate higher standards *in* peer review---and consequently write future papers more clearly *before* peer review.
There is no gender difference in readability in the draft version of authors' first publication---although a significant difference has emerged by the time those articles are eventually published. Initial drafts of women's second publication, however, are already better written than men's---although the still further widens during peer review.  
how stereotypes form or 
the source of discrimination---taste-based, attention, pollution---
Indeed, many theories could explain *why* women are discriminated against in peer review---taste-based, attention, pollution, or any number of theories on how stereotypes form. The fact that it occurs, however, has important implications on how women behave. 

Its findings confirm similar conclusions in research on employee performance reviews, teaching evaluations and online comments---women receive more abusive feedback, less credit for intelligence and creativity and are expected to be more organised, prepared and clear[#Boring,2015,Correll2016,Gardiner2016)

, thereby masking the effect of discrimination. 
Indeed, irrespective of the theory behind *why* referees and/or editors discriminate against women---and there may bewomen are held to higher standards
Moreover, I find evidence that when facing discrimination, 

I find that women not only write more clearly when expected to do so---but also the way they do this can actually make these contortions resemble voluntary choice. This suggests women may modify their behaviour in ways that superficially mask the effect of discrimination by making their contortions resemble voluntary choice. 

Many of these differences are undoubtedly influenced by innate gender differences in biology and behaviour. Irrespective of their origin, however, most have intrinsic value that, under normal circumstances should be appropriately rewarded in an unbiased marketplace. Property listed by female real estate agents sell for higher prices; in the absence of higher standards, women would charge more to sell a house. They do not[#Trulia2011]. Patients treated by female physicians are less likely to die or be readmitted to hospital, in the absence of higher standards, visiting a female doctor would cost more. It does not[#XXXXXX]. Congresswomen are better legislators; in the absence of higher standards, they would be elected at higher rates. They are not[#XXXX]. Female-authored manuscripts are better written; in the absence of higher standards, women's papers would be published in academic journals at higher rates. They are not[#Ceci2014].

Or, basically, women don't expect to be treated any differently than men in peer review. Unsurprisingly, then, for the first paper, the gap in readability emerges entirely during peer review. But women are obviously learning that that their papers need to be more readable than they originally expected. So next time around, they write more clearly from the outset. 
directly---referees spend more time evaluating women’s papers and women spend more time responding---and indirectly---female authors take longer drafting future papers. Both factors prolong peer review
The pre- and post-review analysis using NBER working papers provides evidence of the first effect. I find further evidence of the direct effect in review times at *Econometrica*, where 


But assuming authors do not *want* to be poorly informed or oversensitive, their impact on choices declines with experience. the impact of this mistake declines with experience
 these are probably not states or activities the 
with experience, those mistakes are corrected
. Holding acceptance rates constant, this implies that a widening readability gap between authors with equivalent e
women write more readably (and men possibly less so)
Thus, if women write more readably (and men even less so) after enough time has passed, 
authors who increase readability are motivated entirely by higher acceptance rates
women may miscalculate referee expectations and misconstrue their reports, but with experience they correct these mistakes. 
If an author has previously written a less readable paper and if she could obtain an identical expected acceptance rate at that same readability, she would.
Since increasing readability is motivated by an author's belief that it increases acceptance rates,
 change in readability can only be motivated by an author's belief that it increases acceptance rates. Together these factors imply that if the gap is actually wider after enough time has passed for beliefs to converge---but acceptance rate's aren't---then women because they have to. They'd be rejected, otherwise.

Earlier, we measured the direct effect, which is basically the change in readability that occurs during peer review. This is women expending effort addressing direct referee concerns during peer review. There is also an indirect effect, however, in that 
 I find the direct effect dominates among early career academics; the indirect effect dominates later in their careers. 
It appears to dominate in papers written by early career academics. In fact, there is very little gender difference in readability in the draft version of an authors' first publication---although a large difference does emerge by the time those same articles are actually published. The indirect effect---i.e., women spending more time upfront drafting their papers---increasingly dominates as women gain experience in peer review.
---and indirectly---female authors take longer drafting future papers. 
Moreover, I find evidence in the indirect effect in a supplemental pre- and post-review analysis using NBER working papers.

The data on draft papers and final papers allows us to identify these two effects over time. We see that men and women draft their papers with identical readability at the beginning of their careers. This indicates that they initially anticipate the same writing standards and they write their papers accordingly.


Unequal time spent making revisions leads to unequal time conducting new research; as a result, women write fewer papers. 
Moreover, they that I argue characterise many instances of female output---and may therefore explain lagging female productivity, wages and promotion rates, more generally.

The robustness of these conclusions is obviously sensitive to the assumptions I have made. The two primary identifying assumptions are (i) $t=3$ is large enough to eliminate gender differences in expectations and (ii) $i$ and $k$ are truly equivalent. But the accuracy of these conclusions also depends on how I have modelled authors' utility. Specifically, authors probably care about getting their papers accepted and they may care about writing well, but their marginal utility from the intersection of the two events---*i.e.*, higher utility from writing well *only* because the paper is published in a top-four journal (as opposed to field journals or second-tier general interest journals)---is assumed to be negligible. For this reason, the impact of author $i$'s personal utility from writing well is reflected only in the first stage when choosing $r_{0it}$; it is otherwise irrelevant for the second phase, when choosing $r_{1it}$. If, on the other hand, women (and not men) *only* care about readability when their papers are published in the very top journals---as opposed to field journals or second-tier general interest journals---*and* wrote initially more poorly because (i) conditional on receiving an R&amp;R, they overestimated $\widetilde R_{it}$ *but* because of risk aversion simply refused to make the changes---and were pleasantly surprised when they were accepted anyway. But then the next time they are assigned $s$, they know $\widetilde R_{it}$. And this is below some threshold; and they enjoy


Women are more risk averse[see, *e.g.*,\]\[][#Borghans2009]---could that play a role? Consider revision rounds as lotteries. An author’s labour reveals his willingness to pay to reduce risk, so a positive link between risk aversion and better writing implies risk averse individuals exert more effort (on average) to reduce their odds of being rejected. Under these conditions, risk aversion leads to higher acceptance rates, all else equal.
So if female authors truly are more risk averse than men, their papers would---and, indeed *should*---be disproportionately represented in academic journals. This is not the case. To the best of my knowledge, publication outcomes expose no female advantage, anywhere, ever[for an overview of the literature, see, *e.g.*\]\[][#Ceci2014]. Meaning risk aversion either doesn't play a role, or conceals acceptance rates that unfairly favour men.

In [][Duration], I determine the cost of more scrutiny---at least at *Econometrica*---female-authored articles take substantially longer to complete peer review.

asymmetry from one upsets symmetric criteria applied everywhere else.

It also suggests that experience teaches women to write more readably---and men less so---thus creating or exacerbating the gender readability gap.

Assuming women do not rationally waste time writing so well---and their earlier, more poorly written papers suggest they're happy enough writing less clearly---[](#Theorem1) suggests that external forces beyond women's control is shaping decisions to write so well. To confirm this, however, requires looking at matched pairs for whom all three conditions are satisfied [simultaneously](#table10X).

 don't have to. The payoffs to writing well do not appear to be *symmetrically* high. Although women write clearer papers, they aren't published more than men---ruling out the last justification for why women exert the time women spend writing clearer papers (Condition 1, [](#Theorem1). 

As discussed in the previous section, every article examined in this paper was published in top economics journals, precluding gender analysis of acceptance rates. Nevertheless, u

Although I cannot directly determine acceptance rates with my data, mean $T$ (lifetime number of publications) do not suggest women are actually *rewarded* for their effort: on average, the women in the sample published 4.53 lar; men had 5.61. Coupled with the discussion in the previous section, thus suggests female-authored papers are accepted no more often than male-authored papers. So [](#table9X) again supports [](#Theorem1)'s conclusion that women receive harsher scrutiny or are allocated harder critics in peer review.

Men's readability scores decline as they publish more papers. For women, however, each additional paper is more readable than her last. Effects are statistically significant for four out of five scores.

First: at time $t$, $i$ and $k$ are equally accurate about referees' evaluation criteria. Second: If $s$ accepts $i$'s paper, it also accepts $k$'s paper.

First authors are those identified in the acknowledgements or listed first when authors are not ordered alphabetically. See [][Data] for more details.

Matches were made using time-invariant factors or variables averaged over $t$. The thresholds referees apply to papers may, however, depend on journal, publication year, co-author characteristics and the signal authors send with their institutional affiliation---not to mention the ratio of female authors. All of these factors vary over $t$. To account for these factors, I estimate authors' readability scores at fixed values of these [variables](#EquationMatching1).
&lt;!--\input{$PPATH/equations/EquationMatching1.tex}--&gt;
 where $\vect X_{it}$ $i$'s time $t$ number of co-authors, institutional rank, institutional rank of his highest ranked co-author, year and a dummy variable for each journal.
I separately estimated [](#EquationMatching1) on female and male authors at $t=1$ and $t=3$. 



Consider a matched pair for which Conditions 2 and 3 are satisfied, suggesting discrimination against $i$. The raw estimate of final gender difference in readability is $R_{i3}-R_{k3}$. This reflects actual discrimination only if author $i$ would otherwise voluntarily choose readability $R_{k3}$ if the probability of acceptance at that readability were identical to her probability of acceptance at readability $R_{i3}$. If $R_{k3}&gt;R_{i1}$, then this is true: $R_{i1}$ forms an upper bound on the choice of readability author $i$ would make in the absence of peer review---*i.e.*, left to her own devices, we know author $i$ would choose some $R\le R_{i1}$. If $R_{k3}&lt;R_{i1}$, however, then we do not know if $i$ would otherwise voluntarily choose readability $R_{k3}$ if the probability of acceptance at that readability were identical to her probability of acceptance at readability $R_{i3}$---we only know that if that statement were true, she would choose no higher readability than $R_{i1}$.
The purpose of Condition 2 is subtle. It rules out the possibility that women just intrinsically prefer writing more clearly than men or enjoy a lower cost of writing. Yet any such intrinsic preference or cost explanation defines the minimum readability at which a women is willing to write. Thus, *all* of her papers, including the most poorly written one, is at least this readability level. Because women will always choose a readability level that satisfies this minimum threshold dictated by their preference, any *increase* in readability must be driven by something else---*i.e.*, either their own mis-information or sensitivity or by factors outside their control.

It is also possible to generate a conservative measure of the average effect of discrimination over all matched pairs in which one member satisfies Conditions 2 and 3---and thus, determine whether the average effect leans against men or women. This is the average of the following figure: $$D_i=\bm1_i\big(R_{i3}-\max\left\{R_{i1},R_{k3}\right\}\big),$$ where $\bm1_i$ equals 1 if the author is female and -1 if the author is male.

For each author, reconstructed readability scores differ only by experience: $$R_{i3}-R_{i1}=-\mu_{i1} + \Delta\,\varepsilon_{i1}.$$ Between matched pairs, they differ on female ratio *and* preferences:  $$R_{i3}-R_{k3}=\beta_1+\Delta\,\alpha_{ik} + \Delta\,\varepsilon_{ik3}.$$ Assume both conditions are satisfied. But if $R_{i3}&gt;R_{i1}$, then $i$ $\alpha_i&lt;\alpha_k$, and $R_{i3}-R_{k3}$ underestimates $\beta_1$. If $R_{i1}&gt;R_{i3}$ then $\mu_{i1}&lt;0$. If at $t=3$ authors $i$ and $k$ enjoy identical acceptance rates at $R_{i3}$ and $R_{k3}$ and there were no bias, then they'd also enjoy identical acceptance rates at $R_{i1}&lt;R_{i3}$. Given his prior decision history, $i$ would prefer to set readability lower than $R_{i3}$. The only reason he *wouldn't* is because he *can't*---not without suffer a lower probability of acceptance. Thus, $-\mu_{i1}&gt;0$ is a conservative estimate of $\beta_1$.
&lt;!--\input{$PPATH/tables/tex/table11NEW}--&gt;


[](#Theorem1) does not require that authors are perfectly accurate at $t=3$ nor that by that point, they are (on average) unbiased about $\widetilde r_{0it}$ and $\widetilde R_{it}$. It requires only that the bias is decreasing over time for all $t$ past $t=3$. [](#Corollary1), however, does require $e_{nit}\le e_{nkt}$ to ensure [](#EquationCorollary1) and [](#EquationCorollary2) do not overestimate $D_{ik}$---although they will still consistent predict the *direction* of discrimination for large enough $t$. (See [](#footnote76).)


The second assumption in [](#Proposition1) is that the states in which $i$ is accepted is a subset of the states in which $k$ is accepted. This is stronger than---and indeed, supplants---Condition 1 in [](#Theorem1). As shown in Appendix X, however, if we relax this assumption and assume only that the weaker Condition 1 is true, then, assuming $i$ is the one held to higher standards, 

It reflects tasted-based discrimination, either by review group $s$ in stage 1 *or* the review group $\overline s\in\Sigma_{A_i}$ with the strictest stage 0 standards.

$\delta_{ik}=\widetilde R_i^s-\widetilde R_k^s$ measures differing evaluation criteria applied to different authors: review group $s$ subjects author $i$ ($k$) to higher readability standards when $\delta_{ik}&gt;0$ ($\delta_{ik}&lt;0$).
When $i$ and $k$ are arbitrary authors, a positive $\delta_{ik}$ could reflect legitimate evaluation criteria related to the topic, novelty and quality of $i$'s work.
If $i$ and $k$ are arbitrary authors,  applied to their work. Between equivalent authors, $\delta_{ik}$ measures discrimination: against $i$ when $\delta_{ik}$ is positive; against $k$ when $\delta_{ik}$ is negative.


is the difference in standards applied to $i$'s papers compared to $k$'s. When $i$ and $k$ are equivalent authors, then a positive $\delta_{ik}$ reflects discrimination against $i$; a negative $\delta_{ik}$ suggests discrimination against $k$.

$\widetilde R_i^s$ depends on the topic, novelty and quality of papers written by author $i$. Assuming $i$ and $k$ write equivalent papers,
&lt;!--\input{$PPATH/equations/equationMatching4.tex}--&gt;
where $\delta_{ik}$ is the the degree to which referees and/or editors apply higher standards to $i$ or $k$ that have nothing to do with the topic, novelty or quality of the papers written by either author. A positve $\delta_{ik}$ indicates discrimination against $i$; a negative $\detla_{ik}$ means $k$ is discriminated against.


&lt;!--\input{$PPATH/equations/equationMatching3.tex}--&gt;
and $R_i^\star$ is the $R$ that solves $\phi_i'(R)=c_i'(R)$---\textit{i.e.}, the readability level $i$ would choose in the absence of earning any utility from acceptance; as shown in the proof of~\autoref{Theorem1}, it forms a lower bound on $r_{0it}$ and $R_{it}$ for all $t$. 

Testing [](#Theorem1) is simply a matter of testing each condition individually on equivalent pairs. Discrimination exists whenever Conditions 1--3 are satisfied. If the number of women who satisfy all three conditions is significantly larger than the number of men, then women are discriminated against more when compared to equivalent men.

These results strongly suggest a causal interpretation of peer review's impact on the gender readability gap. According to all five scores, the readability of female-authored papers grew relative to male-authored papers precisely when these papers were undergoing peer review; the difference is highly significant for two scores and weakly significant for a third. When explicitly controlling for papers' draft readability, the readability gap persists and is highly significant for all five scores. Although it is premature to specify the exact mechanism that induces the gap's formation, these results nevertheless provide strong evidence that the mechanism emerges during peer review.

Determining whether authors are native English speakers would be time-consuming, yet feasible; however, it is not clear how or even if native speakers write clearer than those that are non-native but highly fluent---and most of the authors in the data would fall in one of these two categories. Nevertheless, fluency is presumably related to clarity. Unfortunately, I do not know how to determine author fluency at the time articles were submitted to journals without access to a time machine, a lot of money and the willingness of 7,241 economists to sit a TOEFL exam. But I'm open to suggestions.

Extra scrutiny leads to better papers but isn't without cost. Revisions are expensive, time consuming and and disrupt other research[for a discussion, see\]\[][#Gans1994]. 

 [#Gans1994;] also point out that styles change: George Akerlof found it difficult to publish his 1970 paper "The Market for 'Lemons'" due to its "readable" style because "mathematical rigour" was then in fashion. Nevertheless, despite sometimes preferring mathematics, journals have probably always frowned on any unjustifiably complicated prose that is present.

Of course, these estimates establish only correlations; causality requires referee reports---which I do not have. Nevertheless, but by process of elimination, I show that the most straightforward (and plausible) interpretation is that female-authored papers *are* subject to more scrutiny in that 
Working paper readability is unsurprisingly highly correlated with a published article's readability, but explicitly controlling for it has little impact on gender differences---women's papers remain 2--4 percent better written.

Although gender has no apparent impact on journal acceptance rates, my paper suggests it nevertheless has an important impact on referee feedback. 

While the question of gender differences in peer review feedback may be new, the general question of whether women receive harsher feedback is not. The findings in this paper confirm conclusions from recent research on employee performance reviews and teaching evaluations.
The contribution of this paper is threefold. Its primary contribution is to shed light on the peer review *process*. To the best of my knowledge, my study is the first not only to suggest bias could exist in the process despite its absence in the outcome, but also to document such evidence.

First study of process. to the best of my knowledge, it is the first study to document evidence of gender bias in the peer review process. As mentioned earlier, gender has no apparent impact on journal acceptance rates. But a gender neutral outcome does not preclude a gender neutral *process*---and to the best of my knowledge, my paper is the first to suggest it. Determining whether female economists are unfairly scrutinised in referee reports suffers from several identification issues, chief among them the fact that accessing those reports is impossible thanks to the privacy ensconced in peer review. To circumvent this limitation, my paper notes that *if* women are subject to greater scrutiny, and that added scrutiny.

I concentrate on the third hypothesis: do reviewers require clearer, more concise writing from women than they do men? For, if referees hold female- and male-authored papers to identical standards, the latter should be just as coherently written as the former. My identification strategy is effectively a process of elimination. I first establish that the readability gap exists. I next establish a causal link between the timing of peer review and formation (or widening) of the gap. Finally, I document numerous stylised facts about the gap that a coherent theory would need to account for.

Given the well-documented human tendency to discount female competency, it should certainly not be used as proof that the wider system is completely fair.
Given the well documented human tendency to discount female competency, it is reassuring to know that at least journal acceptance rates are gender neutral. But as this paper illustrates, a gender neutral outcome does not preclude bias in the process---and it should not be used as a definitive proof that the wider system is completely fair. Instead, what it highlights is a neutrality precisely where neutrality is easy to measure but non-neutrality where it isn't. Other research has illustrated that neutrality is quickly and easily achieved when measurement is easy and monitoring is frequent. The findings in this paper support those conclusions---and suggests that to really root out bias, we need to improve monitoring and transparency in the peer review process, possibly by adopting open review, as discussed in [][p3openreview].

I believe it is the first to identify group differences using readability scores. As discussed in [][p3measuring_readability], these scores are highly correlated with reading comprehension, cheaper than audit studies and randomised controlled trials and arguably more objective than survey data. Using readability in this context obviously applies to a narrow set of research questions and cannot replace these traditional methods; they nevertheless reflect a complementary research tool that untaps a largely ignored, naturally occurring source of pseudo-experiments. A similar approach may (or may not) uncover similar group differences in business proposals, letters to the editor printed in newspapers, student essays, etc.

Only 13 papers were written by women with young children, nine of whom gave birth during peer review

Six more months finding male co-authors---at least in the individual (and therefore anecdotal) recent experience of two women instructed by their referee to "find one or two male biologists to work with (or at least obtain internal peer review from, but better yet as active co-authors)" to prevent the paper from "drifting too far away from empirical evidence into ideologically biased assumptions"[#Bernstein2015].

Unbiased decisions may be inversely related to the decision maker's belief in his own neutrality.
In several surveys female academics report bias from their superiors, peers and students[see, *e.g.*,\]\[][#Williams2015]. 
In fact, there is no significant gender difference in abstract readability for authors' first publication in the data---papers initially composed without prior exposure to top journals' refereeing process. This finding further supports a causal link to peer review. Since readability scores are more gender neutral for first time publications, the gap is unlikely fully explained by either the residual impact of stricter writing standards for women at an earlier stage in their education or inherent gender differences in writing clarity. For, if either were true, a significant persistent gender difference should be present regardless of previous publication counts---and especially in the one with the largest sub-sample.

&lt;!--\label{fn3}--&gt;Applying [](#p3equation1) to several increasing, convex transformations of the female ratio and restricting the sample to only male authors supports this conclusion: coefficients on the transformed variables are generally highly significant and as large (or larger) than the equivalent effect for women (see Online Appendix).


Given the average number of words per sentence decreases during peer review but as a fraction of total word count, syllables, polysyllabic words and difficult words rise, peer review has an uncertain impact on four out of five readability scores. Here I find a pos
Other assessments of peer review's impact on papers' readability scores have found a positive impact; for the Flesch Reading Ease and Dale-Chall scores, [](#p3table7) suggests a negative effect---at least for men. The Flesch-Kincaid, Gunning Fog and SMOG scores are positive. 
For women, however, a combination of their larger fall in words per sentence coupled with greater cuts in hard words relative to hard words means during peer review, readability scores universally increase for female-authored papers; that increase is significant for the majority of scores.
Because women cut more hard words than men, readability scores for female-authored papers universally increase, and that increase is 


significant for the majority of scores.

---and again the drop is steeper for women. The hard word count to total word count ratio actually increases: total abstract word count falls more than hard word counts do, *i.e.*, simpler words are more likely to be cut than are hard words. Because women delete proportionally more 
Because total abstract word count falls more than hard word counts do, however, the hard word count to total word count ratio increases.
---but actually increases hard word count to total word count. This occurs because total abstract word count falls more than hard word counts do.
and may reflect the fact that abstracts are edited also for length.
Peer review unambiguously shortens sentences---although again, the drop is steeper for women. For both men and women, however, it may actually increase the three estimates of "hard" words to total words in panel two. This likely occurs because abstracts are edited not just for readability, but also for length. Because the fall in total word count is greater than the fall in the number of hard words, these ratios increase during peer review. Because women are still deleting a larger proportion of difficult words relative to men, 
although, again the drop is steeper for women.
Papers written by women, however, undergo the most changes. Women's sentence, character and word counts fall by 16--18 percent more than do men's; syllables, polysyllabic and difficult words fall by 6--16 percent more.
The fact that paper's don't universally improve is due to 
Abstracts may be extensively edited during peer review, but readability scores adjust by somewhat less than the textual components used to calculate them. This likely reflects the particular nature of abstracts, which are edited not only for readability, but also for length. When the fall in 
The fall in syllables and so-called "difficult" words is proportionately less than the fall in word count. Because four of the five scores are functions of "difficult" words per total word count, the aggregate effect is ambiguous. According to [](#p3figure2), poorly written draft papers emerge from peer review with higher readability scores (above the 45 degree line); abstracts that were already well written may come out with slightly lower scores (below the 45 degree line). Nevertheless, [](#p3figure2) also highlights that female-authored published papers are consistently more readable than they were as working papers relative to male-authored papers.

Female academics are less likely to make tenure, take longer when they do and earn much less than their male peers[#Bandiera2016,Ceci2014,Ginther2004,Sarsons2015,Weisshaar2014]. One culprit is productivity. 

More generally, this study augments recent research linking gender pay and promotion differentials to productivity by showing men are held to lower evaluation standards. As expected, women adjust to this favouritism by raising the quality of their output, thereby increasing the time required to complete a task. If the marginal cost of this adjustment exceeds its benefit, female productivity will decline. 

These statistics are uncomfortable, but their causes are myriad: lower publishing rates, career choices, motherhood and, probably, bias. In lab experiments women are subject to tougher standards. Their qualifications and ability are underestimated[#Foschi1996,Grunspan2016,Moss-Racusin2012,Reuben2014]. Female-authored manuscripts are evaluated more critically[#Goldberg1968,Krawczyk2016,Paludi1983]; when collaborating with with men, women are given less credit[#Heilman2005,Sarsons2015].
Peer review is not immune. Using five reliable measures of writing clarity, I show that female-authored articles published in top economics journals are better written than similar papers by men; the simplest interpretation is that editors and referees expect clearer, more direct writing from women. Because better writing takes effort to compose, higher standards prolong female review times---by six months at *Econometrica*---and may be a fundamental factor behind lower publishing rates.

Policy has responded by encouraging more flexible work hours, better paid and longer maternity leave and greater nursery care options. These policy responses are important; but the analysis of submit-accept times suggests women without any family constraints take the longest in peer review. This suggests current policy interventions may have very little impact on the gender and earnings gap in academia. To the extent that women face similar higher standards in other high-skilled occupations, these findings therefore suggest such policy interventions will do little to increase female representation in senior positions. This emphasis on schedules and child-rearing has shifted focus from cases of actual discrimination witnessed heretofore mainly in the lab---women's qualifications are underestimated, they are evaluated more critically and when collaborating with men, women are given less credit---to external factors independent of the firm. To the extent that it has highlighted the challenges and responsibilities hoisted on the typical working mother, it may even have unintentionally reinforced the gender wage gap by attributing pay differentials caused by discrimination to external factors beyond the firm's controls, effectively creating a convenient justification for lower wages that may actually have been caused by discrimination.

The gap forms dynamically at both the author- and article-levels---it grows between first draft and final publication and over the course of women's careers---preventing factors fixed in either direction from entirely resolving the difference. This rules out inborn advantages and one-off improvements in response to external circumstances unrelated to peer review.
Financing obstacles decline. When the "correct" decision is liquidation, however, judicial mistakes have precisely the opposite effect: more accurate judges correspond to more lenient lending conditions.
that the most informative signal is not always the ideal signal
To do

1. Space at beginning of paragraph =&gt; text substitution for &lt;!--\noindent--&gt;
2. Format proofs

2. Devise term for potentially cash flow/balance sheet insolvent firms.
3. Ensure changes to W, V and Y are consistent throughout text.
4. Create an annex explaining gory detail of equations.

5. Fix proofs.
6. Rewrite introduction.
7. Rewrite literature review.
8. Create the following extensions:
	a. Over borrowing if Z&lt;0
	b. Balance sheet insolvent: D-(X+K2)
	c. Long term contract.

More generally, the impact higher standards has on gender productivity gaps suggests caution in interpreting any estimates of wage discrimination stemming from equations relating earnings to gender, even (perhaps especially) with a large set of variables designed to control for differences in productivity---or, indeed, in any estimates of labour market discrimination.

This raises concerns about ignoring higher standards in wage equations that control for productivity.
More generally, the impact higher standards has on gender productivity gaps suggests caution in interpreting any estimates of wage discrimination stemming from equations relating earnings to gender, even (perhaps especially) with a large set of variables designed to control for differences in productivity. 
 six-month gap in peer review times *still* underestimates the cost to women for higher standards.
 Here, this finding suggests higher standards increase how much time women spend writing papers even before having to spend six more months in peer review. Thus, a six-month gap in review times may *still* underestimate the time tax women pay for higher standards.


lengthen the time between a paper's inception and publication beyond inducing a six-month longer 
higher standards reduce female productivity beyond the 
a six-month gap in review times *still* underestimates how much the impact higher standards have on 

productivity measures are further distored as women pre-emptively alter behaviour in response to hightened expectations.
This suggests productivity measures are further distorted as women pre-emptively alter behaviour in response to heightened expectations. Combined with women's surprising sensitivity to higher standards, 

women address higher standards before peer review even starts.
higher standards also impact how well women women allocate time and effort outside of peer review.
a portion of the effort women put into addressing higher standards occurs before their papers are even submitted.


My evidence also indicates higher standards have subtle, indirect effects on female productivity. I find peer review induces women to draft future papers more clearly. This suggests not only that women are highly sensitive to referee criticism, but also that they adjust behaviour indirectly to deal with them
also indicates that past experience with peer review induces women to draft future papers more clearly from the start. This finding suggests women 


even in a very controlled and highly scrutinized environment, these can be biased against minorities.  within a high-skilled, professional environment while cataloging a mechanism---higher standards---for why this occurs.
Disadvantaged players threw pitches that required less discretion to interpret; advantaged players’s needed more. Because the latter category are harder to hit, advantaged pitchers gave up fewer runs per game---a common metric of pitching performance. Thus disadvantaged (usually minority) pitchers appeared less skilled than they actually were. An important contribution of this paper is to confirm this conclusion in the context of gender discrimination within a high-skilled, professional environment.
They found calls by home plate umpires expressed 
home plate umpires made racially biased calls in low-scrutiny settings; disadvantaged pitchers respond to these situations by throwing more conspicuous strikes. An important contribution of this paper is to confirm both conclusions in the context of gender discrimination in a previously unmonitored, high-skilled, professional environment.

 and reduces research output, academia's benchmark for pay and promotion. Thus while performance may be an important to explaining the underrepresentation of women in economics, at least equally important is how we judge that performance. 
Moreover, women's permanent behavioural adjustment accumulates at a rate of X% per paper, meaning the gender readability gap mirrors wage dynamics in many high-skilled occupations[#Bertrand2009]. Higher standards may therefore contribute to women's general underrepresentation in senior positions.



Moreover, my results suggest a possible link between higher standards and the gradual increase of the gender wage gap with age. I find the readability gap does not decline with seniority---*i.e.*, women are subject to higher standards throughout their careers. By limiting the rate at which women accrue publications, they generate a wage and promotion gap that widens as economists age.
This limits the rate at which women accumulate publications, and leads to a widening gap in accrued publications Because academic pay is generally determined by researchers’ aggregate publication counts, this phenomena may explain why the gender wage and promotion gap is widest between well-published, senior economists.
potentially explains why academia’s publication gap is widest between well-published, senior economists. Because pay in academia is generally determined by publications accumulated over an academic’s career
This potentially explains both why the 
 and generates the widest publication gap between well-published, senior economists.
gender productivity gaps are widest between well-published, senior economists, because differences in standards prevent women from accumulating more publications.
 women must contend with higher standards throughout their careers---and that translates into a widening
This indicates that female economists are subject to a constant difference in writing standards over the course of their careers---which translates into a a widening 
, then the gap between male and female productivity will widen
If higher writing standards mean female economists take twice as long to write a paper, then their wage growth will be twice as slow as men, unless pay and promotions fairly compensate for both publication quantity and writing quality.
As I show in [][p3Experience], the readability gap grows by X% for each new paper a woman writes because women accumulate permanent improvements in how well they write. This gradual progression indicates that women incur *constantly* higher standards throughout their careers---*i.e.*, they do not benefit *at all* from lower standards as they gain experience. If constantly 
Frustration may also lead women to drop out of the labour market
it’s widest between well-published, senior economists---mirroring wage dynamics in many high-skilled occupations
mirrors wage dynamics in many high-skilled occupations[#Bertrand2009]---the gap is particularly wide between well-published, senior economists---suggesting their accumulative impact plays a role in women’s “glass ceiling”.

permanently improve their writing in response to referee criticism---and that accumulates at a rate of X% per paper. The analyses in [][p3NBER] and [][#p3Experience] suggest female-authored drafts are already better written before peer review even starts; the gap is particularly wide between well-published, senior economists. This finding suggests women write better in anticipation of higher standards yet to come. Thus higher standards may prolong peer review indirectly as women write more clearly prior to submitting. This increases the time required to write a paper. Female productivity again declines.

My findings suggest that gender discrimination can crop up in surprising and subtle ways. It also suggests that until we no longer see this type of discrimination in the lab, we should assume it still exists, it has just manifested itself in areas we have yet thought to look in.

 re suffers from a form of attention discrimination in so-called "lemon-dropping" markets---*i.e.*, markets in which the average candidate is acceptable. ---in a recent model by[#Bartos2016;]. find applicants of a particular sex or ethnic group are more heavily scrutinised either because employers dislike them (simple discrimination), believe they are less productive (statistical discrimination) or cannot as accurately interpret their signals (screening discrimination). The results from my field experiment supports these conclusions. I extend their work by suggesting an employers' tende

With respect to the mechanisms through which discrimination emerges, this study most closely resembles work by[#Parsons2011;] on Major League Baseball umpires. They found home plate umpires made racially biased calls in low-scrutiny settings; disadvantaged pitchers respond to these situations by throwing more conspicuous strikes. An important contribution of this paper is to confirm both conclusions in the context of gender discrimination in a previously unmonitored, high-skilled, professional environment.

My findings suggest these gaps will endure in every profession unless standards for women align with men's or women are fairly rewarded for raising them.
Higher writing standards improve quality but also lengthen the time between a paper's inception and publication, thereby limiting the number of papers its author can write. 
My evidence also indicates women write better in anticipation of higher standards yet to come, indicating performance m
Performance is important, yet it is also important how we judge performance. If higher standards for female output persists in narrow dimensions without contributing to the value of that output, this will lower female productivity
Given women are prone to adjust behaviour in anticipation of these higher standards, even measures that appear highly objective are likely affected by them; estimates of gender gaps in career outcomes that control for performance may therefore underestimate labour market discrimination.
Longer reviews limit the number of papers a woman can write in her lifetime.
This increases time between a paper's inception and its publication and limits the number of papers a woman can write. Assuming research productivity measures do not commensurately reflect higher standards, female productivity will remain artificially low. This may resolve academia's "Publishing Paradox".
This paper sheds light on gender differences in productivity.
Lower productivity due to higher standards provides a novel explanation for women's underrepresentation in economics departments. Mirroring most professions, female economists are less likely to make tenure, take longer when they do and earn much less than their male peers[#Bandiera2016,Ceci2014,Ginther2004,Sarsons2015]. These patterns persist despite appropriate controls for individual and work characteristics, and for at least a decade, they nave not declined. 
I find papers by both sexes are altered during peer review; the changes made to female-authored papers, however, are far more substantial and are positively correlated with the length of time spent in peer review
The results demonstrate that a gender neutral outcome does not preclude a gender neutral *process* that can still bias the outcome in unintended ways.
This conclusion indicates that instances of gender discrimination may have been underestimated in the past.

This conclusion suggests that controlling for productivity in estimates of gender earnings and promotion gaps may underestimate labour market discrimination unless those measures adjust for differences in standards.
The readability formulas used in this analysis provide a transparent, objective and accurate measure of higher standards---albeit in only the narrow dimension of writing clarity.
This increases the time between a paper's inception and publication and decreases the number of papers women can write. Assuming expending effort on writing well is not commensurately rewarded in measures of research productivity, female productivity will appear lower than it is.
If higher standards for female output do not increase its value---*e.g.*, better writing does not impact tenure and pay decisions---female productivity will decline. 

Unless women are accurately compensated for this tax on their time and resulting quality of their output, they will appear less productive---an obvious disadvantage when employers rely on imperfect measures to make hiring and promotion decisions. 

Alternatively, if clients tolerate fewer grammatical errors in female-authored legal briefs but do not pay more for those briefs, female lawyers will either reduce their fees or bill fewer hours than they actually worked, both key performance indicators in the legal industry.

Referee criticism prompts clearer drafts of future papers

While performance is important, what's also important is how we judge performance. The readability formulas used in this analysis are transparent, objective and accurate measures of writing clarity. An important advantage of readability formulas is that they establish an objective measure of quality that changes with referee subjectivity. Publication in a top economics journal is a transparent and comprehensive indicator of performance. Combining these measures therefore produces a productivity measure that transparently reflects the *actual* productivity of women. Such a measure may prove a potent instrument to correct for estimates of gender gaps in career outcomes that control for performance.

Finally, this paper contributes to research that women are held to different---usually higher---standards than men as well as broader empirical work on the negative implications of stereotypes.
This research also adds to other recent studies that have shown that women are held to different, usually higher, standards than men in the workplace. Although writing standards are the focus here, prior research suggests women and men are held to separate standards beyond this narrow dimension.
This research also contributes to the recent research on the impact of stereotypes. Its findings confirm similar conclusions in research on employee performance reviews, teaching evaluations and online comments---women receive more abusive feedback, less credit for intelligence and creativity and are expected to be more organised, prepared and clear[#Boring2015,Correll2016,Gardiner2016].
These findings also emphasise the role transparency and monitoring play in reducing discrimination. Journal acceptance rates are easy to measure and frequently auditing; both factors foster accountability, which in turn encourages gender neutrality[#Foschi1996]. Indeed, prior investigations suggest acceptance decisions are genuinely bias-free [see, *e.g.*,\]\[][#Abrevaya2012,Blank1991,Borsuk2009,Gilbert1994,Lloyd1990]. The content of referee reports however, is subject to much less scrutiny; referees have fewer incentives to suppress bias.

My findings suggests these gaps will endure unless standards for women align with men's or women are fairly rewarded for raising them. 

Women in many high-skilled professions appear less productive than men[#Ceci2014,Xie2005,Ecklund2011,Azmat2016]. Unfortunately, there are relatively few studies that provide compelling explanations for these productivity differences: productivity gaps remain despite accounting for children, responsibilities and hours worked.
Instead, this paper suggests that academia's primary yardstick of productivity---the number of articles published in top journals---is distorted during peer review. Using several objective measures of writing quality, I show that editors and referees in top economics journals expect clearer, more direct writing from women. As a consequence, two factors emerge to reduce female productivity: (i) female-authored papers spend more time in peer review; and (ii) women start writing subsequent papers clearer upfront in anticipation of referee criticism.

More broadly, these findings suggest that tougher standards in arbitrary dimensions reduce female productivity. They highlight that using performance measures to understand gender gaps in career outcomes likely underestimates---or even altogether misses---instances of gender discrimination in the labour market. Thus narrowly focusing public policies on increasing female productivity---e.g. by encouraging men to shoulder a larger portion of childcare responsibilities or promoting flexible work schedules---may not be sufficient to achieve gender equality in the workforce.
women may modify their behaviour in ways that superficially mask the effect of discrimination by making their contortions resemble voluntary choice.

Indeed, under these conditions---properly addressing referee concerns increases one's acceptance odds, and women more consistently and accurately address those concerns---female-authored papers *should* enjoy higher acceptance rates.  Thus *either* ignoring referee concerns has no consequence
Yet comparisons between 
Indeed, if women are better at addressing referee concerns, their papers *ought* to be accepted at higher rates. Since there is no evidence that 
 Failing to address referee concerns is effectively a lottery: I benefit by saving time, but lose by suffering a higher probability of being rejected during a revise and resubmit
Responding to referee requests can be seen as a lottery: if I address those concerns, I increase the probability my paper is eventually accepted but also lose time in addressing those concerns. A female strategy 

Instead, women---and only women---apparently figure out that better writing makes peer review smoother; they write subsequent papers clearer from the start. This evidence is consistent with higher standards.

There is no evidence selection effects are behind the [trend][p3ExperienceExplanation]. Senior female economists do not co-author with more women and senior male economists do not co-author with more men. (In fact, senior economists regardless of gender publish more often with members of the opposite sex than they did as juniors.) It is also not true that initially bad female writers (or initially good male writers) disproportionately leave academia: the readability of an author's first paper has zero predictive power on how often he (or she) eventually publishes in a top four economics journal.




This third point is consistent with the possibility that higher standards are causing the gap. 

 learn that writing well is not worth the effort---*i.e.*, readability declines (or at a minimum, remains constant) over the course of their careers. 
women's writing gradually gets better but men's does not. 
With no benefit to better writing, however, they cannot account for the relationship actually observed: women most familiar with how peer review works write the clearest.


update posterior beliefs about the benefits and/or expectations of readability or the expectations of referees after successive rounds of peer review. Thus (ii), readability declines over the course of women's careers; or (i)
point to constant or declining readability as women update believes about the payoffs to clear writing after successive rounds of peer review
 in order to write more clearly although they do not actually generate any tangible benefit (acceptance rates do not change)
differences in behaviour, biology and/or information about the costs of writing well as grounds for explaining the gender readability gap. 
These interpretations imply women accept the cost of writing well (longer drafting and revision stages) despite getting no benefit (acceptance rates do not change)
behavioural, biological and or misinformation grounds for explaining the readability gap. These explanations 
explanations that imply women accept the cost of writing well (longer drafting and revision stages) despite getting no benefit (acceptance rates do not change). Bayesian learning applied to these theories point to constant or declining readability after successive rounds of peer review. 
with greater exposure to peer review. Without any benefit to better writing, however, readability should not improve---yet exactly those women who write the clearest are also the women most familiar with how peer review works.
These explanations are motivated either by (i) fixed biological differences between the sexes; or (ii) misinformation about the costs of writing well. In the latter case, women update posterior believes about its benefits after successive rounds of peer review---*i.e.*, readability declines over the course of their careers. In the former, readability may decline---as women adapt to environmental expectations---or remain constant. Neither explanation accounts for the relationship actually observed: the clearest female authors are also the women most familiar with how peer review works.

women most familiar with how peer review works write the clearest.
female economists who write the clearest are precisely those women most familiar with how peer review works.


onsider first the author's choice of $r_{1it}$ given $r_{0it}$of $R_{it}$---*i.e.*, To do that, I restrict attention to the "revise and resubmit" (R&amp;R) decision---*i.e.*, I analyse the author's choice of $r_{1it}$, given $r_{0it}$. Section X investigates the relationship between $r_{0it}$ and $r_{1it}$, and correspondingly discusses in more detail the initial screening process of papers and author's original choice of $r_{0it}$.
The author submits the initial draft of his paper to a journal. There, it is assigned to the group of reviewers $e\in\mathcal E$, where $\mathcal E$ is the set of all review groups. Given the focus of the analysis, we have assumed that review group $e$ has already decided to invite the author to revise and resubmit his manuscript. At the same time, they have collectively decided that the paper should be published if and only if after revising the document, the paper's readability is at least $\widetilde R_{it}^e$. $\widetilde R_{it}^e$ is a threshold on readability specific to the review group $e$; it depends on the characteristics of the paper---*e.g.*, topic, novelty and quality---journal and author; it may be prejudiced by the review group conditional on any of these factors.
Unfortunately, the concept of readability is complex and ambiguous, meaning members of the review group cannot convey $\widetilde R_{it}^e$ directly. Instead, they compile a "referee report" from which the author forms expectations about $\widetilde R_{it}^e$ by assigning subjective probabilities $\mu_{1it}^e(R)$ to all $R\in\mathbb{R}$. $\mu_{1it}^e$ depends on characteristics of the paper, journal and review group and may be biased by the author's interpretation of the referee report or his mis-information about peer review.
The author's subjective expected utility from choosing readability $r_{1it}$ given his initial choice $r_{0it}$ is $$\mathrm{\widehat M}_{1it}^e(R_{it})\,u_{i}-c_{1i|r_{0it}}(r_{1it}),$$ where $\mathrm{\widehat M}_{1it}^e(R_{it})$ is the sum of all $\mu_{1it}^e$ such that $\widetilde R_{it}&lt;R_{it}$, $u_{i}$ is the author's utility of having his paper accepted and $c_{1i|r_{0it}}(r_{1it})$ represents the cost of making changes $r_{1it}$ given the paper's initial readability $r_{0it}$. $c_{1i}$ is increasing and convex in $r_{0it}$ and $r_{1it}$. Marginally increasing $R_{it}$ generates proportionally less utility but requires proportionally more effort when the paper is already well written.
$u_i$ is strictly positive and independent of $R_{it}$. Authors probably care about getting their papers accepted and they may care about writing well, but their marginal utility from the intersection of the two events---*i.e.*, higher utility from writing well *only* because the paper is published in a top-four journal---is assumed to be negligible. For this reason, the impact of author $i$'s personal utility from writing well is reflected only in the first stage when choosing $r_{0it}$ (see Section X, where it is included as a specific element in the author's objective function); it is otherwise irrelevant for the second phase, when choosing $r_{1it}$.
&lt;!--\input{$PPATH/theorems/lemma}--&gt;
In other words, women chose to make their papers more readable because they think it will improve the probability their papers are accepted. This does not imply that utility or risk aversion or whatever doesn't factor into women's choice of $R_{it}$. As long as writing more clearly means a higher probability of acceptance, a higher $u_j$ will indeed induce authors to write more clearly. Women may also be more responsive to referee criticism (reflected in a more pessimistic $\mu_{j0}^e$ for all $\widetilde R_j^e$), but it has to be accompanied by a higher probability of acceptance. Absent that, SEU rational agents will not choose $R_{j}&gt;R_{j}'$.
Do women get an advantage from writing more clearly? Theorem X is conditional on having received an R&amp;R. That is, all women who receive an R&amp;R and still write more clearly than men during the revision process expected a strictly higher probability of acceptance.
This can explain a wide variety of behaviours. For example, women are more sensitive to referee criticism not because they enjoy being sensitive but because they *think* it is rewarded with a higher acceptance rate. Similarly, women may indeed be more risk averse---reflected in X by a higher instantaneous utility of acceptance---but if they choose to write more clearly, again, it was only because they *thought* it would improve their probability of acceptance.

**Corrollary**. Assume $R_{kt1}$ and $R_{it1}$ satisfy Assumption (3) and $\widehat\mu_{kt1}(R_{kt})=\widehat\mu_{it1}(R_{it})$, where . Then there exists a $\widehat\Delta_{ik}(R_0)&lt;0$ such that $$\lim_{t\rightarrow\infty}\Delta_{ikt}(R_0)\longrightarrow\widehat\Delta_{ikt}(R_0)$$ almost surely.
Yet readability *does not* improve the probability that women's papers are accepted. Although women's papers are indeed more readable, to my knowledge no prior investigation of gender bias in peer review has previously shown that women's papers were accepted at any *higher* rate than men's.
Women might be more likely to choose a higher R if their utility or acceptance was higher or marginal costs were lower. But what Theorem X establishes is that that they don't do that for the hell of it---they are only going to write more clearly if comes with an additional benefit.



Groups classified together share, *inter alia*, a disposition toward the paper, attachment to the author, attention to detail and range of conscientiousness. 



; it is independent of $\widetilde r_{0it}^e$ and likewise conditional on characteristics of the paper, journal, review group and author. If $R_{it}&lt;\widetilde R_{it}^e$, the paper is rejected.

$\rho_{it}^e$ reflects the quality and clarity of referee reports written by group $e$. $\widetilde r_{0it}^e$ and $\widetilde R_{it}=\widetilde r_{0it}+\widetilde r_{1it}^e$ are readability thresholds imposed by group $e$ for granting author $i$'s $t$th paper a "revise and resubmit" (R&amp;R) and accepting it for publication, respectively. That is, the author is given an R&amp;R if $$\widetilde r_{0it}&lt;r_{0it}.$$ But then after granting the author an R&amp;R, review group $e$ is willing to accept it only if the revised manuscript is at least $$\widetilde R_{it}^e&lt;R_{it}.$$
make two separate decisions. The first decision is whether to invite the author a "revise and resubmit" (R&amp;R); the second decision is whether 
are distinguished by two rejection thresholds on readability imposed at 
minimum readability threshold they impose on author $i$'s $t$th paper, denoted by $\widetilde r_{0it}$ and $\widetilde r_{1it}$, and the quality and clarity of their referee report, denoted by $\mathcal r\in\mathcal R$, where $R$  is the set of all referee reports.

 each of which is distinguished by the quality and clarity of its referee report. Depending on the characteristics of the journal, paper---*e.g.*, topic, novelty and quality---and author---*e.g.*, institutional affiliation and gender---review group $e$ grants author $i$ a "revise and resubmit" (R&amp;R) if $\widetilde r_{0it}^e&lt;r_{0it}$, where $\widetilde r_{0it}^e$ is a readability threshold specific to review group $e$.

denote the publications threshold on readability at time 1. In the even an R&amp;R is granted, review group $e$ will still reject the revised paper at time 1 if $$R_{it}&lt;\widetilde R_{i}^e,$$ where $\widetilde R_{i}^e=\widetilde r_{0i}^e+\widetilde r_{1i}^e$,
where $\widetilde r_{1i}$ reflects the difference between the initial threshold for receiving an R&amp;R and the threshold for publication.
time 1 threshold on readability specific to review group $e$ and author $i$---*i.e.*, the 
n the event an R&amp;R is granted, review group $e$ will still reject the revised paper at time 1 if $$R_{it}&lt;\widetilde R_{i}^e,$$ where $\widetilde R_{i}^e=\widetilde r_{0i}^e+\widetilde r_{1i}^e$, $R_{it}=r_{0it}+ r_{1it}$, $r_{1it}$ are the author's revisions and $\widetilde r_{1i}$ reflects the difference between the initial threshold for receiving an R&amp;R and the threshold for publication. $\widetilde r_{0i}$ and $\widetilde r_{1i}$ are assumed to be independent.
The concept of readability is complex and ambiguous, meaning members of the review group cannot convey it directly. Instead, $e$ tries to convey this information in a referee report, denoted by $\rho_i^e$. The author uses the report $\rho_i^e$ to form expectations about $\widetilde R_i^e$ by assigning subjective probabilities $\widehat\mu_{1it}^{\rho_i^e}(R)$ to all $R$. $\widehat\mu_{1it}^{\rho_i^e}$ may be biased by the author's interpretation of the referee report or a skewed perception of peer review.
 Authors, in turn, misconstrue even excellent reports when they are, *inter alia*, over- or under-confident in their paper's other qualities or excessively sensitive to low-probability events. Thus, the author's interpretation of the report is imprecise; the expectations he forms about $\widetilde R_i^e$ can only be represented in the subjective probabilities $\widehat\mu_{1it]^e(R)$ he assings to all $R$ after reading the report.

It *also* implies that if author $i$ choses a higher readability than another author with an otherwise equivalent paper, then author $i$ anticipated a higher probability of acceptance, *ex ante*.


that unless author $i$ chooses the same $r_{0it}$ and $R_{it}$ every single time period, 
Yet when he chooses to *change* his readability from one time to the next, that decision is driven by his desire to amp up his probability of acceptance.
[](#theorem1) shows that if at time $t$ author choses $r_{0it}$ and $R_{it}$ but at some other time $t'$ he chose $r'&lt;r_{0it}$ and $R'&lt;R_{it}$, then the higher readability at time $t$ is driven exclusively by the author's desire to attain a higher acceptance rate. 
 author $i$'s *ex ante* subjective probability of being accepted given his optimal readability choices $r_{0it}$ and $R_{it}$ is never equal to (and strictly greater than) his subjective probability of acceptance at every other $r'&lt;r_{0it}$ and $R'&lt;R_{it}$.


[](#theorem1) also implies that an increase in readability between time $t'$ and time $t$ was driven by the expectation of being rewarded for it with a higher acceptance probability. It *also* implies that if author $i$ chose a higher readability than another author with an otherwise equivalent paper, then author $i$ anticipated a higher probability of acceptance, *ex ante*.
the optimal choice of readability is always linked to the probability of acceptance: although many reasons may factor into a woman's choice of $r_{0it}$ and $R_{it}$, but absent a higher acceptance rate, women do not increase
women chose to make their papers more readable because they think it will improve the probability their papers are accepted. This does not imply that utility or risk aversion or whatever doesn't factor into women's choice of $R_{it}$. As long as writing more clearly means a higher probability of acceptance, a higher $u_i$ will indeed induce authors to write more clearly. Women may also be more responsive to referee criticism (reflected in a more pessimistic $\pi_{0it}^s$ for all $\widetilde R_{it}^s$), but it has to be accompanied by a higher probability of acceptance. Absent that, SEU rational agents would prefer to choose a smaller $R$.
This can explain a wide variety of behaviours. For example, women are more sensitive to referee criticism not because they enjoy being sensitive but because they *think* it is rewarded with a higher acceptance rate. Similarly, women may indeed be more risk averse---reflected in X by a higher instantaneous utility of acceptance---but if they choose to write more clearly, again, it was only because they *thought* it would improve their probability of acceptance.

But I'm also relatively certain that if I could get away with writing less readably, then I would.


This does not imply that $u_i$, $\phi_i$ and $c_i$ do non influence $r_{0it}$ and $R_{it}$. It does imply, however, that their influence should not be reinforced over timeFor their influence on readability to be *reinforced* however, they must bolster other *explicit* objectives. For example, sensitive authors do not become more sensitive unless that sensitivity is useful for achieving the explicit objectives in [](#equationX1) or [](#equationX2). Thus, Indeed, if at any point he chooses to write his current paper more clearly than some previous paper, this choice was motivated by a higher ex ante subjective expected probability of acceptance. Absent that advantage, author $i$ prefers the smaller readability chosen previously.
$u_i$, $\phi_i$ and $c_i$ remain constant over time. Thus, changes to $r_{0it}$ and $R_{it}$ imply changes in the subjective probabilities author $i$ forms about $\widetilde r_{0it}$ and $\widetilde R_{it}$. Thus, when authors write current papers more readably than past or future papers, they do so because they expected to be rewarded for it with a higher probability of acceptance. Changes to the optimal choices $r_{0it}$ and $R_{it}$ are motivated entirely by a desire to achieve his other objective of getting his paper accepted into a prestigious academic journal.
[](#theorem1) indirectly implies that if author $i$ choses a higher readability than another author with an otherwise equivalent paper, then author $i$ anticipated a higher probability of acceptance, *ex ante*. Of course, that doesn't mean author $i$ is right---it is perfectly plausible that he's misinformed and there is no difference in their probabilities of acceptance. As $t\rightarrow\infty$ in a fixed state $s$ with perfect recall of his past histories or in any state with access to his peers' histories, author $i$ eventually figures this out.


(For an overview of the literature, see, *e.g.*, [#Ceci2014;].) Thus, if female author's improve both improve their writing over time *and* persistently write more clearly then men, then [](#Theorem1) implies that female authors suffer from a subtle form of discrimination. Either referee assignment is biased in favour of men---*i.e.*, journal editors disproportionately refer female-authored papers to the toughest critics---or referees are more critical of female-authored work.



This suggests two simple empirical tests can establish if editorial standards and/or referee assignment beyond authors' control is driving the readability gap, or, instead, it's being driven by gender differences in biology/behaviour and/or knowledge about referee expectations.
The first test 

[](#Theorem1) relies on two principle identifying assumptions: (i) [](#equationX1) and [](#equationX2) accurately reflect authors' objective functions; and (ii) men and women are accurately sorted into appropriate equivalence classes. If so, then The first identifying assumption is discussed at length in [](#Robustness). For the moment, I assume it is satisfied and focus instead on Conditions (1)--(3) under conditions in which the topic, novelty and quality of a paper is independent of gender.

Thus, 
For either phenomena to cause a *persistent* readability gap, however, then female-authored papers will be disproportionately represented in academic journals.
Under these conditions, female-authored papers should enjoy higher acceptance rates, all things equal.
If they don't---*i.e.*, if Condition 1 is satisfied---then diligently addressing every referee concern has no apparent upside---acceptance rates are unaffected---and a very clear downside---constant redrafting takes time. In that case, women may indeed have a higher utlity or acceptance or simply be more risk averse
In the absence of other reason to keep readability so high, rational women re-examine initial beliefs… and then start 
When Condition 1 is satisfied, however, this behaviour cannot persist long-term. If women care more about being accepted, 
Persistent effort in the absence of any tangible reward requires systematic failure among women to efficiently allocate resources
If women's hard work solely behind long-term readability differences, however, they must be rewarded with higher acceptance rates.
For such gender differences in preferences to persist long term, however, 
 write more clearly in peer review. A gender readability gap---even one formed *during* peer review---could reasonably reflect gender differences in preferences. For example, more risk averse individuals or those with higher utility of acceptance might exert more effort (on average) to reduce their odds of being rejected. They expect to be rewarded, however, with higher acceptance rates, all else equal. As a consequence, their papers would---and, indeed *should*---be disproportionately represented in academic journals. So gender differences in preferences lead to appropriate gender differences in outcomes.
If better written female-authored papers are accepted at higher rates than more poorly written papers by otherwise equivalent male authors, then women are being rewarded for the time and effort they put into writing more clearly. The gender readability gap could reasonably reflect gender differences in the utility of acceptance. For example, women are more risk averse[see, *e.g.*,\]\[][#Borghans2009]. Consider revision rounds as lotteries. An author’s labour reveals his willingness to pay to reduce risk, so a positive link between risk aversion and better writing implies risk averse individuals exert more effort (on average) to reduce their odds of being rejected. Under these conditions, risk aversion leads to higher acceptance rates, all else equal. So if female authors truly are more risk averse than men, their papers would---and, indeed *should*---be disproportionately represented in academic journals. 


As for Condition 1: readability gap could emerge from underlying gender differences in $u_i$: ambitious authors or those strongly averse to being rejected probably work harder to secure publications. Nevertheless, they eventually learn readability's true impact on rejection, so $i$'s unconditional probability of acceptance must be higher than $k$'s for $u_k&lt;u_i$ to produce $R_{kt}&lt;R_{it}$ indefinitely.

[1] First authors are those identified in the acknowledgements or listed first when authors are not ordered alphabetically. See [][Data] for more details.</Text>
            <Comments>Note that higher standards could be the result of always accepting male-authored papers more often than female-authored papers, conditional on strategy (and $X$): $a_m(x,X)&gt;a_f(x,X)$, or rewarding men's strategy more even though it generates the same quality as women's strategy, $a(x_m,X)&gt;a(x_f,X)$, or both.
Note that higher standards could be the result of always accepting male-authored papers more often than female-authored papers, conditional on strategy (and $X$): $a_m(x,X)&gt;a_f(x,X)$, or rewarding men's strategy more even though it generates the same quality as women's strategy, $a(x_m,X)&gt;a(x_f,X)$, or both.
Note that higher standards could be the result of always accepting male-authored papers more often than female-authored papers, conditional on strategy (and $X$): $a_m(x,X)&gt;a_f(x,X)$, or rewarding men's strategy more even though it generates the same quality as women's strategy, $a(x_m,X)&gt;a(x_f,X)$, or both. Similarly, higher costs could imply that men's costs are always lower for a given strategy, *i.e.*, $c_m(x)&lt;c_f(x)$, or $x_f$ is costlier to implement for both parties, *i.e.*, $c(x_m)&lt;c(x_f)$, or both.
Note that higher standards could be the result of always accepting male-authored papers more often than female-authored papers, conditional on strategy (and $X$): $a_m(x,X)&gt;a_f(x,X)$, or rewarding men's strategy more even though it generates the same quality as women's strategy, $a(x_m,X)&gt;a(x_f,X)$, or both. Similarly, higher costs could imply that men's costs are always lower for a given strategy, *i.e.*, $c_m(x)&lt;c_f(x)$, or $x_f$ is costlier to implement for both parties, *i.e.*, $c(x_m)&lt;c(x_f)$, or both.
Recall that [](#table7) suggests the *change* in the gender readability gap that occurs in peer review was actually negative in the double-blind sample---*i.e.*, the gender readability gap reverses.
Applying [](#Corollary1) to [](#equation14), however, requires additionally assuming that $\mu_{it}$ does not partially correlate with a paper's ratio of female authors conditional on the experience and gender of its author---*i.e.*, $\text{female ratio}_{it}$ cannot act as a collider on the causal chain from higher standards to greater readability, conditional on the gender and experience of an author.
Although, if referees harbour stereotypes about author's institutions in a way that is partially correlated with gender---*e.g.*, by holding institutions with more women to higher standards---then *unconstrained* women may adapt by moving to non-stereotyped institutions.
In the August 2018 version of this paper ([available on my website](http://www.erinhengel.com/research/publishing_female20180828.pdf)), estimates were based on data from *Econometrica* alone and included these controls. Results and conclusions are similar to those presented here.
Abstract readability is highly correlated with the readability of other sections in a paper (see [][MeasuringReadability] and [](#figure3)).
As discussed in [][MeasuringReadability], readability formulas exploit the well-established empirical fact that simple vocabulary and short sentences are easier to understand and straightforward to quantify. The formulas used in this paper are five of the most widely tested and reliable ones for adult reading material: Flesch Reading Ease, Flesch-Kincaid, Gunning-Fog, SMOG and Dale-Chall scores.
Abstract readability is highly correlated with the readability of other sections in a paper (see [][MeasuringReadability] and [](#figure3)).
Controlling for citations and other proxies of an article's quality---*e.g.*, order in an issue and author fixed effects---has very little impact on the results. Nevertheless, most estimates in this paper should be interpreted as gender differences in readability, conditional on the underlying quality of the paper as proxied by citations.
Unobserved heterogeneity in submission quality should not bias conclusions elsewhere in the paper as long as it does not partially correlate with the share of female authors on a manuscript. If women are held to higher standards, however, this assumption is violated when controlling for review time and estimating the impact gender has on readability (or visa versa). (Conditioning on one when estimating the other opens up a backdoor channel to the unobserved heterogeneity.) Although controlling for citations introduces a similar concern, [](#figure2) and [](#table10) suggest correlations are weak (and hence bias low). [#Hengel2016;] presents most analyses without citation controls; results are very similar to those shown here.
Most studies that use outcome tests to measure racial discrimination assume zero inter-group differences in the counterfactual equilibrium.
Talk about outcome tests: Ayres and Becker propose them as ways to identify discrimination. That's not novel. Other people use employer learning to identify discrimination, so the learning process itself is not novel. But no one uses the learning process of women to identify discrimination against them. It's a combination of the outcome test and learning models.
Each of [](#Theorem1)'s conditions must technically hold for the same author in two different situations---before and after gaining experience and when compared to an equivalent, experienced author of the opposite gender.
[#Weisshaar2017;] evaluates the probability of making tenure in Sociology, Computer Science and English departments.
To verify this conclusion, I would ideally rerun the analysis on the subset of matched pairs where the woman has a $t=1$ *and* $t=3$ solo- or exclusively female-authored paper. Unfortunately, only ten matched pairs satisfy this criteria. Nevertheless, in these restricted samples I still observe discrimination in about 50-60 percent of matched pairs *and* on average over 90 percent suggest discrimination against the female pair. In twenty matched pairs, the woman's $t=1$ and $t=3$ papers are majority female-authored; in 76, her papers are at least 50 percent female-authored. In both restricted samples, about 60--70 percent of matched pairs exhibit discrimination---and 80--90 percent of those are discrimination against the woman. The average proportional effect on discrimination is 6--9 percent.
Unfortunately, testing a similarly restricted sample using the matching analysis described in this section runs into sample size restrictions---only ten women have $t=1$ and $t=3$ solo- or exclusively female-authored papers, respectively. Nevertheless, in these restricted samples I still observe discrimination in about 50-60 percent of matched pairs *and* over 90 percent suggest discrimination against the female pair. But twenty women have majority female-authored papers (*i.e.*, females are more than 50 percent of authors) and 76 are at least 50 percent female-authored. In these restricted samples, about 60--70 percent of matched pairs exhibit discrimination---and 80--90 percent of those are discrimination against the woman. The average proportional effect on discrimination is 6--9 percent.
Including factors outside $i$'s control as independent variables could bias results because they may be the conduit through which $i$ chooses to achieve the higher standards set on her. See the discussion around [](#equation15) for more details.
Yet even the latter is potentially endogenous. If referees harbour stereotypes about author's institutions in a way that is partially correlated with gender---*e.g.*, by holding institutions that are more likely to hire women to higher standards---then unconstrained women will adapt by moving to non-stereotyped institutions.
Nevertheless, in this tiny sample I still observe discrimination overwhelmingly against the female pair---see the Robustness section for more details. Moreover, [](#table8XC) reproduces [](#table8) using only exclusively female-authored papers---which are overwhelmingly solo-authored or authored with a women of equivalent experience. Results are very similar to those presented in [](#table8).
As I show in an earlier version of this paper, however, the *reverse* is, in fact, true: as $t$ increases women are more likely to co-author with men; men are more likely to co-author with women[\]\[p. 25][#Hengel2016].
That is, a dual-authored paper published in 2008 in the *American Economic Review* where $t=3$ for $i$ and his co-author and their institutions rank 48 and 54, respectively.
Conditional on correct calculation, the scores' proxies of vocabulary complexity---number of syllables per word, number of words with three or more syllables and number of words not on the Dale-Chall list of easy words---are highly accurate[for a discussion, see\]\[][#Chall1995]. The relationship may even be causal: words *become* shorter as they are used more frequently[#Zipf1935;].
Nevertheless, one could simply interpret gender differences as "women write shorter sentences and use words with fewer syllables, fewer polysyllabic word and more words on the Dale-Chall list of easy words".
Or even "women write shorter sentences and use words with fewer syllables, fewer polysyllabic word and more words on the Dale-Chall list of easy words".
For those sceptical about the scores' various proxies of vocabulary complexity, the inescapable conclusion would be that women write shorter sentences and use words with fewer syllable, fewer polysyllabic word counts and more words on the Dale-Chall list of easy words.
A similar argument can be made about a fourth area of classical measurement error: making the leap from using syllables counts and word lists to infer the number of hard words in a text. Given the extensive evidence that both are indeed highly accurate proxies for vocabulary complexity, this particular field would bias results if men were more likely than women to use simple vocabulary that contained many syllables *and* those same words are not on the Dale-Chall list of easy words.
An alternative bias is instead the "file drawer bias"---*i.e.*, all the other studies showing no effect just don't get published. I try to counteract this effect by collecting a relatively large sample. Nevertheless, the required sample size necessarily depends on the number of female authors, of which there are not very many. Thus, the possibility that the effects I observe in *three separate samples* are just statistical flukes can not be ruled out.
This conclusion is obviously specific to economics, although it may also apply to fields with an analogous culture of presenting, disseminating and publicising working paper results. Again, please interpret these results with caution.
Forty-three articles with at least one female author and 236 articles with no female author.
The discussion in [](#footnote46) also applies to the precise accuracy of the assumption's phrasing used here.
[#Blank1991;] ranks institutions by National Academy of Science departmental rankings. Those and similar official rankings are based largely on the number of papers published in the journals analysed here.
Note that higher desk rejection rates and arduous review are substitutes. For example, if desk rejection rates are gender neutral, authors subjected to higher standards will undergo more arduous peer review. Greater scrutiny would therefore replace higher desk rejection rates when editors or even referees) monitor and implement a policy of gender neutral acceptance rates.
This study suffers from the same criticism. For example, it does not take into account the impact higher standards have on (potential) female economists' choice of field, specific topic or even their decisions to remain in the workforce.
Assuming no gender difference in acceptance rates at $t=3$ and given evidence that women are held to higher standards documented earlier, [](#figure6) suggests---but does not prove---that manuscripts by junior female economists are disproportionately rejected. See also [](#Footnote132) for an alternative interpretation in which acceptance rates are identical but scrutiny is not.
&lt;!--\label{Footnote132}--&gt;Alternatively, if desk rejection rates are gender neutral, authors subjected to higher standards will undergo more arduous peer review. Greater scrutiny would therefore replace higher desk rejection rates when editors (or even referees) monitor and implement a policy of gender neutral acceptance rates.
When these issues are also correlated with factors that are in turn correlated with the variable of interest, they can potentially bias results.
And in any case, readability scores are perfect (or almost perfect) predictors of sentence and word length. Thus, the figures presented in this paper will always capture differences in the weighted averages of these two measures. And if women write shorter sentences and use simpler words, that's informative, on its own. See [][Alternatives] for a detailed discussion on interpreting gender differences in readability scores as actual gender differences in readability as opposed to simply gender differences in sentence and word length.
Note that readability measures more accurately capture a weighted average of vocabulary and, especially, sentence length. Measurement error is only introduced when we extrapolate from these to components and infer "readability".
It's worth nothing that none of the alternative readability measures perfectly capture "readability" either. Even the presumed gold standard---human judgement---itself suffers from a high degree of inconsistency[SOURCE]. Some research has even suggested that the cloze procedure, reading comprehension tests and even readability scores themselves are more accurate at gauging readability than humans in fact are[SOURCE].
At a bare minimum, no study (to my knowledge) has ever shown that any of the five scores used here are significantly inversely related to reading difficulty. Evidence from [#Begeny2014;] suggests the four grade-level readability scores, and particularly the SMOG and Dale-Chall scores, are more accurate for higher ability readers. (The study did not assess the Flesch Reading Ease score.)
Similarly, intelligence tests predict intelligence but studying for intelligence tests does not actually create intelligence. Implicit bias tests predict biases but learning to perform well on one is unlikely to  cause one to be less biased.
Please see [#Loughran2016;] for a detailed review of this literature.
When the co-authoring relationship is between a senior and junior colleague, the junior tends to write the manuscript while the senior puts more effort into revising it. Since both writing the original draft and revising the manuscript influence readability, co-authors in both relationships likely influence the readability of a paper. [CONFIRM REFERENCE]
The cloze randomly deletes selected words in a particular passage of text and ranks texts by readers' ability to correctly enter the missing words.
Specifically, 3,000 words understood by 80 percent of fourth-grade readers (aged 9-10).
Although several readability studies and measures already existed at the time, most relied exclusively on word lists and were developed for (and tested on) children. Interestingly, the earlist recorded attempt to quantify readability was from a group of Talmudists in 900 A.D.
The Dale-Chall formula is intended for adults and chlidren above the 4th grade reading level. The Dale-Chall formula was updated in 1995. I use this updated formula throughout this paper.
Flesch and Gunning's work had an enormous impact on journalism. Together, they helped bring down the reading grade level of front-page newspaper stories from the 16th grade level (*i.e.*, university-level) to the 11th grade level (*i.e.*, high school level).
Alternative measures using alternative criteria to estimate word difficulty and/or sentence length have also been developed. They generally fall into two camps: those that use noisier criteria for these two components (but are easier to calculate), such as the Coleman-Liau index and those that use more conventional criteria to gauge reading difficulty but are more difficult to calculate. It has been repeatedly shown, however, that after controlling for sentence length and word complexity, adding additional features of the text does not improve the predictive value of scores. Moreover, the evidence on character counts as a proxy for readability is limited, and the evidence that does exist suggests it is a much noisier indicator than syllable counts and other more sophisticated factors meant to approximate word difficulty.
[#Ardoin2005;] found that all four scores were negatively correlated with their designated proxy for reading difficulty---words read correctly per minute---although the effects in the Flesch-Kincaid and Dale-Chall scores were not significant. 
[#Beniot2017;] ask readers to compare two snippits of text and decide which is easier to read. They found that the Flesch Reading Ease ranking corresponded to the human comparison 56.8 percent of the time---*i.e.*, it performed about 6.8 percent better than chance. Note, however, that the snippits they use were very close in readability to one another. As a precondition for participating in the survey, participants had to correctly identify the more readable text between two snippits with much wider dispersion of readability scores. Thus, their lower accuracy is predominately a function of the nearness in readability in the two comparison texts. Given their setup, it is obvious that readability scores are much more accurate when comparing two different texts with a more noticable difference in readability.
Syllables per word was not originally meant to capture word difficulty, per se, but instead to capture how abstract a word was, on the assumption that the more abstract a word or idea is, the more difficult it is to comprehend. Flesch originally measured word abstraction by counting affixes. Later, he switched to syllables, which were easier to count and tended to correlate highly with the original mesure
In fact, the use of word lists to classify the difficulty of written material in a scientific manner goes back as far as 900 A.D. to a group of Talmudists. They counted the words and individual ideas so that they could know how many times each word appeard in the scroll and how frequently each word appeared in an unusual sense. Among the reasons for the elaborate counting of the Torah were clarification of unusual meaning and the devision of the reading of the weakly portions into approximately equivalent comprehension units.
Of second order importance is the probability female authors are responsible for making editorial changes during peer review.
Standard errors are generally higher for the less inclusive measure and effect sizes tend to be smaller for the more inclusive measure.
Moreover, conducting the analysis in [][Matching] would mean restricting the sample to only those women with at least three 100% female-authored papers; there are only 22, and only an additional 26 women have two. Indeed, 26% of female authors with at least three top publications have always authored with at least one man.
One could also include in that designation papers with a female first author. I therefore designate the lead author has he who was listed first in a paper in which authors were listed non-alphabetically or he was listed as the "corresponding author" or "lead author" in the acknowledgements. Unfortunately, however, this added only another 35 (0.38 percent) "female" observations.
Assuming no gender difference in acceptance rates at $t=3$ and given evidence that women are held to higher standards documented earlier, [](#figureA1) suggests---but does not prove---that manuscripts by junior female economists are disproportionately rejected.
Consistent with [](#table7), readability may actually decline during peer review. As discussed in [][NBER], this may be an artifact specific to abstracts, which are edited for length in addition to readability. Alternatively, writing (too) well upfront satisfies the review group with the highest initial readability threshold. Because referee reports reveal $s$ (and therefore $\widetilde R_i^s$), a readability decline after receiving an R&amp;R indicates that a majority of groups have laxer standards. This explanation is consistent with the theoretical model in [][SEUModel].
&lt;!--\label{Footnote132}--&gt;Alternatively, if desk rejection rates are gender neutral, authors subject to higher standards will undergo more arduous peer review. Greater scrutiny would therefore replace higher desk rejection rates when editors (or even referees) monitor and implement a policy of gender neutral acceptance rates.
Recall that the author recognises groups that reviewed his earlier papers even though the identities of the groups' individual members remain unknown to him.
Many thanks to Kevin Schnepel for suggesting this idea.
A possible exception is *Behavioral Ecology*, which increased its number of female first-authored papers after switching to double-blind review in 2001[#Budden2008a]. Whether that increase was due to bias or the universal upward trend in female authorship, however, has been somewhat controversial[#Budden2008b,Budden2008c,Webb2008,Whittaker2008].
Predictably, giving birth slows down poor review. The coefficient on motherhood, however, is consistently negative (indicating a productivity boost) and almost always highly significant when subjected to several robustness checks. This result is provocative and discussed in [][Duration]. I encourage interpreting it with caution, however, given (i) counterintuitive results; (ii) the analysis did not intend to measure motherhood’s impact on review times; and especially (iii) only a small number of mothers with young children are published in *Econometrica*.
Readability scores are highly correlated across an article's abstract, introduction and discussion sections[#Hartley2003a]. See [][Data] for further discussion.
For a discussion on the reliability of readability formulas, see [#DuBay2004;] and [][MeasuringReadability]. A sixth commonly used measure is the Lexile Framework. Because its formula and software are proprietary, I do not include it in the analysis.
Any female behaviour that also improves acceptance rates would result in female-authored papers being disproportionately represented in academic journals. They are not[#Ceci2014]. See [][alternatives] for further discussion on this point in the context of risk aversion.
This study investigates the revision process conditional on passing initial screening, at which point most manuscripts are ultimately accepted[#Duflo2016]. These conditions resemble [#Bartos2016;]'s "lemon-dropping" market---*i.e.*, average candidates are acceptable. In contrast, initial screening by scholarly journals more likely resembles "cherry-picking" markets, in that most manuscripts are not selected. Attention and labour market discrimination are inversely related in lemon-dropping markets, only.
While 1--6 percent seems small, it is based on a single paragraph. Assuming a similar standard applies to every paragraph in a paper and improving each one takes slightly more time, the accumulated impact may be substantial.
The underlying assumption driving the result is that women’s papers are more costly to evaluate. As discussed in [][AppendixModel], higher evaluation costs can be motivated by any of the three prominent theories of discrimination---*i.e.*, taste-based[#Becker1957], statistical[#Phelps1972,Arrow1973] or screening[#Cornell1996].
Predictably, giving birth slows down poor review. The coefficient on motherhood, however, is consistently negative (indicating a productivity boost) and almost always highly significant when subjected to several robustness checks. This result is provocative and discussed in [][Duration]. I encourage interpreting it with caution, however, given (i) counterintuitive results; (ii) the analysis did not intend to measure motherhood’s impact on review times; and especially (iii) only a small number of mothers with young children are published in *Econometrica*.
Any female behaviour that also improves acceptance rates would result in female-authored papers being disproportionately represented in academic journals. They are not[#Ceci2014]. See [][alternatives] for further discussion on this point in the context of risk aversion.
At a bare minimum, women would not be expected to write *more* clearly with greater exposure to peer review.
This study investigates the revision process conditional on passing initial screening, at which point most manuscripts are ultimately accepted[#Duflo2016]. These conditions resemble [#Bartos2016;]'s "lemon-dropping" market---*i.e.*, average candidates are acceptable. In contrast, initial screening by scholarly journals more likely resembles "cherry-picking" markets, in that most manuscripts are not selected. Attention and labour market discrimination are inversely related in lemon-dropping markets, only.
While 1--6 percent seems small, it is based on a single paragraph. Assuming a similar standard applies to every paragraph in a paper and improving each one takes slightly more time, the accumulated impact may be substantial.
The underlying assumption driving the result is that women’s papers are more costly to evaluate. As discussed in [][AppendixModel], higher evaluation costs can be motivated by any of the three prominent theories of discrimination---*i.e.*, taste-based[#Becker1957], statistical[#Phelps1972,Arrow1973] or screening[#Cornell1996].
Predictably, giving birth slows down poor review. The coefficient on motherhood, however, is consistently negative (indicating a productivity boost) and almost always highly significant when subjected to several robustness checks. This result is provocative and discussed in [][Duration]. I encourage interpreting it with caution, however, given (i) counterintuitive results; (ii) the analysis did not intend to measure motherhood’s impact on review times; and especially (iii) only a small number of mothers with young children are published in *Econometrica*.
Indeed, the discrimination documented in this paper could be described by a variety of theories: LIST.
[#Bloor2008;]'s analysis considers only full-time (or maximum part-time), salaried physicians in the U.K. Similar results are found in Canada and the U.S., where physicians are paid on a per-service basis[#CICH2005,Benedetti2004].
[#Seagraves2013;] find that normal houses (*i.e.* homes not sold under special sales conditions, such as foreclosures, fixer-uppers, corporate-owned properties, transfers and estate sales) sell at a significantly higher price when listed by a female real estate agent. While the authors do find buyers pay less if they are represented by a male agent, the effect is only present for homes sold under special sales conditions. In an earlier study, [#Turnbull2007;] did not find any significant gender difference in selling performance for listing and selling agents.
The evidence on general accident rates (including non-fatal accidents) is mixed. [#McFadden1996;] found no difference in female vs. male accident rates after adjusting for pilot experience and age. [#Walton2016;] found female accident rates were *higher* among inexperienced pilots but *lower* among experienced pilots.
See also [][Discussion] where I more thoroughly address the possibility that female-authored manuscripts are systematically lower quality than male-authored papers.
These findings are not universal. [#Schubert1999;] find evidence that men are more risk averse when lotteries are framed as losses.
That is, risk averse authors thoroughly respond to referee reports---losing time but enjoying better acceptance odds; risk neutral ones don’t---saving time but at greater risk of being rejected. Although it is not always true that risk averse individuals pay more to reduce risk[see, *e.g.*,\]\[][#Eeckhoudt1997,Langlais2005], this directionality is needed (on average) to claim risk aversion induces better writing in female-authored papers.
Please refer to the previous section for a more thorough review of the (substantial) prior research on gender neutrality and journals' acceptance rates. It too finds no female advantage in journals' acceptance rates.
Prior research has found peer review slightly improves the readability of articles and abstracts
Differences between men's and women's differences (columns 3 and 6) divided by men's difference.
A possible exception is *Behavioral Ecology*, which increased its number of female first-authored papers after switching to double-blind review in 2001[#Budden2008a]. Whether that increase was due to bias or the universal upward trend in female authorship, however, has been somewhat controversial[#Budden2008b,Budden2008c,Webb2008,Whittaker2008].
Any female behaviour that also improves acceptance rates would result in female-authored papers being disproportionately represented in academic journals. They are not[#Ceci2014]. See [][p3alternatives] for further discussion on this point in the context of risk aversion.
These explanations are motivated either by (i) fixed biological differences between the sexes; or (ii) misinformation about the costs of writing well. In the latter case, women update posterior believes about its benefits after successive rounds of peer review---*i.e.*, readability declines over the course of their careers. In the former, readability may decline---as women adapt to environmental expectations---or remain constant. With no benefit to better writing, however, it does not increase. See [][p3Alternatives] for a fuller discussion.
Members *within* a review group do not necessarily share these traits. For example, assume review groups are composed of two people and classified only by how well they know the author. Then every review group in which one member knew the author but the other didn't would be classified together.
That is, risk averse authors thoroughly respond to referee reports---losing time but enjoying better acceptance odds; risk neutral ones don't---saving time but at greater risk of being rejected. Although it is not always true that risk averse individuals pay more to reduce risk[see, *e.g.*,\]\[][#Eeckhoudt1997,Langlais2005], this directionality is needed (on average) to claim risk aversion induces better writing in female-authored papers. These findings are not universal. [#Schubert1999;] find evidence that men are more risk averse when lotteries are framed as losses.
These findings are not universal. [#Schubert1999;] find evidence that men are more risk averse when lotteries are framed as losses.
That is, risk averse authors thoroughly respond to referee reports---losing time but enjoying better acceptance odds; risk neutral ones don't---saving time but at greater risk of being rejected. Although it is not always true that risk averse individuals pay more to reduce risk[see, *e.g.*,\]\[][#Eeckhoudt1997,Langlais2005], this directionality is needed (on average) to claim risk aversion induces better writing in female-authored papers.</Comments>
        </Document>
        <Document ID="ED7F365B-08A0-413B-AD7D-9121D3DDACAD">
            <Title>Open review</Title>
        </Document>
        <Document ID="05289442-8D6A-45D1-9417-0B6549C0EDB2">
            <Title>Double-blind review</Title>
            <Text>Gender bias is possible only when authors' identity is known or can be reasonably guessed. Two factors make that feasible: non-blind review and Google. Do either exacerbate the readability gap between male- and female-authored abstracts? No and Yes. While the gap has grown with search engines' popularity and accuracy, double-blind review does not seem to dampen it. And it may make things worse.
Two journals---*QJE* and *AER*---employed double-blind review at some point during the time period covered by the data. *AER*\'s spell began 1 July, 1989 and ended 1 July, 2011. *QJE* used double-blind procedures until 1 June, 2005. *Econometrica*, *JPE* and *AER Papers &amp; Proceedings* have never blinded referees to papers' authors.
The first panel in [](#p3table9b) displays marginal effects of double-blind review by gender from [](#eq:p3eq5):where $\text{double-blind review}_j$ equals 1 if article $j$ was evaluated under double-blind refereeing and $\text{era}_j\text{ effect}$ splits data into three time periods: 1950--1969, 1970--1989 and 1990--2015. Male effects for male authors co-authoring with no females $(\beta_2)$; female effects for female authors co-authoring with no males $(\beta_2+\beta_3)$.
In the first panel, switching to double-blind review does not significantly affect the readability of female-authored abstracts. For whatever reason, however, it does affect men. When blindly evaluated, male-authored abstracts are up to !!DoubleBlindMan percent less clear. The effect is weakly significant for two out of five scores.
The last row of panel one displays gender differences. Because men write slightly sloppier under double-blind review but women do not, all statistics agree that a double-blind refereeing policy exacerbates the readability gap; three are significant.
The second panel of [](#p3table9b) re-estimates [](#eq:p3eq5) with primary *JEL* effects on *AER* abstracts published on or after 1990. The data therefore cover a single journal during periods of both double- and single-blind review. Figures support conclusions drawn from the first panel. Again, all readability scores---four of which are significant---confirm double-blind review exacerbates gender differences. On the source of that difference, four agree that male-authored abstracts are less readable (the Dale-Chall score suggests the opposite) while all five newly affirm female-authored abstracts are more readable when referees are blinded to authors' identities.
Gender differences in writing standards grow under double-blind review. This result dovetails with a finding in[#Blank1991;]. After a two-year controlled experiment whereby manuscripts submitted to *AER* were randomly subjected to either single- or double-blind review she found referees under the latter system rate female-authored articles more harshly. Although her result lacks significance it suggests referees are easier on women's papers as long as they know authors' gender. By one logical extension, female-authored manuscripts deserve more criticism because they are poorer quality.
A combination of lower quality research and less scrutiny of papers written by women may drive significance here. If women's manuscripts are not as good, referees, blinded or not, *should* peruse them more carefully---a byproduct of which could be better written papers after-the-fact or more attractive prose compensating for structural weaknesses before it. That only the blinded referees do might explain why they are harsher on male-authored manuscripts but easier on female-authored ones when gender is definitely known.
But I'll leave that question to physical scientists and instead offer an alternative explanation. I mean, who wants to believe biological features over which one has no control determine whether research is good or not? Not I, said the woman!
When blinded to authors' identities, reviewers may be more susceptible to "snap judgements" based on gender stereotypes than when they aren't. Referees that know authors' identities have a lot more information at their disposal when reviewing a paper---previous publications, current employment, education, etc. They also have the freedom to query colleagues on reputation, work ethic and general competency. All of this information impacts how papers are assessed---for example, "the author's identity might signal to a referee whether it is necessary to double-check technical details in a paper with a great deal of mathematical analysis"[\]\[p. 1042][#Blank1991].
Double-blind review makes overt information-gathering illegal but it does not stop referees from guessing authors' identities---which they did with surprising accuracy before the internet[#Blank1991], and presumably perfect accuracy after it---meaning superficial qualities are known but not complemented by more reliable indicators of reputation. Assuming referees are about as evolved as the rest of humanity, they underrate women and overrate men without extra evidence of the former's competency[#Foschi2000]. The end result may be harsher referee evaluations---and consequently better written text---of female-authored articles under double-blind review.</Text>
            <Comments>Specifically, single-blind review (the author is blinded to the referee's identity but not the reverse) or review under which the identities of both parties are mutually known.
&lt;!--\label{fnm:p3fn32}--&gt;From the beginning of May 1987 to the end of May 1989 *AER* subjected half of submitted papers to double-blind review, half to single-blind review[see\]\[][#Blank1991]. Because I do not know the process accepted manuscripts were reviewed under, all estimation results in this section omit these data.
*Econometrica* and *JPE* have always employed single-blind refereeing. Papers are chosen for *AER Papers &amp; Proceedings* by *AER*\'s president and/or American Economic Association committees. Neither are blinded to authors' identities.
\input{$P3/equations/eqn5.tex}
$\text{double-blind review}_j$ is 1 for articles published during an official policy of double-blind review. A final publication date, however, may substantially lag the actual review date[for an illustration and discussion, see\]\[][#Blank1991]. Because only *AER* articles published post May 1989 are included in the analysis (see &lt;!--\autoref{fnm:p3fn32}--&gt;) and all *QJE* articles published before June 2005 were submitted to double-blind review, the number of articles subjected to non-blind review incorrectly classified as having undergone blind review should be small. The second panel of [](#p3table9b) further limits misclassification by including only *AER* articles published post 1990.
Year effects were replaced due to multicollinearity between $\text{double-blind review}_j$ and the combination of year and journal dummies.
\input{$P3/tables/tex/table9b.tex}
Unless journals have an explicit policy of publishing female-authored research over male work of equal quality---which may be the case---because all prior evidence points to gender neutral *acceptance rates* and there appears to be no gender difference in citations per article except in Norway[#Ceci2014,Aksnes2011], women's research is probably just as good as men's.
I use "snap judgement" to refer to quick decisions made without deep investigation or logical deliberation. It is akin to choices made with "System 1" in[#Kahneman2011;].</Comments>
        </Document>
        <Document ID="3FC7E572-E16E-49F4-8CD6-0F3C88A18D78">
            <Title>Untitled</Title>
        </Document>
        <Document ID="8BCB8FD2-8ED9-4A76-8887-A627F4E9F7AE">
            <Title>Proofs</Title>
            <Text>Each proof is restricted to only those results not otherwise shown in the text of the previous section. No separate proof is needed for p[](#prop3).</Text>
            <Comments>\input{$PPATH/p1/proofs/proofs.tex}</Comments>
        </Document>
        <Document ID="F3136EA2-10B7-4EC2-8AA1-43B66FA0BA1E">
            <Title>Convex female ratio</Title>
            <Text>As discussed in [][p3authorlevel], [](#p3table6) suggests an increasing, convex relationship between female ratio and readability. The following tables restrict samples to male authors and apply several increasing, convex transformations to female ratio based on [](#p3equationAX). The plot of [](#p3equationAX) at different values of $x$ is shown in X; as it illustrates, [](#p3equationAX) assumes little or no relationship between readability and female ratio when few or no co-authors are women; past some threshold, however, the relationship increases exponentially.
</Text>
            <Comments>\input{$PPATH/p3/equations/p3equationAX}</Comments>
        </Document>
        <Document ID="368582DB-A926-4377-A113-433CA3E708E6">
            <Title>Author seniority</Title>
            <Text>To account for author seniority, I control for an author's number of top-five (*AER*, *Econometrica*, *JPE*, *QJE* and *REStud*) publications at the time a paper was published. For co-authored articles, only the data corresponding to the most prolific author is used.</Text>
        </Document>
        <Document ID="610A19BE-EEC0-447E-9D4F-A6F55990F6DD">
            <Title>Unadjusted $R_{it}$</Title>
            <Text>[](#table8_R) and [](#figure8_R) replicate the analysis in [][MatchingResults] but [](#equation14) does not adjust for the ratio of female authors on a paper.
&lt;!--
\vfill
\input{$HOME/Dropbox/Readability/draft/0-tex/generated/Table-9-R}
\vfill
\input{$HOME/Dropbox/Readability/draft/0-tex/fixed/Figure-5-R}
\vfill
\clearpage
--&gt;</Text>
        </Document>
        <Document ID="15222CF6-76C0-4039-8B17-F0666374F200">
            <Title>Adjudication</Title>
            <Text>Bankruptcy occurs only if the entrepreneur and his creditor cannot agree whether to continue operating or liquidate the firm. Both parties expect strictly higher returns under separate outcomes. As illustrated in [](#lem1) and its [proof](#p1appendixproofs), the concavity of creditor returns and convexity of entrepreneurs' bind the latter to reorganisation, the former to liquidation.
LEMMA
In bankruptcy, the entrepreneur prefers to reorganise the project; the creditor to liquidate it.
elem
Guided by legislation, a judge settles the conflict. Insolvency law calls for reorganisation when it is "reasonably likely"[#HarrisSimmons1989] to maintain "the survival of the company, and the whole or any part of its undertaking, as a going concern"[\]\[§2(3)(a)][#GreatBritain1986]. Specifically, viable projects---projects with continuation value greater than their assets' piecemeal resale value---are reorganised, *i.e.*,
$$!e[p1equation1]V_1^L&lt;\ol V_1^C,e!$$
 where $\ol V_1^C$ is the time 1 expected value of $V_2^C$. When the reverse is true, the project is non-viable: unlikely to survive as a going concern without "unnecessarily harm[ing] creditors as a whole"[][#HarrisSimmons1989]. Non-viable projects are liquidated at time 1.
The judge does not know whether a project before her is viable. Although the creditor and entrepreneur do, per [](#lem1) the latter has an incentive to present evidence that the project should be reorganised while the former will argue just as forcefully for liquidation. The judge, meanwhile, gathers publicly available material on the project's assets and earnings and independent market research on similar projects in the same industry. All together, this information colours her opinion of the firms' true value and forms the basis of her ruling. That ruling is correct with probability $p$, where $p\in(\sfrac{1}{2},1)$ depends on the quality and veracity of the documentation provided by the entrepreneur and creditor as well as the intelligence the judge gathers herself. Creditors' expected returns just before her official ruling are
$$\ol C_1^B=q\,\ol C_1^R+(1-q)\,C_1^L,$$
 where $q=p$ if the project is a viable one, $1-p$ if it isn't and $\ol C_1^R$ is the time 1 expected value of $C_2^R$.
Even with perfect information the judge's decision is biased in favour of reorganisation. $\ol V_1^C$ drives her ruling ([](#p1equation1)), but the time 1 expected value of $V_2^R$, $\ol V_1^R$, really determines whether a bankrupt project is viable or not. Only when $\ol V_1^R$ exceeds $V_1^L$ does the project's true value---including reorganisation costs---surpass the principal amount of the loan. Since $\ol V_1^R&lt;\ol V_1^C$, too few bankrupt firms are liquidated and too many are reorganised.</Text>
            <Comments>British administration orders are granted on even weaker terms. The court must only be satisfied that the project has a "real prospect of" or "good, arguable case for" profitability---*i.e.*, less than the balance of probabilities[][#HarrisSimmons1989].
$p$ is an inverse function of the variance of information available on the distribution of time 2 earnings. When financial records provided by the firm are accurate---*e.g.*, because disclosure laws are strong---and significant outside information exists on similar firms---*e.g.*, because the financial analysis sector is well developed---the variance of information is low; $p$ is close to 1. When the opposite is true, the variance of information is high; the judge's decision is random ($p=\sfrac{1}{2}$).
&lt;!--\label{p1footnote1}--&gt;Theoretically, the judge could base her decision either wholly or partially on [](#p1equation1). However, insolvency laws rarely (or only vaguely) reference such costs, giving judges little scope or even desire to adjust their rulings[*e.g.*, judges may prefer to rule in line with legal precedent to prevent being overturned on appeal; see\]\[][#Gennaioli2008]. Additionally, given reorganisation's costs are difficult to quantify and tend to occur long after a judge has ruled in a particular case, she has few opportunities to educate herself on their extent and incorporate them into future rulings. Indeed, she may not even wish to make an accurate decision to begin with. Forum-shopping and judges' desire to attract high-profile bankruptcy cases may lead to a preference for reorganisation over liquidation[][#Gennaioli2010].</Comments>
        </Document>
        <Document ID="001E946F-302B-4710-AA2D-EFF1F23C5F5B">
            <Title>Results</Title>
        </Document>
        <Document ID="908E044B-E27B-4AEC-934D-FE4BF8C2FC1D">
            <Title>[](#table10), male effects</Title>
            <Text> [](#tableC17) shows $\widehat R_{k3}$ for men in the matched sample. Grade-level effects (Flesch-Kincaid, Gunning Fog, SMOG and Dale-Chall) have been multiplied by [negative one][MeasuringReadability].
&lt;!--\input{$HOME/Dropbox/Readability/draft/tex/generated/tableC17}</Text>
        </Document>
        <Document ID="AA128736-711D-460A-9A46-DD744F260D9D">
            <Title>preface</Title>
        </Document>
        <Document ID="0D157A8A-4F7E-4FAC-BA03-572162E1537B">
            <Title>[](#table6_FemRatio), full output (first and final columns)</Title>
            <Text>[](#table6_full) displays coefficients from estimating [](#equation2) using OLS. The first row displays coefficients on working paper score ($R_{jW}$); the second row shows the coefficient on female ratio (papers with fewer than 50 percent female authors are classified as male, see [][Gender]); the third rows shows the coefficient on the interaction between blind review and the ratio of female authors on a paper. (All three coefficients are also shown in the first panel of [](#table6_FemRatio).) Remaining rows present estimated coefficients on the other (non-fixed effects) control variables: $N_j$ (number of co-authors), $\text{max. }t$ (author seniority), $\text{max. }T$ (author prominence), number of citations (asinh) and dummy variables equal to one if article $j$ is authored by at least one native English speaker or is classified as theory, empirical or other. Similarly, [](#table6_change_full) displays coefficients from estimating [](#equation3). The coefficients on female ratio and its interaction with blind review correspond to estimates presented in the second panel of [](#table6_FemRatio).
As discussed in [][NBERResults], we do not observe the citations papers would have received had they not undergone peer review. Nevertheless, [](#table6_full) suggests a negative (albeit insignificant) relationship between published readability *conditional* on draft readability; [](#table6_change_full) suggests a negative (but again insignificant) relationship between citations and the change in readability that occurs *during* peer review. Thus, they tentatively suggest that the readability revisions women are asked to make during peer review may not ultimately improve the quality of their papers.
&lt;!--
\input{$HOME/Dropbox/Readability/draft/0-tex/generated/Table-G.1}
\input{$HOME/Dropbox/Readability/draft/0-tex/generated/Table-G.2}
\clearpage
--&gt;</Text>
            <Comments>The coefficient on citations is negative ***only*** when controlling for $R_{jW}$ or using the change in readability as the dependant variable. Otherwise, readability positively correlates with both working paper and published paper readability (see [][AppendixMeasurementError]; results for draft readability and using the specific sample from [](#table6_FemRatio) available on request).</Comments>
        </Document>
        <Document ID="577D95F7-966E-46C5-A4EB-C923F194752D">
            <Title>347 (alastair’s macbook's conflicted copy 2016-10-06)</Title>
            <Text>The data pre- and post-peer review make it possible to identify the contemporaneous effect of gender on peer review by eliminating selection bias---*i.e.*, the effect of omitted variables that are independent of immediate peer review, but correlated with gender. To isolate gender differences in readability pre-existing peer review from those incurred during it, I use two separate (yet analogous) estimation strategies based on a conceptual framework outlined by[#Ashenfelter1994;].  The first strategy removes selection effects by regressing female ratio on the change in score via OLS. In the second strategy, selection effects are eliminated by differencing coefficients on female ratio from a feasible, generalised least squares (FGLS) regression on abstract readability of NBER working papers and, separately, their published versions.
[](#p3equationAX1) defines the linear relationship between readability scores and non-peer reviewed articles. Its components shape writing clarity *before* external evaluators demand edits.
&lt;!--\begin{equation}\label{p3equationAX1}
\text{score}_{jW}^s=\beta_0+\beta_1\,\text{female ratio}_j+\bm\uptheta\,\vect X_j+\mu_j+\vep_{jW},
\end{equation}--&gt;
 where $\text{score}_{jW}^s$ is readability score $s$ for the working paper version ($W$) of article $j$, $\vect X_j$ is a vector of version-invariant variables (publication year and productivity effects) and $\mu_j$ are article fixed effects. $\vep_{jW}$ is version $W$'s unobserved error term.
Published articles are obviously revised versions of earlier papers. Their readability depends on $\text{score}_{jW}^s$ as well as factors that affect writing clarity anytime after drafts are released as working papers. This relationship is shown in [](#p3equationAX2).
&lt;!--\begin{equation}\label{p3equationAX2}
\text{score}_{jP}^s=\text{score}_{jW}^s+\beta_0^P+\beta_1^P\,\text{female ratio}_j+\bm\upzeta\,\vect Z_{jP}+\vep_{jP},
\end{equation}--&gt;
 where $\beta_0^P$ is an additive constant specific to article $j$'s published version ($P$), $\beta_1^P$ reflects an additional impact $\text{female ratio}_j$ may have in peer review and the vector $\vect Z_{jP}$ contains factors that only influence version $P$ (editor, journal and journal-year interaction effects). $\vep_{jP}$ is an error term specific to version $P$; $\vep_{jP}$ and $\vep_{jW}$ are uncorrelated.
The parameter of interest is $\beta_1^P$. $\beta_1^P$ is not separately identified in [](#p3equationAX2); furthermore, OLS estimates are biased by article fixed effects. Subtracting [](#p3equationAX1) from [](#p3equationAX2) deals with both issues. Fixed effects are removed; the coefficient on $\text{female ratio}_j$ is specific to peer review:
&lt;!--\begin{equation}\label{p3equationAX3}
\text{score}_{jP}^s-\text{score}_{jW}^s=\,\beta_0^P+\beta_1^P\,\text{female ratio}_j+\bm\upzeta\,\vect Z_{jP}+\vep_{jP}.
\end{equation}--&gt;
 [](#p3equationAX3) is estimated via OLS.
A second method obtains $\beta_1^P$ in stages. As mentioned, correlation between $\mu_j$ and the observable variables biases OLS parameter estimates of [](#p3equationAX1) and [](#p3equationAX2). To define that correlation, I adopt the general structure from[#Ashenfelter1994;]:
&lt;!--\begin{equation}\label{p3equationAX4}
\mu_j=\eta\,\text{female ratio}_j+\bm\updelta\,\vect X_j+\bm\upgamma\,\vect Z_{jP}+\omega_j,
\end{equation}--&gt;
 where $\omega_j$ is uncorrelated with $\text{female ratio}_j$, $\vect Z_{jP}$ and $\vect X_j$. Substituting [](#p3equationAX4) into [](#p3equationAX1) generates the following reduced form representation of [](#p3equationAX1):
&lt;!--\begin{equation}\label{p3equationAX5}
\text{score}_{jW}^s=\beta_0+\beta_1^\prime\,\text{female ratio}_j+\wt{\bm\uptheta}\,\vect X_j+\bm\upgamma\,\vect Z_{jP}+\vep_{jW}^\prime,
\end{equation}--&gt;
 where $\beta_1^\prime=\eta+\beta_1$, $\wt{\bm\uptheta}=\bm\uptheta+\bm\updelta$ and $\vep_{jW}^\prime=\omega_j+\vep_{jW}$. To obtain a reduced form for published article readability, substitute [](#p3equationAX5) into [](#p3equationAX2):
&lt;!--\begin{equation}\label{p3equationAX6}
\text{score}_{jP}^s=(\beta_0+\beta_0^P)+(\beta_1^\prime+\beta_1^P)\,\text{female ratio}_j+\wt{\bm\uptheta}\,\vect X_j+(\bm\upzeta+\bm\upgamma\vect)\,\vect Z_{jP}+\vep_{jP}^\prime
\end{equation}--&gt;
 where $\vep_{jP}^\prime=\vep_{jW}^\prime+\vep_{jP}$. [](#p3equationAX5) and [](#p3equationAX6) are explicitly estimated via FGLS. $\beta_1^P$ is identifiable post-estimation by subtracting reduced form coefficients.</Text>
        </Document>
        <Document ID="D6DE499D-5502-4E75-9D27-C7558A0DF419">
            <Title>Robustness</Title>
        </Document>
        <Document ID="F97F3B14-C5E0-4BC4-8E55-A73A9617216A">
            <Title>Mechanisms</Title>
        </Document>
        <Document ID="7BC6299C-6393-4E8D-9B04-050656159485">
            <Title>[](#tableH2_FemRatio), tests of coefficient equality</Title>
            <Text>
[](#tableH3) tests equality of coefficients in each column of [](#tableH2_FemRatio). It rejects equality between coefficients in the first and third columns at $p&lt;0.05$ for the Flesch Reading Ease, Flesch-Kincaid, Gunning Fog and SMOG scores.
&lt;!--
\input{$HOME/Dropbox/Readability/draft/0-tex/generated/Table-I.3}
\clearpage
--&gt;</Text>
        </Document>
        <Document ID="6B416027-752E-4EE0-AADB-3EEA6C218A10">
            <Title>Description of control variables</Title>
        </Document>
        <Document ID="7EC30A3C-D3B1-4696-8D15-77F10174FB53">
            <Title>Conclusion</Title>
            <Text>Female-authored articles published in top economics journals are better written than similar papers by men. A large part of this gap originates in peer review. To identify the causal mechanism, I derive an empirical test from a subjective expected utility model of an author's decision-making process. The resulting estimate suggests higher standards in peer review cause senior female economists to write at least seven percent more clearly than they otherwise would.
I also document evidence that sheds more light on the impact higher standards have on women's choices and their productivity measurement: (i) female-authored papers spend more time under review; (ii) women appear to quickly adapt to higher standards *in* peer review by adjusting their behaviour *before* it. The latter emphasises that observational data almost always capture discrimination in equilibrium; to identify it, one must determine where the equilibrium would have been had discrimination not occurred. The former suggests higher standards could play a role in academia's "Publishing Paradox".
If higher standards apply elsewhere in the economy they may also rationalise many instances of lower female output: work that is evaluated more critically *at any point in the production process* will be systematically better (holding prices fixed) or systematically cheaper (holding quality fixed). This reduces women's wages---for example, if judges require better writing in female-authored briefs, female attorneys must charge lower fees and/or under-report hours to compete with men---and distorts measurement of female productivity---billable hours and client revenue decline; female lawyers appear less productive than they truly are.
Finally, this study exploits publicly available data to identify and evaluate discrimination's impact on those who are discriminated against. But it is narrowly focused and---because it doesn't study the bias's source---proposes few solutions. More research is needed to better understand how and where higher standards for female authors emerge in peer review. Future studies could apply a similar theoretical framework and more holistic measure of research quality---*e.g.*, citations---to these and other economics journals. Readability scores analysed using a similar setup could also evaluate asymmetry anywhere ideas are communicated orally or in writing and large amounts of source material are easily obtainable---journalism, speeches, student essays, business plans, Kickstarter campaigns, *etc.* But to answer the deeper questions this paper raises requires confidential data on the editorial process---including access to referee reports. I hope journals are challenged to be candid about bias and willing to support the access and research needed to better understand it.</Text>
        </Document>
        <Document ID="B574767E-5F2C-433A-8F56-2F2577254B6C">
            <Title>Semi-blind review</Title>
            <Text>[](#table6_FemRatio) tentatively suggests double-blind review may have successfully reduced peer review's impact on the gender readability gap *before* the internet. In this appendix, I show evidence (also tentative) suggesting that it may have been less effective *after* the internet. In particular, I re-estimated [](#equation3) on the sample of articles published in or after 1998 and defined a dummy variable equal to one if a journal had in place an official policy of double-blind review at the time a paper was published. The coefficients on female ratio (papers with fewer than 50 percent female authors are classified as male, see [][Gender]) and its interaction with the redefined indicator of blind review are presented in [](#table7_semiblind). The coefficient on female ratio reflects the gender gap in journals that did not have in place an official policy of double-blind review (*i.e.*, *Econometrica*, *JPE*, *QJE* after 2004 and *AER* after 2011). The coefficient on the interaction between female ratio and blind review reflects the gender gap in journals that did have an official policy of blind review (*i.e.*, *QJE* before 2004 and *AER* before 2011).
Four out of five scores suggest a positive gender readability gap among both blindly reviewed papers and non-blindly reviewed papers published after the advent of the internet. Thus, it does not appear that blind review was able to alleviate the gender readability gap once referees could easily determine authors' identities by simply Googling them.
Editors knew submitting authors' identities---and therefore genders---both before and after the internet as well as under single- and double-blind review. Thus, the reversed gap in double-blind review [pre-internet](#table6_FemRatio) and positive gap [post-internet](#table7_semiblind) may suggest bias from referees---as opposed to editors---drives observed gender differences in readability. Nevertheless, this conclusion is based on noisy (often insignificant) estimates and should therefore only be made cautiously.
&lt;!--
\input{$HOME/Dropbox/Readability/draft/0-tex/generated/Table-G.4}
\clearpage--&gt;</Text>
            <Comments>Many thanks to an anonymous referee for suggesting this idea.</Comments>
        </Document>
        <Document ID="2D4500FC-774E-4302-851C-5B8EDFA71D17">
            <Title>New Folder</Title>
        </Document>
        <Document ID="C17ED5AA-791B-418B-A8AF-022A6F9AF7B0">
            <Title>NBER working paper release dates vis-à-vis Journal submission dates</Title>
            <Text>As shown in [][NBERResults], female-authored papers are more likely to be released as NBER Working Papers *after* the underlying paper was already submitted to *Econometrica*. Here, I show that this conclusion is robust to controlling for year fixed effects. Specifically, I regress the ratio of female authors on the time difference (in months) between a paper's NBER Working Paper release and submission to *Econometrica*. The coefficient on female ratio from this regression is 6.81 months (robust standard error 4.31), indicating that the female ratio of an article is correlated with later NBER release.
[](#figure8) plots gender differences over year. The horizontal grey line denotes that the paper was submitted to *Econometrica* and released as an NBER Working Paper at the same time. Values below this line indicate that papers in a year were, on average, released as an NBER Working Paper *before* they were submitted to *Econometrica*; values above this line indicate that papers in a year were, on average, submitted first to *Econometrica* and then released as an NBER Working Paper. Points in blue are effects estimated for 100 percent male-authored papers; points in pink are effects estimated for 100 percent female-authored papers.
&lt;!--\input{$HOME/Dropbox/Readability/draft/tex/fixed/figure8}--&gt;
As can be seen in [](#figure8), men and women both generally release their papers as NBER working papers only *after* they've already submitted the paper to *Econometrica*. For women, however, this delay is more pronounced.
&lt;!--\clearpage--&gt;</Text>
        </Document>
        <Document ID="10F9894E-55C7-4312-92A8-7A7F80C9058D">
            <Title>Alternative program for calculating readability scores</Title>
            <Text>In this section, I replicate [](#table3_FemRatio), [](#table6_FemRatio), [](#tableH2_FemRatio) and [](#table4_FemRatio) using readability scores generated by the [R `readability` package](https://github.com/trinker/readability), an alternative program for calculating Flesch-Kincaid, Gunning Fog and SMOG readability scores. Replications for other tables and figures presented in the paper are not shown, but will be made available on request.
`Textatistic` and `readability` employ different strategies to adapt the scores to automated calculation---*e.g.*, `readability` counts semi-colons and dashes as sentence-ending terminations; `Textatistic` does not. Results appear robust to these (and other) small discrepancies: coefficients are similar to those presented in the body of the paper.
&lt;!--
\input{$HOME/Dropbox/Readability/draft/0-tex/generated/Table-3-R}
\clearpage
\input{$HOME/Dropbox/Readability/draft/0-tex/generated/Table-5-R}
\clearpage
\input{$HOME/Dropbox/Readability/draft/0-tex/generated/Table-8-R}
\clearpage
\input{$HOME/Dropbox/Readability/draft/0-tex/generated/Table-F.2-R}
\clearpage
--&gt;</Text>
            <Comments>Readability scores were originally developed to be calculated by hand. Automating their calculation requires slightly adapting the algorithms. For example, all five scores define sentences as grammatically independent units of thoughts---*e.g.*, two independent clauses connected by a dash or semi-colon count as two separate sentences. Unfortunately, semi-colons and dashes are frequently used in other ways and it is difficult to programmatically distinguish between contexts.</Comments>
        </Document>
        <Document ID="AD958E6B-34B0-4744-92D8-5CF04C4C3863">
            <Title>Condition 3</Title>
        </Document>
        <Document ID="9CCC98A2-CA39-4B40-BFE1-A8A3E0BD9FC5">
            <Title>Supplemental analysis</Title>
        </Document>
        <Document ID="D2F71FD6-EE1C-49FF-938C-E65C11B8E95D">
            <Title>Extensive form</Title>
            <Text>[](#p1figureC3) is an extensive form representation of the bankruptcy game. This representation is not unique---the actions and [proofs][p1appendixproofs] are also consistent with a game in which the entrepreneur files for bankruptcy and/or the creditor proposes to liquidate or enter a workout. Complete details on assumptions and legal rationale behind actions in bankruptcy are outlined in [][p1model]; a brief description is provided in the notes.</Text>
            <Comments>\input{$PPATH/p1/figures/tex/p1figureC3}
It is still assumed the entrepreneur has all the bargaining power when negotiating a workout, thus if the creditor makes the workout offer, it would satisfy [](#lem2). For a full discussion on how the entrepreneur's bargaining power changes the game, see [][p1appendixbargainingpower].</Comments>
            <Notes>&lt;!--\end{appendices}--&gt;</Notes>
        </Document>
        <Document ID="2A425993-9056-4E20-BF99-9D618B607519">
            <Title>Dissertation</Title>
        </Document>
        <Document ID="29A31DD2-C169-4A85-822A-D29E31F6BA9F">
            <Title>Alternative thresholds for \\(\text{mother}_j\\)</Title>
            <Text>[](#table10_thresholds) repeats the regression pre\\-sented in [](#table10_FemRatio) column (5) using alternative age thresholds to define motherhood: $\text{mother}_j$ equals 1 if paper $j$'s co-authors are all mothers to children younger than three (first column), four (second column), *etc.* Changing this threshold has little effect on female ratio's coefficient. The coefficients on $\text{mother}_j$ and $\text{birth}_j$ are persistently negative and positive (respectively), although magnitudes and standard errors vary. Remaining coefficients are unaffected.
&lt;!--
\input{$HOME/Dropbox/Readability/draft/0-tex/generated/Table-H.3}
\clearpage
--&gt;</Text>
        </Document>
        <Document ID="DAEFBE26-2900-4C78-AD90-93AAA4B9DB95">
            <Title>Determining gender</Title>
        </Document>
        <Document ID="2608C4D4-3A6F-4D5D-80BD-764A4848ACBA">
            <Title>Gender</Title>
            <Text>Authors were initially assigned a gender using [GenderChecker.com](http://genderchecker.com)'s database of male and female names. Three separate Mechanical Turk workers, a research assistant or I then manually verified them based on photos and other information found on faculty websites, Wikipedia articles, *etc.* In situations where the author could not be found but several people with the same first and last name were and all shared the same gender, the author was also assigned that gender. For the remaining cases, I emailed or telephoned colleagues and institutions associated with the author.
Determining the "gender" of a paper is not nearly as straightforward. For solo-authored manu\\-scripts---of which there are 4,014 in the sample---gender corresponds to the sex of the author. As discussed in [][Underrepresentation], however, top economics journals have collectively published just 266 by women. Only a slightly larger number were written entirely---*or even mostly*---by women.
&lt;!--\input{$HOME/Dropbox/Readability/draft/0-tex/fixed/Figure-1}--&gt;
Instead, I assume an article's gender is related to its proportion of female authors. A gender readability gap---if it exists---is presumably a function of (i) the probability a passage of text was written and/or revised by a female co-author; and (ii) referees' beliefs about female authors' contributions to the writing and/or revision of a co-authored paper. Prior research suggests co-authors---regardless of seniority---share responsibility for writing and (especially) revising collaborative work[see, *e.g.*, \]\[][#Hart2000a,Kumar2016]. Thus, the intersection of (i) and (ii) is likely positively related to the ratio of female authors on a paper.
[](#figureX) corroborates this hypothesis. It plots abstract readability against a paper's ratio of female authors. The slope of the regression line is positive, relatively large (1.88 points on the Flesch Reading Ease scale) and statistically significant; however the relationship is not entirely linear. In particular, it appears to be close to zero when women make up fewer than 50 percent of authors and increasing in the share of female authors only after that. For this reason, I define papers with a strict minority of female authors as male-authored; for papers with 50 percent or more female authors, I allow an article's gender to increase linearly with its proportion of female authors. For robustness, however, I also repeat most analyses (a) on the sample of solo-authored papers, only; (b) comparing papers with a senior female co-author to entirely male-authored papers; (c) on the subset of papers authored by a single gender; (d) using a binary variable equal to one if at least one author is female; and (e) using a binary variable equal to one if at least half of all authors are female. Standard errors from (a) and (c) tend to be larger; those from (b), (d) and (e) usually similarly sized or smaller. In general, however, results do not meaningfully [change][AppendixAlternativeMeasure].</Text>
            <Comments>313 papers in the sample were authored entirely by women. Women made up more than 50 percent of all authors in another 47. In 35 observations, a woman was the lead author---*i.e.*, the first author was female in a paper with authors listed non-alphabetically or in which contributions were explicitly noted.</Comments>
        </Document>
        <Document ID="EAAB808A-4643-4CB6-A498-D080AD766950">
            <Title>Anonymity</Title>
            <Text>Gender bias is possible only when authors' identity is known or can be reasonably guessed. Two factors make that feasible: non-blind review and Google. Do either exacerbate the readability gap between male- and female-authored abstracts? No and Yes. While the gap has grown with search engines' popularity and accuracy, double-blind review does not seem to dampen it. And it may make things worse.</Text>
            <Comments>Specifically, single-blind review (the author is blinded to the referee's identity but not the reverse) or review under which the identities of both parties are mutually known.</Comments>
        </Document>
        <Document ID="FA3C24A8-E42C-42CC-A1CE-FEED91EE1CCD">
            <Title>Bankruptcy.</Title>
            <Text>If the entrepreneur and lender cannot see eye-to-eye on whether to restructure their debt or liquidate the project, each petitions the court to force the other to accept his desired result---reorganisation or compulsory liquidation. A judge adjudicates.
Reorganisation---administration in the U.K.---is a supervised version of continuation. The entrepreneur formulates a restructuring plan; a court-appointed administrator approves and oversees its implementation.
As discussed in the introduction, reorganisation is expensive. Legal and accounting fees, lost customers, suppliers and employees, added bureaucracy and even theft from fraudulent administrators add up. The upshot is delayed production, asset depreciation and lower profits. $Y\in(0,1)$ captures this. It measures the fraction of project value reorganisation wastes. What's left is only
$$V_2^R=X_1+(1-Y)\l(X_2+K_2\r).$$
Compulsory liquidation is the second option. In theory, compulsory liquidation is more transparent, straightforward and faster than reorganisation. Service and employment contracts are immediately voided and business operations cease. Secured creditors repossess their liens; a court-appointed administrator auctions off remaining assets and distributes the proceeds to unsecured creditors. Any excess is settled on equity.
Winding up a business is less ambiguous than rescuing it---giving corruption, bureaucracy and legal fees less scope to eat away a firm's eventual liquidation value. And since that value is independent of unrealised future earnings, bankruptcy's impact on business reputation is especially irrelevant. Compulsory liquidation, therefore, probably wastes a smaller fraction of a firm's time 1 liquidation value than reorganisation wastes of its time 2 continuation value.
To incorporate this idea without adding unnecessary complexity, I assume compulsory liquidation incurs no added cost and is instantaneous. Gross returns are therefore identical to those in voluntary liquidation.
Whether reorganised or liquidated, the creditor is no longer guaranteed the full face value of his loan. Given $X_1&lt;D$, neither $V_1^L$ nor $V_2^R$ is necessarily large enough to cover $D$. When it isn't, the creditor takes home the entirety of the project's gross value; otherwise, he earns $D$. His gross returns in liquidation and reorganisation are, respectively
$$C_1^L=\min\{D,V_1^L\}\eqOr C_2^R=\min\{D,V_2^R\}.$$
As before, the entrepreneur keeps whatever remains: $E_1^L=V_1^L-C_1^L$ if the project is liquidated and $E_2^R=V_2^R-C_2^R$ if it's reorganised.</Text>
            <Comments>In Germany and France, control cedes to a court-appointed administrator; in the U.S., it remains with existing managers. In most countries payments cease on outstanding loans and an automatic stay---lasting anywhere from three months in Germany to over a year in France---is applied to secured claims. The firm can usually obtain new financing, often at terms more favourable than existing debt. After a certain period---at least four months in the U.S. and more than 18 in France---a plan is proposed to restructure debt and reorganise the firm. In the U.S., this period can be extended indefinitely by the bankruptcy court. Creditors vote on it and the court approves it; in some jurisdictions equity holders also have a say. In the U.S. a judge can impose a plan already rejected by creditors. In France, creditors have no vote; only the court decides whether the plan is implemented.
Generally, these proceeds are distributed according to legally defined absolute priority rules. For example, in the U.S., administrative and legal fees incurred during proceedings are paid first; next, statutory claims, including unpaid taxes and wages; finally, unsecured debt. Evidence from the U.S. suggests this ordering is very rarely violated when firms are liquidated in Chapter 7[#Bris2006].
It is empirically difficult to disentangle the cost of reorganisation from the cost of liquidation. Evidence from the U.S. suggests that legal and accounting fees are roughly eqivalent[#Ferris1997,Ferris2000], but recovery rates in Chapter 7 are lower than those in Chapter 11[#Weiss1990,Bris2006]. (Because these studies cannot compare Chapter 7 and Chapter 11 recovery rates for the same firm, however, the data are not especially informative on the actual cost of each regime. Additionally, recovery rates in Chapter 7 partially incorporate losses incurred in Chapter 11 given most firms that are eventually liquidated previously attempted a reorganisation.) In contrast, empirical work conducted in Italy and the U.S. after each country introduced or significantly expanded reorganisation procedures suggest creditors anticipate lower returns when reorganisation is more likely[#Rodano2012,Scott1986]. Indeed, emerging markets with weak institutions and severe restrictions on reorganisation appear far more capable of recovering creditors' claims than their peers with more generous procedures[#Djankov2008].
&lt;!--\label{p1footnote2}--&gt;In general, however, a nominal court fee may be due---£200 in the U.K.---and both sides undoubtedly incur legal and administrative fees they otherwise wouldn't be subject to if the liquidation were voluntary. For example, in the U.K., the party petitioning the court for compulsory liquidation must advertise in the press a number of days before the process takes place.</Comments>
        </Document>
        <Document ID="9E9E5009-40A1-40C8-B32E-0F718FDC0797">
            <Title>dissertation</Title>
            <Text>Latex input: dissertation-header
Title: &lt;$projecttitle&gt;
Author: &lt;$author&gt;
Abstract One: &lt;$synopsis&gt;
Abstract Two: &lt;$synopsis&gt;
Abstract Three: &lt;$synopsis&gt;
My Preface: This dissertation is the result of my own work and includes nothing which is the outcome of work done in collaboration. It is not substantially the same as any that I have submitted, or, is being concurrently submitted for a degree or diploma or other qualification at the University of Cambridge or any other University or similar institution. I further state that no substantial part of my dissertation has already been submitted, or, is being concurrently submitted for any such degree, diploma or other qualification at the University of Cambridge or any other University of similar institution. Finally, this dissertation does not exceed the prescribed word limit for the relevant Degree Committee.
Base Header Level: 2
Latex input: dissertation-begin
Latex footer: dissertation-footer</Text>
        </Document>
        <Document ID="252AE959-2A15-4A79-992A-A83F2FDC3EEB">
            <Title>Untitled</Title>
        </Document>
        <Document ID="80CBCD83-081D-4233-ACA7-DA39005D272C">
            <Title>Tables</Title>
            <Text>The following tables display full output used to generate marginal effects in [][p3Results] (for space considerations, [](#p3table8)'s full output is not provided).</Text>
            <Comments>\input{$P3/tables/tex/table4a_1.tex}
\input{$P3/tables/tex/table4a_2.tex}
\input{$P3/tables/tex/table4a_3.tex}
\input{$P3/tables/tex/table4a_4.tex}
\input{$P3/tables/tex/table4a_5.tex}
\input{$P3/tables/tex/table5a.tex}
\input{$P3/tables/tex/table6a.tex}
\input{$P3/tables/tex/table9a_1.tex}
\input{$P3/tables/tex/table9a_2.tex}
\input{$P3/tables/tex/table10a_1.tex}
\input{$P3/tables/tex/table10a_2.tex}</Comments>
        </Document>
        <Document ID="C0314BE2-F81B-44F7-8F5F-BCCA3DE230AB">
            <Title>Introduction</Title>
            <Text>Firms that need large capital outlays may not get credit any time insolvency involves reorganisation. When those procedures are expensive, however, even projects with low upfront investment needs and high expected earnings are at risk. As discussed in [][rescue_culture], wasteful reorganisation exacerbates moral hazard and opportunistic behaviour by weakening creditors' bargaining positions during workouts. Even if formal procedures are rarely used, reducing their cost should still improve the lending environment.
This is indeed what I find empirically. Using data from a broad, cross-country, firm-level survey conducted over several years, I show cheaper reorganisation is significantly and consistently correlated with lower self-reported obstacles to finance. This finding supports empirical work suggesting that lenders are less willing to make loans, charge higher interest rates, require shorter maturities and demand more collateral in countries with weak creditor protections[#Qian2007]. Ironically, rescue attempts in these countries are rarely successful[#Djankov2008] and official procedures seldom used[#Claessens2005].
Poorly informed and/or biased courts are thought to exacerbate the problem---possibly spawning a glut of distressed firms kept alive at creditors' expense[*e.g.*, Hungary, see\]\[][#Franks2013]. General law and economics theory suggest eliminating this kind of under-liquidation requires judicial accuracy. The literature focuses on how best to coax honesty out of disputing parties[see, *e.g.*,\]\[][#Milgrom1986,Milgrom1981]. Different models recommend various designs that induce truth-telling[see, *e.g.*,\]\[][#Fishman1990,Hay1997,Shin1994,Shin1998,Sobel1985], thereby improving the quantity and quality of information the judge has at her disposal. It follows, therefore, that better information helps judges distinguish between viable and non-viable firms and leads to more desirable outcomes in bankruptcy[see, *e.g.*,\]\[][#Ayotte2007,Bergman2007].
Possibly, however, low quality information performs a public service. When reorganisation is expensive, poorly informed bankruptcy courts are more likely to liquidate high-earning firms---forcing them to accept greater responsibility during workouts. The most informative signal is therefore not necessarily the ideal signal; judicial error may partially correct for a dysfunctional reorganisation system.
The relationship between judicial accuracy and lending depends on firm-specific characteristics which are unidentifiable from the data. To partially overcome this, I simulate a choice occasion (is finance an obstacle or not?) for subgroups of firms with similar traits and apply a mixed logit model. It appears better informed courts significantly increase self-reported financing obstacles for roughly two-thirds of firms but reduce it for the rest. I find no such heterogeneity when similarly evaluating reorganisation's cost.
Without data on surveyed firms' capital investments I cannot definitively attribute these disparate responses to the prediction of the model. Nevertheless, it is telling that heterogeneity is concentrated in countries with Chapter 11-style bankruptcy procedures. Where receivership is employed (see [][rescue_culture], [][p1policy_implications]) more information is undeniably a good thing---and coincides with prior empirical work in which borrower information positively affects private sector lending[see, *e.g.*,\]\[][#Jappelli2002,Love2003,Galindo2001,Berger2005,Brown2009].
In the next section, I theoretically deduce the impact of the cost of reorganisation and quality and depth of financial information using the model of [][rescue_culture]. [][p2Data] describes the data; [][p2strategy] outlines the empirical strategy; [][p2Results] presents results.</Text>
            <Comments>In a typical game, a judge resolves a conflict by ruling in favour of one side or the other. She hopes her ruling is socially optimal but insufficient information and each party's incentive to exaggerate his grievance means she's never certain what the socially optimal choice actually is. 
This angle is similar to one by [#Bernhardt2004;]. They illustrate that higher quality information may lead to outcomes *ex post* which induce suboptimal behaviour *ex ante*.
[#Pagano1993;] abstract entirely from bankruptcy yet reach an analogous conclusion: more information reveals credit-worthiness, forcing "risky" borrowers to pay more---sometimes pricing them out of the market. In their model, safety and risk are exogenously determined; my model suggests a root cause---knowing a borrower is safe or risky is loosely related to knowing a firm is not viable or viable when reorganisation is on the table. A safe firm is a non-viable one; their shorter expected lifespans reduce losses. Instead, riskiness is attached to viable firms, since they are more likely reorganised in bankruptcy and exploit that during debt renegotiations.</Comments>
        </Document>
        <Document ID="FC7BF06E-B752-4791-B4CA-8CB9E0DF0741">
            <Title>Higher standards beyond academia</Title>
            <Text>Although analysed in a specific context---academia---higher standards impose a quantity vs. quality trade-off that characterises many instances of female output. According to raw numerical counts, women produce less than men. Female reporters write fewer front-page bylines[#Klos2014]; female real estate agents list fewer homes[#Seagraves2013]; female physicians see fewer patients[#Bloor2008,Benedetti2004] and submit fewer grant proposals[#Gordon2009]; female pharmacists and lawyers work and bill fewer hours, respectively[#Azmat2017,Goldin2016].
When ranked by narrowly defined outcome measures, however, women often outperform. Female students earn better grades[#Voyer2014,Funk2017]; female auditors are more accurate and efficient[#Chung2001,ODonnell2001,Ittonen2013,Niskanen2011]; congresswomen secure more federal funding for their districts, sponsor more legislation and score higher on a composite measure of legislative effectiveness[#Anzia2011,Volden2013]; houses listed by female real estate agents sell for higher prices[#Seagraves2013,Salter2012]; patients treated by female physicians are less likely to die or be readmitted to hospital[#Tsugawa2016]; female pilots are involved in fewer fatal accidents[#Vail1986,Bazargan2011]; female economists write more clearly.
Additionally, if---like senior female economists---women internalise higher standards in somewhat roundabout ways, they could contribute to other labour market phenomena: sectoral and occupational concentration[#Blau2016,Cortes2016,Pertold-Gebicka2016]; women's tendency to under negotiate pay[#Babcock2003] and apply only to jobs they feel fully qualified for[#Mohr2014]. They may likewise reinforce work habits---*e.g.*, conscientiousness, tenacity and diligence---that correlate with quality and connote "femininity": female physicians consult longer with patients[#Roter2004]; female politicians fundraise more intensely[#Jenkins2007]; female faculty commit fewer instances of academic misconduct[#Fang2013]; female lawyers make fewer ethical violations[#Hatamyar2004]; female pharmacists are less likely to face performance-related disciplinary action[#Schafheutle2011].
&lt;!--\clearpage--&gt;</Text>
            <Comments>An earlier study did not find any significant gender difference in selling performance[#Turnbull2007].
The evidence on general accident rates is mixed[#Walton2016].
Evidence in several countries suggests female pharmacists are also less likely to commit criminal offenses (prescription fraud, drug trafficking, *etc.*) and minor professional misdemeanours (*e.g.*, inadequate written records)[#Tullett2003,Payne1997]. Similar gender trends have been found for physicians, dentists and other medical professionals[see\]\[for a review][#Firth-Cozens2008].</Comments>
        </Document>
        <Document ID="63CB6D77-211B-489C-AE1C-02E856B6D37A">
            <Title>Revisions</Title>
        </Document>
        <Document ID="CCCB92C0-578B-44AD-B46C-C1BDC7A531C1">
            <Title>Article</Title>
        </Document>
        <Document ID="C97C25C4-FAE9-4E24-9FF2-DC11A2AEC103">
            <Title>Hypotheses</Title>
            <Text>This paper's goal is to test the validity of [](#prop5) and [](#cor3). Consider first $Y$. The local cost of debt recovery should impact lending only when formal court-sponsored intervention is unavoidable; when creditors control assets, cumbersome proceedings and expensive oversight are probably irrelevant. This leads to the following two testable hypotheses.
HYPOTHESIS
In countries with dual-chapter bankruptcy, an increase in $Y$ increases the probability a firm reports finance as an obstacle.
ehypoth
HYPOTHESIS
In countries with receivership, an increase in $Y$ has no effect on the probability a firm reports finance as an obstacle.
ehypoth
A similar argument suggests $p$ has no effect in receivership countries. Yet, [](#cor3) claims independence if $p$ impacts only judicial accuracy. More realistically, security prices in efficient capital markets, stringent disclosure laws and well-developed credit-rating agencies and financial analysis sectors better reveal "firm fundamentals" to lenders[#Berkovitch1999]---an unexplored component of [][rescue_culture] given both parties possess identical information.
Indeed, theoretical and empirical research has mostly shown that this type of information reduces borrower-lender asymmetry, thereby alleviating credit constraints. It limits information monopolies enjoyed by incumbent lenders[#Padilla1997,Sharpe1990] and encourages better borrower behaviour[#Klein1992]. Greater information is linked to more lending[#Jappelli2002,Djankov2007], lower costs[#Brown2009] and more loans to riskier borrowers[#Berger2005]. The take-away is that receivership should make borrower information irrelevant to resolving bankruptcies but still vital for getting credit.
Testing the impact of $p$ under dual-chapter bankruptcy is complicated by an additional identification issue. $p$ is undeniably important---it crucially impacts the probability a firm is liquidated in bankruptcy. Its sign, however, is ambiguous. According to [](#prop5), a higher $p$ reduces credit rationing for non-viable firms and increases it for viable firms. Because firm viability is not known, however, [](#p2equation5) aggregates $p$'s impact across all firm types.
Although I cannot identify $p$'s impact by firm, I *can* distinguish its impact between bankruptcy regimes. [](#hypoth3) contrasts countries with receivership to those without it---and theorises that information's effect on firms in the latter is less precisely estimated and hews more closely zero than it does for firms in the former.
HYPOTHESIS
In countries with receivership, higher $p$ reduces the probability a firm reports finance as an obstacle by an amount greater than the analogous effect in countries with dual-chapter bankruptcy.
ehypoth
</Text>
            <Comments>When loan contracts can specify either receivership or Chapter 11-style procedures to resolve potential defaults---as was possible in the U.K. between 1986--2003---evidence suggests a large majority  are based on receivership[#Armour2008].
Extrapolating beyond this assumption, [](#cor3) technically makes no prediction on $Y$, either; in contrast to credit information, however, there is no convincing theory that contract enforcement costs matter in the absence of contract disputes.
Several partial equilibrium theoretical models actually predict an ambiguous effect. For example, as discussed in [][p2Introduction], [#Pagano1993;] demonstrate that greater levels of borrower information boost loans to safe borrowers but cut them to risky ones.</Comments>
        </Document>
        <Document ID="E0990667-B3D3-4C51-8EDC-D663EC5C51BB">
            <Title>dissertation</Title>
        </Document>
        <Document ID="7CCF514E-E6AE-4E0B-A543-CB9AD657BFD3">
            <Title>Women's papers spend longer under review</Title>
            <Text>&gt; "Writing simply and directly only looks easy"[\]\[p. 53][#Kimble1994].
Good writing takes time[#Hartvigsen1981,Kroll1990]: skilled writers spend longer contemplating a writing assignment, brainstorming and editing; they also write fewer words per minute and produce more drafts[#Faigley1981,Stallard1974]. As a consequence, higher writing standards---and, indeed, higher standards applied more generally[see, *e.g.*,\]\[][#Card2020,Moon2020]---should result in female authors spending longer in peer review, all things equal.
On the other hand, better writing by female economists could perfectly offset some other advantage present in men's papers, conditional on quality. In this case, the time-cost of publishing a paper will instead be gender neutral---since if it weren't, women could reduce their time spent in review by adopting a strategy marginally closer to men's (or visa versa).
To formalise this idea, consider male and female researchers who use strategies $x_m,x_f\in\mathcal X$ to produce papers of identical quality $Q\in\mathcal Q$. Let $q$ represent the function mapping $\mathcal X$ onto $\mathcal Q$ and define $q^{-1}(Q)$ as the set of strategies in $\mathcal X$ that achieve the same $Q$.
If men and women are held to identical standards in peer review, then both will accrue identical rewards, conditional on $Q$, *i.e.*,
&lt;!--\input{$HOME/Dropbox/Readability/draft/0-tex/equations/equation7}--&gt;
 where $x$ is any strategy in $q^{-1}(Q)$ and $a_g(x_g,Q)$ is the acceptance rate for gender $g\in{m,f}$ given strategy $x_g$ and quality $Q$.
If men and women are also equally capable researchers, then neither side should have to exert more effort, conditional on acceptance rate (and, hence, $Q$)---*i.e.*, given [](#reward), there must exist some $\hat x_m,\hat x_f\in q^{-1}(Q)$ such that
&lt;!--\input{$HOME/Dropbox/Readability/draft/0-tex/equations/equation8}--&gt;
 where $c_g(x_g)$ is the cost to gender $g$ of implementing the strategy $x_g$. In the absence of higher standards, [](#cost) implies that men's and women's time-cost of review should be equal, conditional on $Q$.
Men's and women's time-cost of review does not appear to be equal. [](#figure10) displays histograms of time (in months) between dates male- (defined as having a ratio of female authors below 50 percent) and female-authored papers (defined as having a ratio of female authors above 50 percent) are first submitted to and their final revisions received by the editorial offices of *Econometria* and *REStud*. Women's review times disproportionately cluster above the mean: their articles are five times more likely to experience delays above the 75th percentile than they are to enjoy speedy revisions below the 25th.
&lt;!--\input{$HOME/Dropbox/Readability/draft/0-tex/fixed/Figure-3}--&gt;</Text>
            <Comments>Higher standards come from accepting male-authored papers more often than female-authored papers, conditional on $Q$---*i.e.*, $a_m(x,Q)&gt;a_f(x,Q)$---rewarding men's strategies more than women's strategies even though they both generate identical $Q$---*i.e.*, $a(x_m,Q)&gt;a(x_f,Q)$---or both.</Comments>
        </Document>
        <Document ID="33054FFA-2997-4165-94F9-0FD0E66C0167">
            <Title>Discussion of potential alternative mechanisms</Title>
            <Text>A gender readability gap exists. It's still there after including editor, journal and year effects---meaning it's probably not caused by antiquated policies and attitudes, long since overcome. The gap is unaffected by field controls, so it doesn't seem to be related to women researching topics that are easier to explain. Nor does it appear to be caused by factors correlated with gender but actually linked to authors' (or co-authors') competence as economists and fluency in English---if so, institution, native speaker and citation controls would reduce it. They do not.
The gap grows between first draft and final publication and over the course of women's careers, likely precluding inborn advantage and one-off improvements in response to external circumstances unrelated to peer review. This probably also rules out gender differences in (i) biology/behaviour---*e.g.*, sensitivity to referee criticism---or (ii) knowledge about referee expectations. If diligently addressing every referee concern has no apparent upside---acceptance rates are unaffected---and a very clear downside---constant redrafting takes time---even the most oversensitive, ill-informed woman would *eventually* re-examine initial beliefs and start acting like a man, no? Yet this is not what we observe. The largest investments in writing well are made by female economists with greatest exposure to peer review---*i.e.*, those with the best opportunity to update their priors.
Women's papers are more likely assigned female referees[#Abrevaya2012,Gilbert1994]. If women are more demanding critics, clearer writing could reflect their tougher reviews. Women concentrate in particular fields, so it’s natural female referees more often review female-authored papers. Nevertheless, for the readability gap to exist only because of specialisation, controlling for *JEL* classification should help explain it. It does not: including 20 primary or 731 tertiary *JEL* category dummies has little effect. So if referee assignment is causing the gap, it may be because journals disproportionately refer female-authored papers to the toughest critics. Meaning it isn't referees who are biased---it's editors.
[][NBER] suggests a link between the gender readability gap and peer review; the evidence presented in [][Mechanisms] suggests that factors outside women's control drive it. Yet oversensitivity and/or poor information could create the former gap while *another* gender bias unconnected to peer review generates the latter. One in particular comes to mind: the feedback women receive in conferences and seminars. Perhaps experienced female economists tighten prose (before or after submission) in response to audience member remarks? Recent evidence suggests female speakers are indeed given a harder time[#Dupas2021]. Nevertheless, most conference and seminar participants are also current (or future) journal referees. Neutral peer review feedback is inconsistent with non-neutral conference/seminar feedback when originating from the same group---especially since gender neutrality is emphasised in both environments.
Perhaps women focus on writing at the expense of some other aspect of a paper due to a comparative advantage? Women's chosen publication strategy results in similar (or lower) acceptance rates and longer review times compared to the one employed by men. If men and women are equally capable researchers then writing well cannot be a comparative advantage and at the same time be strictly dominated by another strategy.
In the universe of straightforward alternatives, this leaves us with one: female economists are less capable researchers. As mentioned earlier, factors correlated with gender but actually related to competency should decline when appropriate proxies are included. The sample itself is one such proxy---these are, after all, only articles published in the top four economics journals. Adding other controls---author seniority, institution, total article count, citations and published order in an issue---has no effect. The gap is widest for the most productive economists and even exists among articles originally released as NBER working papers---both presumably very clear signals of merit. Indeed, contemporary female-authored papers published in a top-four economics journal are, in fact, cited more than male-authored papers[#Moon2020].
Yet I cannot rule out the possibility that women's work is systematically worse than men's in a way that is somehow not full captured by citations, proxies for author prominence and seniority or author-specific fixed effects---or that the female and male authors in [][MatchingResults] are not really equivalent. And if this is true, editors and referees *should* select and peruse our papers more carefully---a byproduct of which could be better written papers after-the-fact or more attractive prose compensating for structural weaknesses before it.
"Quality" is subjective; measurement, not easy. Nevertheless, attempts using citation counts and journal acceptance rates do not indicate that men's research is any better, conditional on publication: as discussed in [][SEUModel], men's and women's papers are accepted at similar rates unconditional of quality. As already mentioned, recent research specific to economics suggests female-authored papers may be cited more conditional on publication[#Grossbard2018,Moon2020,Card2020].
More complicated, multi-factor explanations could resolve inconsistencies present when each is analysed in isolation. Perhaps female economists are perfectionists, and it gets stronger with age? Or, a preference for writing well coupled with unaccounted for co-author characteristics could combine to cause women's more readable papers *and* their increasing readability. Alternatively, measurement error and/or co-variate controls could have interacted with gender in ways I did not anticipate.
Still, no explanation matches the simplicity of biased referees and/or editors. Coherence and economy do not establish fact, but they are useful guides. This single explanation neatly accounts for all observed patterns. If reviewers apply higher standards to female-authored papers, they will be rejected more often and/or subject to tougher review. Added scrutiny should improve exposition but prolong publication. Women would internalise the rewards they receive from writing more clearly, accounting for their better writing over time.
&lt;!--\clearpage--&gt;</Text>
            <Comments>I also conducted a primitive surname analysis [see\]\[pp. 35--36][#Hengel2016]. It suggests that the female authors in my data are no more or less likely to be native English speakers.
While women do appear more *internally* responsive to feedback---criticism has a bigger impact on their self-esteem---available evidence suggests they aren't any more *externally* responsive to it, *i.e.*, women and men are equally likely to change behaviour and alter performance after receiving feedback[#Johnson2002,Roberts1989].
This statement is especially relevant if the opportunity cost to women for "wasting" time on needless tasks is higher---*e.g.*, because of family responsibilities.
Note that women are only a fraction of all referees---8 percent in 1986[#Blank1991], 10 percent in 1994[#Hamermesh1994] and 14 percent in 2013[#Torgler2013]. [#Abrevaya2012;] report female-authored papers were only slightly more likely to be assigned a female referee between 1986--1994; matching increases between 2000--2008.
It's not clear whether women's reports are more critical. A study specific to post-graduate biologists suggests yes[#Borsuk2009]; other studies specific to economics suggest not[#Abrevaya2012,Card2020].
Specifically, men and women publishing in the same field face the same pool of referees, so controlling for that pool should reduce gender differences in readability if specialisation contributes to it.
Relatedly, perhaps female-authored research is more provocative and therefore warrants more scrutiny. Yet if this explained the gap, controlling for *JEL* classification should reduce (or eliminate) it---unless women's work is systematically more provocative even among researchers in very narrow fields. There is some evidence for this hypothesis---provocative work is (presumably) highly cited work and recent female-authored papers published in top economics journals are cited more[#Moon2020,Card2020]. Yet more provocative, cited research would probably be published at higher rates---and there is no evidence women's paper's are more frequently accepted[#Ceci2014].
This is a form of biased referee assignment identified in [](#Theorem1). It would also apply if the readability gap reflects referees' apathy for women's work. Readability is particularly relevant when interest in---and knowledge about---the topic is low[#Klare1976,Fass1978]. Thus, a gap could emerge if editors fail to assign interested and knowledgable referees to female-authored papers.
Assuming men and women are equally capable researchers, women would only emphasise a particular aspect of a paper at the expense of others if doing so achieved a similar outcome/effort trade-off as the one employed by men. As discussed in [][Duration], however, the outcome/effort combination women *currently* experience appears to be strictly worse than men's.
Published order in an issue was introduced as a set of indicator variables in an earlier version of this paper [\]\[pp. 42 and 44][#Hengel2016].
Nevertheless, a significant amount of research finds evidence of bias against women in the decision to cite, unconditional of publication[#Ferber1986,Ferber1988,Dion2018,Koffi2021]. This suggests that citations under-estimate the quality of female-authored work.
While women score higher on maintaining order[#Feingold1994]---a trait including organisation and perfectionism---significant differences are not universally present in all cultures[#Costa2001], and differences that are present appear to decline---or even reverse---as people age[#Weisberg2011].
&lt;!--\label{FootnoteSeniorWomen}--&gt;This might occur, for example, if women are excluded from male networks as $t$ increases; consequently, senior female economists may be more likely to co-author with other women than junior female economists. Relatedly, women may have preferred to have written their $t=1$ publication more clearly, but senior male co-authors held them back; at $t=3$, they enjoy more freedom to achieve their desired (higher) readability by writing on their own or with other women. As I show in an earlier version of this paper, however, as $t$ increases, women are more likely to co-author with men, while men are more likely to co-author with women[\]\[Table 12, p. 25][#Hengel2016].
In support of this hypothesis, existing and ongoing research suggests female workers are held to higher standards in job assessments: they are acknowledged less for creativity and technical expertise, their contributions are infrequently connected to business outcomes; guidance or praise supervisors do offer is vague and tends to under-estimate women's "potential" [#Correll2016,Benson2021]. Students display a similar bias. [Data](http://benschmidt.org/profGender/) from [Rate My Professors](http://www.ratemyprofessors.com/) suggest female lecturers should be "helpful", "clear", "organised" and "friendly". Men, instead, are praised (and criticised) for being "smart", "humble" or "cool"[#Schmidt2015]. A study of teaching evaluations similarly finds students value preparation, organisation and clarity in female instructors; their male counterparts are considered more knowledgable, praised for their "animation" and "leadership" and given more credit for contributing to students' intellectual development[#Boring2017].</Comments>
        </Document>
        <Document ID="FA72F2D4-234B-4D2B-B5ED-16F2115A1EC3">
            <Title>Model</Title>
            <Text>I adapt the model in[#Bartos2016;] to a single-stage decision maker's choice about a submission. Before the game starts, I assume papers have undergone an initial screening by editors to rule out submissions that fail to satisfy the following condition:

$$\EE[q]=q_G+q_1+\EE[q_2]&gt;0,$$

where $q$ is the unknown objective quality of the paper, and can be expressed as the sum of observable $q_G$ and $q_1$ and unobservable $q_2$. $q_G$ is the average quality of the author's gender group, $G$; $q_1$ was revealed during pre-screening and shows an initial indication of the degree to which the paper's quality deviates from the group average; $q_2$ is the unknown portion of the paper's quality.

For the decision maker, the decision to reject is based on the inherent unknown payoff $\pi$, which consists of two components,

$$\pi=q-d_G,$

where $d_G$ is the referee's distaste toward the author's gender $G$. $d_G$ is private information to the referee. The reservation payoff from rejecting the applicant is 0. 

Figure X illustrates the game in extensive form. Pre-screening reveals $q_G$ and $q_1$, both of which are then observed by the referee. Post pre-screening, the referee may either to accept the paper immediately or instead postpone the decision until he has fully reviewed the document.

In the latter case, the referee reads the paper in detail and writes a referee report. This stage also includes re-reviewing the paper in the event the referee requested clarifications of and/or changes to the paper. This process is assumed to be time-consuming for the referee, and this cost is captured by $c$. Once the process is complete, however, $q_2$ is revealed, thus the referee is rewarded by knowing $q$ precisely. At this stage, when all costs of information acquisition are sunk, the applicant is accepted if and only if $q&gt;d_G$.

Define $q_1^\star$ as the $q_1$ below which the paper is reviewed and above which it is accepted immediately. That is, $q_1^\star$ is the $q_1$ that satisfies the following condition:

$$c=-\EE[\min\{q_G+q_1^\star+q_2,0\}].$$

I now describe how the probability of spending longer in review---reflected by the fact that the paper is not immediately accepted---increases regardless of the specific form of discrimination that is present.

Proposition 1. Lower $q_G$ and $\sigma_2$ and higher $d_G$ are more heavily scrutinised in lemon-dropping markets.

A distaste toward a certain group is captured by the parameter $d_G$[#Becker1971]. Here, a higher distaste implies more attention is paid to the applicant. Costly information serves to fast track certain expected high quality submissions into acceptance to potentially save on the cost of reviewing them.

Next, I consider statistical discrimination[#Phelps1972,Arrow1973], which is driven by differences in beliefs about the applicant's quality. In the model, this channel is represented by a change in $q_G$. The implications of a reduction in $q_G$ are then the same for an increase in the distaste parameter $d_G$: more attention is paid to the applicant.

Last, I consider the effects on attention of a greater difficulty to understand signals from a culturally dissimilar group. In a departure from[#Bartos2016;], I interpret this in that less information was revealed in the pre-screening phase, and so $\sigma_2$ is higher. This results in a mean-preserving spread, and serves to increase the benefits of filtering out bad papers during review.</Text>
            <Comments>I assume revealing $d_G$ to the editor would cause reputational harm in ecess of $d_G$. This assumption prevents the referee from rejecting a paper without reviewing it first; I make it in order to rule out blatant discrimination. The only way to reject a paper without revealing this information is to justify rejection on some other grounds---a process which requires reviewing the paper.
Immediate rejection is ruled out, since doing so reveals $d_G$.
At this stage, I assume the referee is sufficiently familiar with the paper to justify rejecting it without revealing $d_G$.
An alternative way to interpret $d_G$ is t osuggest it captures the extent to which referees *feel* (wrongly) the initial screening phase by (unbiased) editors was unfair. That is, if referees feel that editors are pressured to publish more female-authored papers</Comments>
        </Document>
        <Document ID="BE72F41A-3C38-45B4-8D5D-06D35D41B25A">
            <Title>[](#table10), male effects</Title>
        </Document>
        <Document ID="DDB754E7-DD00-4DA3-82D3-4534B0CD8CA0">
            <Title>Proofs</Title>
            <Text>Only [](#prop5) is proved. [](#cor3) follows directly from [](#prop4).
&lt;!--\input{$PPATH/p2/proofs/proofs}--&gt;</Text>
        </Document>
        <Document ID="8EDD2579-E7F5-48CC-BBBE-C048BCBD97F7">
            <Title>Robustness</Title>
            <Text>[](#table10) documents a rise in the readability gap as women publish more articles. [](#table11) points to an individual-specific explanation driving this phenomenon. One such explanation is "learning-by-doing". If the payoff from lucid exposition is high, people will catch on---either by internalising explicit comments on text readability in referee reports from earlier papers or making the (un)conscious connection that review times are faster when text is clearer. Applying that payoff only to women yields a succinct explanation for the gap's observed growth.
&lt;!--\input{$PPATH/figures/tex/figure6}--&gt;
But there are two possible alternatives. I investigate both and show that neither is likely. As discussed in [][AuthorLevel], readability and female ratio may be nonlinearly related---specifically, evidence suggests the latter is increasing and convex in the former. Thus, if women are more likely to co-author with other women (or by themselves) when they already have several publications behind them, the observed increase would actually reflect that late-career concentration.
This is not the case. [](#table12) displays the marginal effect of $t$ for men and women from a fixed effects regression on female ratio. The effect for men is positive; the effect for women negative. As $t$ increases, genders diversify: men publish with more women and women publish with more men.
&lt;!--\input{$PPATH/tables/tex/table12}--&gt;
The second explanation applies to the evolution of female writers as a group---perhaps the clearest women publish more often? [](#equation10) investigates:where $\Phi$ is the standard normal cumulative distribution function, $T_i\ge x$ equals one if author $i\text{'s}$ total publication count, $T_i$, is at least the positive integer $x$ and subscript 1 refers to his first publication. [](#table13) displays $\Phi^\prime\cdot\l(\beta_2+\beta_4\r)$---the marginal effect of female authors' first paper readability scores on the probability of publishing multiple times.
&lt;!--\input{$PPATH/tables/tex/table13}--&gt;
The readability of a woman's first paper does not predict how often she will publish. Regardless of the threshold chosen, it has no impact on her eventual productivity and is indistinguishable from zero in all estimates.</Text>
            <Comments>A related possibility is that women are more responsive to referee reports. This and other explanations are addressed in [][Alternatives].
\input{$PPATH/equations/equation10}</Comments>
        </Document>
        <Document ID="51B6F600-C7F0-401E-A9C3-5CFABD9D24FC">
            <Title>Firm-level data</Title>
            <Text>To account for credit-rationing and control for firm-specific characteristics, I use data from the World Enterprise Surveys (WES). The WES are a series of company surveys conducted by the World Bank and the European Bank for Reconstruction and Development (EBRD). They cover business constraints in emerging and developing markets. The surveys start in 2005 and are generally repeated every three years. They contain 117,105 observations from 135 countries.
One question asks the degree to which access to finance---including interest rates, fees and collateral---is an obstacle to business operations. Answers range from 0 (no obstacle) to 4 (very severe obstacle). I define $\text{obstacle}_i$ as a binary variable equal to 1 if firm $i$'s response was moderate (2) or above---roughly half the sample.
Additionally, I control for several firm-specific characteristics. $\text{Age}_i$ is the difference between the year firm $i$ was established and the year in which the survey took place. $\text{Size}_i$ distinguishes between small (fewer than 20 employees), medium and large (more than 100 employees) companies in ascending order from 0 to 2. $\text{Exporter}_i$ is a binary variable equal to 1 if the firm exports all or part of its output; $\text{foreign-owned}_i$ and $\text{state-owned}_i$ equal 1 if the firm is partly or wholly foreign- or state-owned, respectively. $\text{Manufacturing}_i$ equals 1 if firm $i$ operates in the manufacturing industry.
[](#p2table1)summarises these variables. Surveyed firms tend to be small, privately owned operations that produce for the domestic market. Export status, foreign ownership and size are the same for firms reporting finance as an obstacle compared to those that don't. State-owned firms are less likely to find financing an obstacle, but the difference is slight. Younger firms and those in manufacturing, however, have a significantly tougher time accessing finance than their older, service-oriented peers.</Text>
            <Comments>The WES were conducted prior to 2005 but earlier surveys did not report firm size or employ a consistent sampling methodology.
Fully state-owned enterprises are not surveyed.
\input{$PPATH/p2/tables/tex/p2table1}</Comments>
        </Document>
        <Document ID="39237732-77A0-4FEE-A26D-77ECE8A9EA9F">
            <Title>Readability scores</Title>
            <Text>To measure writing clarity, I use the five most common, widely tested and reliable readability formulas for adult-level material: Flesch Reading Ease, Flesch-Kincaid, Gunning Fog, SMOG (Simple Measure of Gobbledegook) and Dale-Chall. The formulas for each are shown in [](#tab:formulas). [][AppendixReadability] discusses the scores in more detail and reviews the literature on their validity.
&lt;!--\input{$HOME/Dropbox/Readability/draft/0-tex/fixed/Table-1}--&gt;
The Flesch Reading Ease formula ranks passages of text in ascending order---*i.e.*, more readable passages earn higher scores. The other four formulas generate grade levels estimating the minimum years of schooling needed to confidently understand an evaluated text; as a result, more readable passages earn lower scores. In order to simplify interpretation, I multiple the four grade-level scores by negative one. Thus, higher scores universally correspond to clearer writing throughout this paper.
To calculate the scores, I wrote the Python module `Textatistic`. Its code and documentation are available on GitHub; a brief description is provided in [][AppendixTextatistic]. For added robustness, I also re-calculate scores and replicate most results using the `R readability` [package][AppendixAlternativeMeasure].</Text>
        </Document>
        <Document ID="B7F0DE17-4271-4D50-93F7-078BAD727EC4">
            <Title>Summary statistics</Title>
        </Document>
        <Document ID="B4274C52-4B25-452A-92C4-11AB5031CDD2">
            <Title>Introduction</Title>
            <Text>Ladies, our papers aren't published that often in "top-four" economics journals. In 2015, the average share of female authors per paper was 15 percent. Only eight percent were majority female-authored. Just four percent were written entirely by women. The *Quarterly Journal of Economics* did not publish a single exclusively female-authored paper between 2015--2017 (inclusive). In several recent years, *Econometrica* and the *Journal of Political Economy* have not either.
These statistics are uncomfortable, but their causes are myriad: lower publishing rates, career choices, motherhood and, possibly, bias. In lab and field experiments women are subject to tougher standards. Their qualifications and ability are underestimated[#Foschi1996,Grunspan2016,Moss-Racusin2012,Reuben2014]. Female-authored manu\\-scripts are evaluated more critically[#Goldberg1968,Krawczyk2016,Paludi1983]; when collaborating with men, women are given less credit[#Heilman2005,Sarsons2015].
This paper uses five measures of writing clarity to suggest that women are likewise held to higher standards in peer review. (i) Female-authored articles published in top-four economics journals are better written than similar papers by men. The difference cannot be explained by year, journal, editor, topic, institution, English language ability or by proxying for article quality with citations and accounting for author prominence and seniority. (ii) The gap widens precisely while papers are being reviewed. I compare published articles to their pre-reviewed drafts. Forty percent of the gap originates *during* peer review. (iii) Female economists improve their writing; male economists don't. I use a dynamic model of an author's decision-making process to show that tougher editorial standards and/or biased referee assignment are the only explanations consistent with men's and women's diverging choices. A conservative estimate derived from the model suggests higher standards in peer review cause senior female economists to write at least seven percent more clearly than they otherwise would.
I also document evidence that higher standards confound productivity measurement and their own identification. First, higher standards presumably delay review. To investigate this hypothesis, I estimate gender differences in submit-accept times at *Econometrica* and a fifth highly regarded journal, the *Review of Economic Studies*. Female-authored papers spend three to six months longer in peer review even after controlling for, among other things, author seniority, motherhood, childbirth, citations and field.
Second, my data suggest women gradually adapt to higher standards *in* peer review by writing more readably *before* it. Such behavioural responses to discrimination---or ``feedback effects''---make it difficult to distinguish biased treatment from voluntary choice. More broadly, they emphasise that discrimination is usually observed in (or on the path toward) an equilibrium state. Studies that analyse only one slice of that path must take into account how the general equilibrium would have evolved had discrimination not occurred. Otherwise, they risk underestimating it, misallocating responsibility or even concluding bias against men.
Higher standards impose a quantity/quality trade-off that plausibly contributes to academia's "Publishing Paradox" and "Leaky Pipeline". Spending more time revising old research means there's less time for new research. Fewer papers results in fewer promotions, possibly driving women into fairer fields. Since there is evidence of this trade-off in a variety of occupations---*e.g.*, doctors, real estate agents and airline pilots---higher standards could distort women's productivity, more generally.</Text>
            <Comments>For comparison, 28, 26 and 13 percent of assistant, associate and full professors, respectively, are women[#Lundberg2017]. "Majority female-authored" refers to papers with a ratio of female authors strictly above 50 percent.
While seven percent seems small, it is based on a single paragraph. Assuming a similar standard applies to every paragraph in a paper and improving each one takes slightly more time, the accumulated impact may be substantial.</Comments>
        </Document>
        <Document ID="40EB9DC5-9E7B-41F5-9A6D-3BB99C603C47">
            <Title>Condition 2</Title>
            <Text>The purpose of Condition 2 is subtle. It rules out the possibility that women just intrinsically prefer writing more clearly than men or enjoy a lower cost of writing. Yet any such intrinsic preference or cost explanation defines the minimum readability at which a women is willing to write. Thus, *all* of her papers, including the most poorly written one, is at least this readability level. Because women will always choose a readability level that satisfies this minimum threshold dictated by their preference, any *increase* in readability must be driven by something else---*i.e.*, either their own mis-information or sensitivity or by factors outside their control.</Text>
        </Document>
        <Document ID="E82A7A4C-7FBE-442B-AAC6-30137C46B264">
            <Title>362 (erin’s imac's conflicted copy 2017-09-26)</Title>
            <Text>To help organise the discussion, I develop a stylised model of readability's marginal impact on the editorial decision making process. It follows an author---denoted by $i$---who publishes several articles in prestigious academic journals over the course of his career. Each article is roughly equivalent in terms of topic, novelty and quality, but varies on readability.
At stage 0, author $i$ drafts his $t$th paper and submits it for peer review. Upon receipt, the journal's editorial office assigns the manuscript to a group of referees. The (finite) set of all potential review groups is represented by $\Sigma$; $\mu_i$ is the set of strictly positive probability measures on $\Sigma$. $\Sigma$ and $\mu_i$ are known to $i$.
Let $r_{0it}$ and $\widetilde r_{0i}^s$ denote manuscript $t$'s non-negative draft readability and the initial rejection threshold review group $s\in\Sigma$ applies to all papers by author $i$, respectively. $s$ rejects the paper at stage 0 if $$r_{0it}&lt;\widetilde r_{0i}^s.$$ $i$ is otherwise granted a "revise and resubmit" (R&amp;R), yet could still be rejected at stage 1 if the readability of his revised manuscript, $R_{it}=r_{0it}+r_{1it}$, does not meet a second threshold, $$R_{it}&lt;\widetilde R_i^s,$$ where $\widetilde R_{i}^s=\widetilde r_{0i}^s+\widetilde r_{1i}^s$. All rejections and acceptances are final. $\widetilde R_i^s\ne\widetilde r_{0i}^s$ to account for different standards at different stages of peer review. $r_{1it}$, $\widetilde r_{0i}^s$ and $\widetilde r_{1i}^s$ are non-negative; the latter two are independent.
To aid the revision process, $s$ writes a referee report from which $i$ forms expectations about $\widetilde R_i^s$ by assigning subjective probabilities $\pi_{1it}^s(R)$ to all $R$. Unfortunately, the concept of readability is complex, some referees write insufficiently detailed reports and inattentive or hypersensitive authors misconstrue even perfectly clear advice. This renders $i$'s interpretation of the report imprecise and his subsequent expectations about $\widetilde R_i^s$ inexact and possibly specious.
Conditional on $r_{0it}$, I assume referee reports by $s$ for $i$ are the same for all $t$ and that each is distinctive enough for $i$ to distinguish $s$ in $\Sigma$. Consequently, author $i$'s stage 1 choice of $R_{it}$ maximises his (immediate) subjective expected utility given $s$,
&lt;!--\input{$PPATH/equations/equationX1}--&gt;
 $\Pi_{1it}^s(R_{it})$ is the cumulative sum of $\pi_{1it}^s(R)$ for all $R\le R_{it}$; $u_{i}$ is the utility of having a paper accepted in a prestigious journal; $\phi_{i|r_{0it}}(r_{1it})=\phi_i(R_{it})-\phi_i(r_{0it})$ and $c_{i|r_{0it}}(r_{1it})=c_{i}(R_{it})-c_{i}(r_{0it})$ are the satisfaction and cost, respectively, from making changes $r_{1it}$ given the paper's initial readability $r_{0it}$. $\phi_i$ is increasing and concave in its arguments, $c_i$ increasing and convex---marginally increasing $R_{it}$ generates proportionally less satisfaction but needs more effort when the paper is already well written. $c_i(0)$ and $\phi_i(0)$ are both 0.
I assume authors' decisions at stage 0 are myopic; $i$'s choice of $r_{0it}$ maximises his initial subjective expected utility for the current paper,
&lt;!--\input{$PPATH/equations/equationX2}--&gt;
 where $\Pi_{0it}^s(r_{0it})$ is the cumulative sum for all $r\le r_{0it}$ of author $i$'s subjective probabilities $\pi_{0it}^s(r)$ about $\widetilde r_{0i}^s$; $v_{1it}^s$ is [](#equationX1) evaluated at the optimal $r_{1it}$.
Finally, I assume authors (i) observe each others' readability choices and publication outcomes; and (ii) use relevant information to update their subjective probabilities. These assumptions imply, at a minimum, that $i$ updates $\Pi_{0it}$ and $\Pi_{1it}$ based on (i) conclusive evidence derived from the choices and outcomes of [equivalent peers](#definition1); and (ii) knowledge acquired during his own prior experience in peer review.
&lt;!--\input{$PPATH/theorems/assumptions}--&gt;
[](#equationX1) and [](#equationX2) incorporate a variety of factors that potentially affect authors' readability choices---editorial standards ($\widetilde r_{0it}$ and $\widetilde R_{it}$); ambition ($u_i$); the cost of drafting and revising manuscripts ($c_i$); an otherwise unexplained intrinsic satisfaction from writing readable papers ($\phi_i$). Poor information, overconfidence and sensitivity to criticism are not explicitly included, on the assumption that people do not *want* to be poorly informed, overconfident and excessively sensitive. These factors nevertheless enter [](#equationX1) and [](#equationX2)---and hence influence choices---via the subjective expectations author $i$ forms about $\widetilde r_{0i}^s$ and $\widetilde R_i^s$.
A single observation $R_{it}$ cannot therefore establish if and to what extent author $i$'s choices are motivated by preferences and costs specific to him ($u_i$, $\phi_i$, $c_i$), editorial standards and/or referee assignment outside his control ($\widetilde r_{0it}$, $\widetilde R_{it}$, $\mu_i$) or the disparate confounding factors mopped by $\Pi_{0it}^s$ and $\Pi_{1it}^s$. An observed *increase* in $i$'s readability in two separate $t$, however, distinguishes the former explanation (preferences and costs) from the combined impact of the latter two. Gradual updating of subjective probabilities, however, slowly reduces the impact of confounding factors. Having ruled out preferences and costs, any difference in readability choices between equivalent peers that persists over time must therefore be attributable to 
Comparison over time to an equivalent peer suggests 

The *path* of $i$'s choices compared to an equivalent peer's identifies editorial standards and referee assigment all other confounded factors incorporated in $\Pi_{0it}^s$ and $\Pi_{1it}^s$.
This idea is captured in [](#theorem1). Let $\bm1_{0i}^s(r)$ and $\bm1_{1i}^s(R)$ are indicator functions equal to 1 if $r\ge\widetilde r_{0i}^s$ and $R\ge\widetilde R_i^s$, respectively, and $\{(r_{0it},R_{it})\}$ and $\{(r_{0kt},R_{kt})\}$ denote the sequence of readability choices $i$ and $k$ make over their lifetimes. Then, roughly, if $i$'s current papers are more readable than his past papers and more readable than $k$'s papers then either (i) $i$ has a higher probability, on average, of being accepted; (ii) $i$ is assigned "tougher" referees---*i.e.*, those with higher $\widetilde r_{0i}^s$ and/or $\widetilde R_i^s$; or (iii) referees, in general, are tougher on $i$'s papers---*i.e.*, $\widetilde r_{0k}^s&lt;\widetilde r_{0i}^s$ and/or $\widetilde R_k^s&lt;\widetilde R_i^s$ for at least one $s\in\Sigma$.
&lt;!--\input{$PPATH/theorems/theorem}--&gt;
$u_i$, $\phi_i$ and $c_i$ remain constant over time. Thus, changes to $r_{0it}$ and $R_{it}$ imply changes in the subjective probabilities author $i$ forms about $\widetilde r_{0it}$ and $\widetilde R_{it}$. Thus, when authors write current papers more readably than past or future papers, they do so because they expected to be rewarded for it with a higher probability of acceptance. Changes to the optimal choices $r_{0it}$ and $R_{it}$ are motivated entirely by a desire to achieve his other objective of getting his paper accepted into a prestigious academic journal.
[](#theorem1) indirectly implies that if author $i$ choses a higher readability than another author with an otherwise equivalent paper, then author $i$ anticipated a higher probability of acceptance, *ex ante*. Of course, that doesn't mean author $i$ is right---it is perfectly plausible that he's misinformed and there is no difference in their probabilities of acceptance. As $t\rightarrow\infty$ in a fixed state $s$ with perfect recall of his past histories or in any state with access to his peers' histories, author $i$ eventually figures this out.
</Text>
        </Document>
        <Document ID="3DD332CF-27D4-4597-9B28-44889B1A7427">
            <Title>Descriptive evidence</Title>
        </Document>
        <Document ID="2C2059F4-D331-439A-93B1-D9A8578F887F">
            <Title>Appendices</Title>
        </Document>
        <Document ID="ADCA6B09-EEDD-4F90-A5FB-F7910D66AD57">
            <Title>Settlement.</Title>
            <Text>Bankruptcy isn't the only option. Creditors and entrepreneurs can always deal with insolvency on their own. One scenario involves a mutual decision to wind-up business operations. In another, both parties agree to a workout.
Monetary or in-kind transfers between borrower and lender are common components of workout agreements; in liquidation, however, they are usually forbidden. Paying a director to voluntarily wind up his insolvent firm qualifies as a "preference payment"---*i.e.*, a payment that "has the effect of putting [its recipient] into a position which, in the event of the company going into insolvent liquidation, will be better than the position he would have been in if that thing had not been done"[\]\[§239 (4)(b)][#GreatBritain1986]. Preference payments are not allowed in either the U.S. or the U.K.
Voluntarily liquidating an insolvent firm differs very little from compulsory liquidation. The process is overseen by a third-party who acts in the creditors' interests. One of his tasks is to inspect the firm's financial records and recover preference payments---which I assume he does efficiently and accurately. Without preference payments, neither side has the power to "bribe" the other to wind up operations; combined with the earlier assumption that liquidation does not erode project value, the process and proceeds are identical to those in compulsory liquidation.
In a workout, the bankrupt debtor negotiates a revised debt contract with his creditors outside the judicial system. Workouts, when allowed, follow a similar script. In the U.S., managers propose a plan to restructure the debt; creditors then vote on it. If unanimously accepted, the plan is implemented without requiring court intervention. Time spent in bankruptcy is reduced or eliminated, making them cheaper and less stressful than formal reorganisation[#McConnell1991]. I assume they are costless.
The debt renegotiation game is very simple; I assume the entrepreneur makes a take-it-or-leave-it offer to replace the original debt $D$ due at time 1 with a new one $\wt D$ due at time 2. If accepted, projects operate another period; gross earnings are identical to $V_2^C$. Under these circumstances, the entrepreneur offers the smallest $\wt D$ the creditor will accept: one which equates the latter's expected earnings in a workout, $\ol C_1^W$, with those of his outside option---bankruptcy ([](#lem2)).
LEMMA
In a workout, the entrepreneur offers the creditor the smallest $\wt D$ such that $\ol C_1^B=\ol C_1^W$. Such a $\wt D$ exists if and only if $\,\ol C_1^B\le\ol V_1^C$.
elem
Assuming his proposal is accepted, the entrepreneur's expected earnings from a workout are
$$\ol E_1^W=\ol V_1^C-\ol C_1^B.$$
 Let $\ol E_1^C$ be the time 1 expected value of $E_2^C$. Since $\ol C_1^B$ is only ever at most $D$, $\ol E_1^C\le\ol E_1^W$ for values of $X_1$ within a sufficiently small neighbourhood of $D$. The upshot? Entrepreneurs are better off in a workout than they would be if continuing while solvent---armed with the threat of bankruptcy, they demand revised terms of credit at lenders' expense.</Text>
            <Comments>Preference payments include most transfers to company directors (and connected persons) one (U.S.) or two (U.K.) years prior to insolvency.
For a discussion on voidable preferences in the U.K., see [\]\[][#Hill2014;]. In the U.S., paying directors to liquidate may also fall under[\]\[§152(6)][#US18;]---otherwise know as the "bankruptcy bribery" statue. It states that "a person who knowingly and fraudulently gives, offers, receives or attempts to obtain any money or property, remuneration, compensation, reward, advantage or promise thereof for acting or forbearing to act in the case under title 11; [...] shall be fined under this title, imprisoned not more than 5 years, or both". (See also [\]\[§727(a)(4)(C)][#US11;].)
In the U.K., all liquidations, regardless of solvency, are overseen by a liquidator with this responsibility. In the U.S., a liquidator is appointed only if the firm undergoes Chapter 7 liquidation or state-governed "Assignment for the Benefit of Creditors" (ABC) procedures. Yet, even if the borrower and creditor negotiate a settlement and liquidate assets outside official procedures, creditors may nevertheless recover preferences in bankruptcy court after concluding the sale. (For a discussion of this issue specific to ABC procedures---a generally weaker mechanism for recovering preferences---see[#Thorne2007;].)
Although altering the financial terms of a debt contract outside bankruptcy requires the unanimous consent of creditors[#UnitedStates1939], if accepted by at least a supra-majority, the firm can file for a pre-pack bankruptcy. Pre-pack bankruptcies fast-track approval of (or approve by default) workout agreements supported by a certain majority of creditors in each class.
&lt;!--\label{p1footnote3}--&gt;This assumption effectively grants the entrepreneur all bargaining power in bankruptcy. It is made for tractability--- without further constraints, multiple equilibria are possible. As shown in [][p1AppendixBargainingPower], however, as long as entrepreneurs extract some surplus during debt renegotiations, all conclusions in this paper hold. Additionally, granting the entrepreneur full power is most consistent with the original motivation of rescue culture. It is also very likely to hold for viable firms---*i.e.*, those that had originally planned to operate two periods and more susceptible to the stigma of failure and side effects of sudden unemployment[see, *e.g.*,\]\[][#Linn1985,Fay2002].
This discrepancy creates motive for strategic default [see\]\[][#Hart1998,Bolton1990]. As shown in [][p1appendixstrategicdefault], however, strategic default is only harmful when coupled with expensive reorganisation. Otherwise, it has no effect on the lending market and probably no effect on interim investment decisions.</Comments>
        </Document>
        <Document ID="28B4E2C9-F726-4822-A245-EF9F1C2B7CDE">
            <Title>Empirical evidence</Title>
        </Document>
        <Document ID="D7631746-6548-4E03-9F4E-22B795937D52">
            <Title>Alternative hypotheses?</Title>
        </Document>
        <Document ID="9FC896E1-7B41-4A06-88F0-211DC8101806">
            <Title>[][Quantification], supplemental output</Title>
        </Document>
        <Document ID="2416FA7C-EC60-4A59-894E-511C6F249ADF">
            <Title>Descriptive statistics</Title>
            <Text>[](#figure4) breaks down abstract readability by publication year and primary *JEL* classification for the data analysed in this paper.
</Text>
        </Document>
        <Document ID="D46F9339-CF33-4A6A-912C-955BC643A9E9">
            <Title>Women are under-represented in top-four economics journals</Title>
            <Text>&lt;!--\input{$HOME/Dropbox/Readability/draft/0-tex/fixed/Figure-2}--&gt;
The right-hand graph in [](#figure5) illustrates women's representation in top-four economics journals over time. The number of papers these journals publish with at least one female author has been steadily increasing over the past several decades---from about 5 percent in the late 1980s to around 25 percent in 2015. Growth in the average share of female authors per paper, however, is more muted: it was about 3--4 percent in the late 1980s; by 2015 it had only increased to 15 percent. The discrepancy between these two figures is because growth in female authorship is largely thanks to an increase in the number of mixed-gendered papers authored by a strict minority of women. In fact, there has been almost no growth in the percentage of papers that are either majority or exclusively female-authored: both figures have hovered around 4--5 percent since the early 1990s. *Econometrica* publishes the fewest exclusively female-authored papers (3 percent of all papers published since 1990), *AER* the most (6 percent); *JPE* and *QJE* fall in between (4 and 5 percent, respectively). Percentages are only slightly higher (and rankings identical) for papers with a strict majority of female authors.
The left-hand graph in [](#figure5) plots the average percentage of female authors per paper across primary *JEL* categories for articles published between 1990--2015. There are clear differences across fields. The average percentage of female authors per paper was lowest in *JEL* codes B (history of economic thought, methodology and heterodox approaches), C (mathematical and quantitative methods), D (microeconomics) and E (macroeconomics and monetary economics) and highest in I (health, education and welfare), Z (other special topics) and O (economic development, innovation, technological change, and growth). Despite this variation, the percentage of female authors per paper does not exceed 20 percent in any field.</Text>
        </Document>
        <Document ID="B9E300B1-D65F-4BB3-9E9D-20DC5718ABCA">
            <Title>Econometrica</Title>
        </Document>
        <Document ID="64F0F44E-6C57-4044-BED3-AEFDC3A4AD3F">
            <Title>[](#table8), alternative measures of an article's "gender"</Title>
            <Text>[](#table8XA), [](#table8XB) and [](#table8XC) repeat the analysis shown in [](#table8) using three alternative measures of an article's "gender". In [](#table8XA), papers authored entirely by women are compared to papers authored entirely by men. In [](#table8XB), papers are considered "female" if at least one author is female. In [](#table8XC), article gender is represented by a binary variable equal to one if at least half of all authors are female; articles below that threshold with at least one female author are excluded.
&lt;!--


\clearpage--&gt;</Text>
        </Document>
        <Document ID="EE478991-6BD5-46F1-8A38-DA02AFB8E93A">
            <Title>[](#table4), alternative program for calculating readability scores</Title>
        </Document>
        <Document ID="AFBB5973-031A-40E0-8A4F-DB041B3876A3">
            <Title>Summary statistics</Title>
        </Document>
        <Document ID="CDA135C4-86E2-4B9D-B900-38896CD643A1">
            <Title>Tables</Title>
            <Text>&lt;!--\input{$PPATH/p2/tables/tex/p2tableB9}--&gt;</Text>
            <Notes>&lt;!--\end{appendices}--&gt;</Notes>
        </Document>
        <Document ID="E4E41B8C-CCB7-4324-AD59-21BB8D756BBE">
            <Title>Mahalanobis matching</Title>
            <Text>[](#table10XA) and [](#figure4XA) replicate the analysis in [][SEUMatching] but instead of using propensity score matching to generate matched pairs, it uses a Mahalanobis matching procedure. The names of the economists in each matched pair are displayed in [](#tableC14XA). The variables used to match authors are identical to those used for the matches based on propensity scores, outlined in [][SEUMatching].
&lt;!--
\vfill
\input{$HOME/Dropbox/Readability/draft/tex/generated/table10XA}
\vfill
\input{$HOME/Dropbox/Readability/draft/tex/fixed/figure4XA}
\vfill
\clearpage
\input{$HOME/Dropbox/Readability/draft/tex/generated/tableC14XA}
\clearpage
--&gt;</Text>
        </Document>
        <Document ID="905F92B9-1587-4170-AA3E-AE56BFE8DE0F">
            <Title>Untitled</Title>
        </Document>
        <Document ID="489FA0A0-FB53-490C-9DE4-5CB820ED9C7D">
            <Title>Article-level analysis</Title>
            <Text>[](#p3table4)presents coefficients from an ordinary least squares regression of ratio of female co-authors on the five readability scores. To account for error correlation by editorial policy, observations are grouped by journal editor/editorial board and standard errors are adjusted accordingly.
Column (1) controls for journal: abstracts written only by women score about one point higher on the Flesch Reading Ease scale; according to the four grade-level measures, they take 1--6 fewer months of schooling to understand. Percentage-wise, women write between 1--3 percent better than men.
Column (2) includes 63 year dummies; column (3) adds another 182 journal and year interaction dummies; column (4) introduces the 98 institution dummies. Controlling for time and institution has little effect. Coefficients and standard errors are very similar to those in the first column.
The coefficients on the journal dummies in (2) are presented in [](#p3table5).They compare *AER*'s readability to the readability of *ECA*, *JPE* and *QJE*, providing a useful check on the reliability of readability formulas in the context of economic writing. As intuitively expected, all five scores agree that *Econometrica* is harder to read; four out of five scores suggest *JPE* is, too, while *QJE* is easier.
Column (5) in [](#p3table4) controls for primary *JEL* classification. Since only post-1990 *JEL* classifications are used, estimates in (5) exclude roughly 40 percent of the data. Nevertheless, coefficients are roughly equivalent---with the exception of the Flesch Reading Ease score which halves and loses significance.
[](#p3figure4) displays results from an ordinary least squares regression on the Dale-Chall score; regressors are: (i) ratio of female co-authors; (ii) dummies for each primary *JEL* code, (iii) interactions from (i) and (ii) and (iv) controls for journal, year, institution and editorial board. Due to small sample sizes---particularly of female authors---[](#p3figure4) includes 549 articles from *AER Papers &amp; Proceedings*. *AER Papers &amp; Proceedings* is coded as a separate journal and edited by the American Economic Association's president-elect.
The pink vertical line in [](#p3figure4)'s left-hand graph is the marginal effect of female authorship at the mean. Its estimate coincides with results in [](#p3table4)---women's papers require six fewer weeks of schooling to understand---and is highly significant (standard error 0.04). Points reflect marginal effects across *JEL* classification (bars represent 90 percent confidence intervals from standard errors clustered by editor). Women earn higher marks for clarity in 12 out of 15 categories; only three are significant: Q (Agricultural and Natural Resource Economics; Environmental and Ecological Economics), N (Economic History), and J (Labour Economics). Men may be better writers in L (Industrial Organisation), O (Economic Development, Innovation, Technological Change, and Growth) and H (Public Economics); none, however, are statistically different from zero. [](#p3figure4)'s right-hand graph displays coefficients from interacting the ratio of female co-authors with each *JEL* code. Q and N are significantly above the mean, O and H significantly below it. Remaining categories are not statistically different from the mean effect.
In general, sample sizes are small and estimates imprecise---only Labour Economics and Microeconomics contain more than 100 papers written only by women (the others average 35). Nevertheless, [](#p3figure4) suggests two things. First, the mostly insignificant interaction terms indicate outlier fields are probably not driving journals' gender readability gap---nor is any specific field bucking the trend. Second, the number of women in a field appears to have little effect on the size of the gap: Agriculture/Environment has one of the lowest concentrations of female-authored papers---but Economic History has one of the highest (Labour Economics falls between the two). Admittedly, Economic History papers are still overwhelmingly---as in 74 percent---penned just by men. But given the readability gap is present in subfields with both above- and below-average rates of sole female authorship, women may need to be better writers even where more of them publish.</Text>
            <Comments>\input{/Users/erinhengel/Desktop/tables/tex/p3table4}
*AER* and *Econometrica* employed a single individual to oversee editorial policy for the journals and time periods covered by the data. *JPE* and *QJE* do not generally name a single individual and instead rely on editorial boards composed of four to five faculty members at the University of Chicago and Harvard, respectively. Editorial boards are considered distinct if they differ by at least one member. In total, 74 groups are formed in this manner. Clustering at the journal-, volume- or paper-level results in very similar standard errors. 
Coefficients from regressions on Flesch-Kincaid, Gunning Fog, SMOG and Dale-Chall scores represent the marginal effect in years of schooling, multiplied by negative one. Monthly figures found by multiplying each by 12.
Quotient of the coefficient on ratio of female co-authors divided by the regression constant.
\input{/Users/erinhengel/Desktop/tables/tex/p3table5}
&lt;!--\label{fn2}--&gt;Codes A, B, P and M dropped due to insufficient number of female-authored papers: each had fewer than 10 papers authored only by women. No paper is classified under category Y.
&lt;!--\label{fn1}--&gt;*AER Papers &amp; Proceedings* does not publish abstracts in its print version; only select years and papers are available online (2003 and 2011--2015), all of which are included.
\input{/Users/erinhengel/Desktop/figures/tex/p3figure4}</Comments>
        </Document>
        <Document ID="7B75AF10-D60E-4946-A17B-AA58B5A61C3F">
            <Title>Discrimination in equilibrium</Title>
            <Text>Discrimination has consequences for people being discriminated against. Women, unlike atoms or rocks, actually have some interest in the outcome of things, and generally attempt to affect those outcomes and thereby pursue our own interests.
Because discrimination alters the gains to search for minority workers, in equilibrium, minority workers are less well matched than are other workers.
In equilibrium search models, employers have a degree of monopsony power that they may exploit.
The presence of firms with a distaste for minority workers reduces the value to continued search for minority workers, and firms will exploit this increased monopsonistic power and offer lower wages to minority workers.
Nonminority workers experience lower wages and have less favourable matches as the fraction of minority workers increases.
Yet, as I highlight in [][SEUModel], this still a conservative estimate of discrimination. Indeed, the only truly accurate test for discrimination will simply never be feasible: the ideal test would compare a woman plausibly experiencing one particular form of discrimination to a version of herself identical on all other dimensions except that she definitively did not. Although the same can be true of any identification strategy, the difficulty when discrimination is based on overt physical cues like gender and race is that one simply cannot plausibly randomise it. Moreover, due to the nature of discrimination, if one person with a particular feature experiences discrimination, *every person* with that feature will experience it as well. Thus we cannot somehow randomise whether subjects receive the treatment (discrimination) either.
The ideal test for discrimination randomises for discrimination holding gender fixed. This neatly defines the marginal impact of discrimination as the difference between the experience of a woman plausibly experiencing one particular form of discrimination and the experience of a version of herself who didn't, but is otherwise identical to her on all other dimensions.
Unfortunately, due to the nature of discrimination and its reliance on overt, physical cues, if one person with a particular feature experiences discrimination, *every person* in that same environment with that same feature probably experiences it as well. (And in any case, we have no reliable means of identifying women similar to her that did not.)
Thus, we are left randomising gender, which is just a *proxy* of discrimination. This introduces several problems. First, because gender is at best just a proxy of discrimination, it will in most circumstances be biased toward zero due to classical measurement error. Second, as highlighted in [](#figure6), 
[](#figure6) and [](#table12) document evidence that female economists are held to relatively constant---albeit higher---standards throughout their careers. Over time, women adjust to those standards by writing more clearly before peer review. Assuming---as other evidence suggests---that women's papers are accepted no more often than men's papers, this implies that female economists at every level of seniority must work harder than their male peers to achieve a similar outcome.
[](#figure6) also highlights that information and behavioural adaptions to that information influence the weight women place on the direct and indirect effect. Thus, studies that are looking for direct effects of discrimination are best investigating people who have not fully learned about---and therefore internalised---the extent to which they experience discrimination. Studies interested in measuring the direct effect must instead look at people who have.
More importantly, trying to recover one or the other effects without considering information and behavioural adaptions to that information will obscure the impact of discrimination. For example, if we try to look for an indirect effect among inexperienced economists, we won't find one---just like we won't find any evidence of a direct effect if we study only experience economist. Indeed, this latter example poses a particular identification conundurm, since senior female economists adjust to biased treatment in ways that confuse underlying discrimination with voluntary choice.
Moreover, even viewing one effect in isolation over increasing $t$ can give a misleading impression of the data generating process. For example, panel one in [](#table12) suggests that the readability gap declines over increasing $t$. This narrow view favours alternative explanations---*e.g.*, sensitivity, poor information and/or justified statistical discrimination---over bias by referees and/or editors.
Only when complemented by [](#figure6) do we fully appreciate that the smaller gap *in* peer review is completely offset by a wider gap *before* peer review. Both observations suggest that studies must account for all relevant decisions at a single point in time *and* the evolution of those decisions over time. Otherwise, they may underestimate discrimination and misallocate responsibility.
Lack of correlation will not imply lack of causality. As [](#figure6) illustrates, this is particularly true in equilibrium. Just like the path of a boat is not affected by the wind pushing its side when it is helmed by an seasoned skipper. Referee demands do not affect writing in peer review when the paper in question is written by an experienced female economist. In equilibrium, the response conceals the cause.
Workers, borrowers and buyers choose which jobs to interview for, which banks to apply to for loans and which dealerships to buy cars from. These choices are what generate Becker's distinction between the average and the marginal discriminator. If workers are aware of differences in discriminatory treatment among employers, they may focus their search on employers less likely to discriminate against them. Or, workers expecting to face more discrimination of the sort measured by audit studies may choose to apply to more jobs. Similar behaviour might endogenously arise in other types of markets. The result of these endogenous behaviours could lead to smaller racial or gender differences in wages than would be indicated by average differences in callback or job interview rates among employers. We need to be more careful about distinguishing these subtle nuances when investigating discrimination.
Victims of discrimination can and do protect themselves from discrimination by adjusting their strategies for dealing with the situation. But even when this is possible, it still represents an important type of discrimination because it limits the strategies women can use. This is especially true if the alternative path is significantly more onerous. "If whites need only bargain for four hours to negotiate a low markup, but blacks must negotiate for eight hours, then a finding that blacks in equilibrium did not pay more than whites for cars would not mean that blacks were uninjured by the dealerships' disparate treatment.
In outcome tests, there's a lot of focus on the infra-marginality problem and omitted variables but less focus on how individuals *react* to discrimination. However, behavioural changes in response to discrimination generates systematic variation in the observed thresholds that is correlated with discrimination. For example, if referees apply a higher threshold to women, and women adapt by writing more clearly ex ante, then in equilibrium the marginal effect of being a woman in peer review may disappear because women have absorbed the discrimination and decided instead to apply it to how well they write ex ante.
When estimating marginal effects of discrimination, it matters where one is on the equilibrium path. When in equilibrium, the marginal effect of the treatment---women in peer review---will be zero because it has been subsumed in the *infra marginal response*. The way people have adapted to discrimination changes how activity occurs at the margin. I call this the *dual marginal* effect of discrimination.
[](#figure6) also highlights the definitional difficulty at the heart of any study on discrimination: what really *is* discrimination? Even if we accept that both the direct *and* indirect versions are valid measures of discrimination, we reach another ambiguity---what exactly is the indirect effect? In [][SEUModel] and [][SEUMatching] I define it as the difference between the readability of the final version of an experienced woman's paper and the final readability of a comparably experienced man's paper. This roughly corresponds to the distance bracketed in [](#figure6).
On first blush, it may seem that the direct effect is somehow the more *natural* effect of discrimination. Yet a priori there is no theoretical reason why it should receive greater weight than the indirect effect. Both effects cause women to experience a more arduous peer review than they would choose to undergo if they did not experience discrimination.
On it's own, [](#time_exp) suggest statistical or attention discrimination specifically faced by younger women, </Text>
            <Comments>This study suffers from the same criticism. For example, it does not take into account the impact higher standards have on (potential) female economists' choice of field, specific topic or even their decisions to remain in the workforce.</Comments>
        </Document>
        <Document ID="1B6066FF-ADC1-48E0-A984-5C708FCDF967">
            <Title>[][ArticleLevel], suplemental output</Title>
        </Document>
        <Document ID="AF603859-1549-4308-8D91-C534680FADC1">
            <Title>[](#table5), alternative measures of an article's "gender"</Title>
            <Text>[](#tableC5a), [](#tableC5b) and [](#tableC5c) repeat the analysis shown in [](#table5) using three alternative measures of an article's "gender". In [](#tableC5a), papers authored entirely by women are compared to papers authored entirely by men. In [](#tableC5b), papers are considered "female" if at least one author is female. In [](#tableC5c), article gender is represented by a binary variable equal to one if at least half of all authors are female; articles below that threshold with at least one female author are excluded.


&lt;!--\clearpage--&gt;</Text>
        </Document>
        <Document ID="0AA73007-2541-415F-A159-0C285887C000">
            <Title>Introduction-1</Title>
            <Text>In economics, theoretical and empirical research on discrimination tends to focus on stereotype formation and belief structures motivating discriminatory actions[*e.g.*,\]\[][#Becker1957,Phelps1972,Arrow1973,Coate1993,Bordalo2016]. The present paper, in contrast, exclusively explores discrimination's impact on the behaviour and choices of people discriminated against.
This perspective has two advantages. First, it offers an alternative framework for studying the phenomenon. Discrimination is typically identified from the actions[*e.g.*,\]\[][#Neumark1996,Bertrand2004] and/or learning processes[*e.g.*,\]\[][#Altonji2001,Fryer2013] of those who discriminate. As I demonstrate here, however, repeatedly observing authors choices also reveals bias by editors and/or referees.
Outcome tests provide powerful evidence of discriminatory treatment[#Ayres2001]---but only if they credibly isolate group differences in the observed equilibrium from those that would have occurred in the non-discriminatory counterfactual equilibrium. Because it is generally assumed that men and women possess different preferences, knowledge about underlying processes and/or personality traits exacerbated by imperfect information, assuming no gender difference in the counterfactual equilibrium is difficult to justify.
Instead, I use information revealed by authors' repeated readability choices to reconstruct it: assuming preferences are fixed over time, earlier choices provide an upper bound on the impact intrinsic preferences play in gender readability gaps; assuming authors update beliefs about the relationship between readability and acceptance rates means later choices are made with more accurate beliefs.
Second, analysing discrimination from the perspective of people discriminated against forces us to think more deeply about its impact on, *inter alia*, occupational choice, worker motivation, human capital investment and productivity measurement. For example, higher standards undoubtedly cause collateral damage to women's productivity---unequal time spent making revisions leads to unequal time conducting new research; a result may write fewer papers. Fewer papers justifies fewer promotions. If women seek fairer employment elsewhere---or quit the labour force entirely---they feed a "Leaky Pipeline". This paper joins a small number of theoretical [*e.g.*,\]\[][#Lundberg1983,Lundberg1991,Goldin2014a]  show that discrimination can lead to suboptimal human capital investment when firms more reliably assess the productivity of members in one group relative to another. Occupational segregation by gender is one potential outcome in [#Goldin2014a;]'s "pollution" theory of discrimination. and empirical [*e.g.*,\]\[][#Parsons2011,Glover2017,Lavy2015] papers examining these effects.
These final results suggest women do not initially anticipate higher standards. Instead, they learn about them over time and adapt their *ex ante* writing style accordingly (feedback effects). Consequently, papers by junior female economists likely experience the toughest review. Consistent with this hypothesis, I find a significantly smaller---albeit still positive---gender gap in peer review times for senior women.
In particular, I find suggestive evidence that women increasingly submit better written papers *ex ante* to offset biased evaluation *ex post*. Thus, the readability gap between senior economists largely forms prior to---therefore appearing independent of---peer review. This pattern of behaviour suggests women internalise tougher standards with strategies that, perversely, may end up disguising it as voluntary choice.
Furthermore, my results advocate using caution when employing performance indicators in equations relating earnings (or other labour market outcomes) to gender. Higher standards raise quality at the expense of quantity. Performance indicators that weight the latter's fall more heavily than the former's rise will appear artificially low. If used to interpret gender wage gaps, they will undervalue women's work and confound estimates of labour market discrimination.
A similar argument was recently made in a study of racial preferences in Major League Baseball. [#Parsons2011;] find that race affects umpire calls, umpire calls influence players' behaviour and players' behaviour impacts performance metrics. As a result, common baseball statistics underestimate the talent of disadvantaged (usually minority) pitchers and overestimate the talent of advantaged (usually white) pitchers. An important contribution of my paper is to confirm this general point---as well as the presence of feedback effects---both in the context of gender discrimination and within a highly educated, professional working environment.
To the best of my knowledge, I am the first to suggest and document empirical evidence that women are held to higher standards in the peer review process. This main idea has been recently corroborated in the context of citations[#Card2019,Hengel2018a,Grossbard2018]. The fact that women's manuscripts may be subject to greater scrutiny and spend longer under review has been similarly found at a field journal[#Hengel2018b] and in a sample of 35 economics and finance journals[#Hengel2019]. It was not, however, present in a more recent set of journals that semi-overlap with the journals analysed here[#Card2019].
As we illustrate in [][AppendixHigherStandards], higher standards impose a quantity vs. quality trade-off that characterises many instances of female output. My results therefore identify another dimension to gender differences in labour market outcomes. Traditional hypotheses focus on obvious discrimination[#Goldin2000], motherhood[#Bertrand2010] and differences in behaviour[*e.g.*,\]\[][#Niederle2010]. Contemporary theories stress inflexible working conditions[#Goldin2014,Goldin2016], preferences[for a review, see, *e.g.*,\]\[][#Blau2016] and policy design[#Antecol2016]. Still other research---which this paper joins---target more subtle and ambiguous forms of discrimination[*e.g.*,\]\[][#Sarsons2015,Wu2017]. The gap probably emerges from all of these factors---and possibly many that are not yet identified. Equality means levelling the playing field in every single one.
This study does however build on (or join) extensive contemporary and parallel research on editorial patterns[#Ellison2002a,Card2013,Clain2017,Casnici2016], bias in editorial decisions[#Abrevaya2012,Card2017,Bransch2017] and female academics' lagging productivity and underrepresentation[#Ductor2018,Bayer2016,Ginther2004,Teele2017].
The remainder of the paper proceeds in the following order. [][MeasuringReadability] discusses readability scores. [][Data] describes the data and the gender representation of articles published in top economics journals. Analyses and results are presented in [][Results]. I close with a summary, [discussion][Discussion] and [conclusions][Conclusion].</Text>
            <Comments>This idea is conceptually related to the infra-marginality problem. See [#Knowles2001,Anwar2006,Anwar2015;] for discussions in the context of racial discrimination.
A similar idea was recently proposed in the philosophy literature[#Bright2017,Lee2016].
Another recent study might also illustrate this point.[#Glover2017;] find that obvious productivity measures decline when minority grocery store workers are overseen by biased managers. If due to demotivation or inattention by managers---as the authors propose---their behaviour reinforces statistical discrimination. On the other hand, slower checkout times, less overtime work and seeing fewer customers could result from biased managers being more critical of minorities' work (*e.g.*, minority workers are more likely to be punished for an incorrect amount of money in the till, not immediately clocking out at the end of a shift or accidentally scanning a single item multiple times).</Comments>
        </Document>
        <Document ID="DC0D39EB-E0F9-4045-8936-1BEAC5BC35C7">
            <Title>Estimation strategy</Title>
            <Text>Implementing [](#Corollary1) first requires that measurement occurs at time $t'$---*i.e.*, a point at which authors are sufficiently experienced for Assumptions 4 and 5 to hold. I assume this point occurs at or before authors' third top-four paper. Authors with one or two top-four publications are probably tenured and well-established in their fields. By publication three, all frequently referee (and some edit) prestigious economics journals. I assume this accumulated experience means: (a) equivalent authors are equally accurate about the standards they are being held to and remaining errors are no longer gender specific; and (b) those errors are getting smaller and smaller each time authors go through another round of peer review.
Additionally, in order to estimate $D_{ik}$, [](#Theorem1)'s Assumptions 1--3 must also hold. Most critically, Assumption 2 requires that $i$'s and $k$'s papers are identical with respect to topic, novelty and overall quality. I attempt to satisfy this assumption by matching every female author with three or more top-four publications to her closest male counterpart. Matches were made using a Mahalanobis procedure with the following co-variates: (1) maximum citation count over $t$; (2) institutional rank at $t=1$; (3) fraction of papers published per decade; (4) fraction of papers published by each journal; and (5) number of articles per primary *JEL* category. Co-variate balance pre- and post-match are shown in [][AppendixMatchingBalance]. [][AppendixMatchingNames] lists each matched pair.
Assume authors are indeed well-matched and also sufficiently experienced at $t=3$. Then under ideal circumstances, comparing $R_{i3}$ to $R_{i1}$ determines the impact information (as proxied for by experience) has on readability, conditional on gender (Condition 2); comparing $R_{i3}$ to $R_{k3}$ determines the impact of gender, conditional on information (Condition 1). Because of co-authoring, however, circumstances are not ideal. In particular, co-authoring means that article gender is neither fixed over $t$ conditional on $i$, nor is $i$'s and $k$'s experience---and hence information---necessarily identical at time $t=3$. I attempt to account for this by predicting $i$'s $t$th paper readability had it only been co-authored with members of $i$'s same sex. To do so, I reconstruct $i$'s time $t$ readability choice at female ratio equal to 1 for women and 0 for men using errors and coefficients from OLS estimation of [](#equation14) in the gender and time appropriate subsample of authors:
&lt;!--\input{$HOME/Dropbox/Readability/draft/0-tex/equations/equation13}--&gt;
 where $g_i=m,f$ if $i$ is male or female, respectively, "female ratio" defines papers with a strict minority of female authors as male-authored; for papers with 50 percent or more female authors, it is the ratio of female authors on a paper (see [][Gender] for more details) and $\varepsilon_{it}$ is the estimated error term. As long as $\varepsilon_{it}$ does not partially correlate with a paper's ratio of female authors conditional on $t$ and $g_i$, then $\widehat R_{it}$ provides an unbiased prediction of $R_{it}$. Regression output from [](#equation14) is shown in [][AppendixReconstruction]. To adjust for the degrees of freedom lost when generating $\widehat R_{it}$, standard errors in subsequent calculations are inflated by 1.05. [][AppendixSEUR] presents results using the unadjusted, observed $R_{it}$ instead of $\widehat R_{it}$.</Text>
            <Comments>Two notes on co-variate choice. First, I eschew mean, median and minimum citation counts in favour of the maximum on the assumption that an author’s "quality" is principally a function of his best paper. Second, most people are at top ranked institutions by $t=3$; by matching on institutions at $t=1$, I try to pair authors with similar career paths.
More specifically, I separately estimate [](#equation14) in the following four subsamples: (i) female authors at $t=1$; (ii) male authors at $t=1$; (iii) female authors at $t=3$; (iv) male authors at $t=3$. I then generate $\widehat R_{it}$ using the appropriate coefficients and errors for each author: (i) $\widehat R_{i1}=\alpha_{1f}+\beta_{1f}+\varepsilon_{i1}$ for a female $i$ at $t=1$; (ii)  $\widehat R_{i1}=\alpha_{1m}+\varepsilon_{i1}$ for a male $i$ at $t=1$; *etc.*</Comments>
        </Document>
        <Document ID="510FE4D4-748C-4E5E-B2EB-3D14433DD0AA">
            <Title>Identification-1</Title>
            <Text>
 =========================
Correlation between $\mu_{jW}$, $\text{female ratio}_j$ and/or $\vect X_{jW}$ biases OLS estimates of [](#p3equationAX1). Yet the 
The coefficient of interest is $\beta_{1P}$. 

 
Realistically, $\vep_{jP}$ has some impact on the unobserved components of the publication process captured by $\mu_{jP}$---for example, referees demand more readability edits when realisations of $\vep_{jW}$ are particularly low. [](#p3equationAX3) defines that correlation.
&lt;!--\begin{equation}\label{p3equationAX3}
\mu_{jP}=\zeta\,\vep_{jW}+\omega_j
\end{equation}--&gt;
 where $\omega_j$ is uncorrelated with $\vep_{jW}$, $\text{female ratio}_j$ and $\vect X_{jP}$. Correlation between $\mu_{jP}$ and $\vep_{jW}$ implies correlation between $\mu_{jP}$ and $\text{score}_{jW}$. OLS estimates of $\beta_{1P}$ are subsequently biased by partial correlation between $\text{female ratio}_j$ and $\text{score}_{jW}^s$.
[](#p3equationAX4) replaces $\mu_{jP}$ with [](#p3equationAX3) and then substitutes $\text{score}_{jP}^s$ and $\vep_{jW}$ with their predicted values (denoted by a carat) from unbiased estimates of [](#p3equationAX1).
&lt;!--\begin{equation}\label{p3equationAX4}
\text{score}_{jP}^s=\widehat{\text{score}_{jW}^s}+\beta_{0P}+\beta_{1P}\,\text{female ratio}_j+\bm\uptheta_P\,\vect X_{jP}+\zeta\,\widehat{\vep_{jW}}+\omega_j+\vep_{jP},
\end{equation}--&gt;
 Assuming $\omega_j$ is uncorrelated with the explanatory variables in [](#p3equation4), estimating [](#p3equationAX4) generates an unbiased estimate of $\beta_{1P}$. More likely, however, correlation between $\mu_{jW}$, $\text{female ratio}_j$ and/or $\vect X_{jW}$ biases OLS estimates of [](#p3equationAX1). [](#p3equationAX3), adopted from [#Ashenfelter1994;], defines the general structure of the underlying correlation.
&lt;!--\begin{equation}\label{p3equationAX5}
\mu_{jW}=\eta\,\text{female ratio}_j+\bm\updelta\,\vect X_{jW}+\bm\upgamma\,\vect X_{jP}+\omega_j,
\end{equation}--&gt;
 where $\omega_j$ is uncorrelated with $\text{female ratio}_j$, $\vect X_{jW}$ and $\vect X_{jP}$. Substituting [](#p3equationAX4) into [](#p3equationAX1) generates the following reduced form representation of working paper readability:
&lt;!--\begin{equation}\label{p3equationAX6}
\text{score}_{jW}^s=\beta_{0W}+\wt\beta_{1W}\,\text{female ratio}_j+\wt{\bm\uptheta}_W\,\vect X_{jW}+\bm\upgamma\,\vect X_{jP}+\wt\vep_{jW},
\end{equation}--&gt;
 where $\wt\beta_{1W}=\beta_{1W}+\eta$, $\wt{\bm\uptheta}_W=\bm\uptheta_W+\bm\updelta$ and $\wt\vep_{jW}=\vep_{jW}+\omega_j$. To obtain a reduced form for published article readability, substitute [](#p3equationAX5) into [](#p3equationAX2):
&lt;!--\begin{equation}\label{p3equationAX7}
\text{score}_{jP}^s=(\beta_{0W}+\beta_{0P})+(\wt\beta_{1W}+\beta_{1P})\,\text{female ratio}_j+\wt{\bm\uptheta}_W\,\vect X_{jW}+(\bm\uptheta_P+\bm\upgamma\vect)\,\vect X_{jP}+\wt\vep_{jP},
\end{equation}--&gt;
 where $\wt\vep_{jP}=\wt\vep_{jW}+\vep_{jP}$. [](#p3equationAX5) and [](#p3equationAX6) are explicitly estimated via feasible GLS (FGLS). $\beta_{1P}$ is identifiable post-estimation by subtracting reduced form coefficients.
GLS estimates of $\beta_{1P}$ may still be biased by arbitrary correlation between $\vect X_{jW}$ and $\mu_{jP}$. This bias is obviated, however, by subtracting $\text{score}_{jW}^s$ from both sides of [](#p3equationAX2):
&lt;!--\begin{equation}\label{p3equationAX8}
\text{score}_{jP}^s-\text{score}_{jW}^s=\,\beta_{0P}+\beta_{1P}\,\text{female ratio}_j+\bm\uptheta_P\,\vect X_{jP}+\mu_{jP}+\vep_{jP}.
\end{equation}--&gt;
 [](#p3equationAX8) is estimated via OLS; the coefficient on $\text{female ratio}_j$ is an unbiased estimate of $\beta_{1P}$ under the weakest identifying assumption of no partial correlation between $\text{female ratio}_j$ after controlling for $\vect X_{jP}$.
 ============================================
 ============================================
 is the weakest identifying assumption necessary to obtain an unbiased estimate of $\beta_{1P}$.
In the absence of $\mu_{jW}$, OLS $\widehat{\text{score}_{jP}^s}$ and $\widehat{\vep_{jW}}$ based on OLS estimation of [](#p3equationAX1) are unbiased; $\beta_{1P}$ from a corresponding two-stage-least-squares (2SLS) estimation of [](#p3equationAX4) are similarly unbiased.

Correlation between $\mu_{jP}$ implies correlation between $\text{score}_{jW}$ and $\mu_{jP}$; partial correlation between $\text{score}_{jW}^s$ and $\text{female ratio}_j$ then biases OLS estimates of $\beta_{1P}$ from [](#p3equationAX2)
Unless partial correlation between $\mu_{jP}$ and $\text{female ratio}_j$ is zero after controlling for $\text{score}_{jW}^s$ and $\vect X_{jP}$, 
Unfortunately, OLS generates unbiased estimates only under an implausible assumption: zero partial correlation between $\mu_{jP}$ and $\text{female ratio}_j$ 
 More realistically, $\text{score}_{jP}^s$ has some impact on the unobserved components of the publication process captured by $\mu_{jP}$. 
As long as $\text{female ratio}_j$ and $\mu_{jP}$ are not partially correlated, it is possible to obtain a consistent estimate of the coefficient of interest, $\beta_{1P}$. The simplest case assumes zero partial correlation between $\mu_{jP}$ and $\text{female ratio}_j$ after controlling for $\text{score}_{jW}^s$ and $\vect X_{jP}$; OLS estimates of [](#p3equationAX2) generate an unbiased $\beta_{1P}$.
More likely, however, $\text{score}_{jP}^s$ has some impact on an unobserved component of the publication process captured by $\mu_{jP}$. 
by eliminating selection bias from version-invariant omitted variables. To isolate gender differences in readability pre-existing peer review from those incurred during it, I use two separate (yet analogous) estimation strategies based on a conceptual framework outlined by[#Ashenfelter1994;]. The first strategy removes selection effects by regressing female ratio on the change in score via OLS. In the second strategy, I regress female ratio on abstract readability of NBER working papers and, separately, published articles using generalised least squares (GLS). Selection effects are eliminated post-estimation by differencing coefficients.
In the absence of $\mu_{jW}$, one might replace $\text{score}_{jW}^s$ in [](#p3equationAX2) with its predicted value and error from an OLS estimation of [](#p3equationAX1). The presence of $\mu_{jW}$, however, generate biased estimates of both. Instead, I define the general structure of the underlying correlation between $\mu_{jW}$ and the observable variables in both [](#p3equationAX1) and [](#p3equationAX2) based on the conceptual framework outlined in 
In the absence of $\mu_{jW}$, OLS of [](#p3equationAX1) produces an unbiased estimate of $\vep_{jW}$. Assuming $\mu_{jP}$ 
compensate for a low realisation of $\vep_{jW}$ by demanding more readability edits. 
In the absence of $mu_{jW}$, OLS estimates of [](#p3equationAX1) are unbiased; replace $\text{score}_{jW}^s$
replace $\text{score}_{jW}^s in [](#p3equationAX2) with its predicted value
It is possible to obtain an unbiased estimate of the coefficient of interest, $\beta_{1P}$, as long as the partial correlation 
A generalised least squares (GLS) regression of female ratio on abstract readability of NBER working papers and, separately, published articles, still produces an unbiased estimate of $\beta_{1P}$.
an unbiased estimate of $\beta_{1P}$ can still be obtained by regressing 
 using generalised least squares (GLS). 
Although correlation between $\mu_{jW}$, $\text{female ratio}_j$ and/or $\vect X_{jW}$ biases estimates of [](#p3equationAX1), selection effects are eliminated post-estimation by differencing coefficients.
, OLS generates unbiased estimates of [](#p3equationAX2).
Correlation between $\mu_{jP}$ and $\vep{jW}$ implies correlation between $\text{score}_{jW}$ and $\mu_{jP}$; OLS estimates of $\beta_{1P}$ are then biased by partial correlation between $\text{score}_{jW}^s$ and $\text{female ratio}_j$. In the absence of $\mu_{jW}$, one might replace $\text{score}_{jW}^s$ in [](#p3equationAX2) with its predicted value and error from an OLS estimation of [](#p3equationAX1). The presence of $\mu_{jW}$, however, generate biased estimates of both. Instead, I define the general structure of the underlying correlation between $\mu_{jW}$ and the observable variables in both [](#p3equationAX1) and [](#p3equationAX2) based on the conceptual framework outlined in 
partial correlation between $\text{female ratio}_j$ and $\text{score}_{jW}^s$ then biases OLS estimates of $\beta_{1P}$. 
Throughout this section, I assume (i) correlation between $\mu_{jW}$, $\text{female ratio}_j$ and/or $\vect X_{jW}$ biases OLS estimates of [](#p3equationAX1); but (ii) partial correlations between $\mu_{jP}$ and $\text{female ratio}_j$ are zero after controlling for $\vect X_{jP}$.
If partial correlations between $\mu_{jP}$ and $\text{score}_{jW}^s$ are similarly zero, OLS estimation of [](#p3equationAX2) generates unbiased estimates of the parameter of interest, $\beta_{1P}$. More likely, however, $\text{score}_{jP}^s$ has some impact on the unobserved components of the publication process captured by $\mu_{jP}$. For example, $\mu_{jP}$ and $\vep_{jW}$ could be correlated as would occur if a low realisation of $\vep_{jW}$ induces more readability edits during peer review. Alternatively, perhaps Dutch writers are both more likely to assiduously address referee comments and more likely to be highly productive economists. Partial correlation between $\text{female ratio}_j$ and $\text{score}_{jW}^s$ then biases the OLS estimate of $\beta_{1P}$.
f partial correlation between $\mu_{jP}$ and $\text{female ratio}_j$ are also zero after controlling for $\vect X_{jW}$, then a second method also generate unbiased estimates of $\beta_{1P}$. As mentioned, correlation between $\mu_j$ and and $\text{score}_{jW}^s$ variables biases the OLS parameter estimates $\beta_{1P}$ in  [](#p3equationAX2). To define that correlation, I adopt the general structure from[#Ashenfelter1994;]:</Text>
            <Comments>$\beta_{0P}$, $\bm\uptheta_P$ and $\bm\upgamma$ are similarly identifiable post-estimation. $\beta_{1W}$, $\eta$, $\bm\uptheta_W$ and $\bm\updelta$ are not.</Comments>
        </Document>
        <Document ID="3D4820AE-D0B6-4745-94A3-34AFDB24DBA2">
            <Title>Robustness</Title>
            <Text>Conclusions drawn in [][MatchingResults] are predicated on several strong assumptions. First, all results depend on match accuracy. Post-match co-variates are well balanced (see [][AppendixMatchingBalance]). They remain well balanced---and similar to the matched population---when restricted to pairs satisfying $D_{ik}\ne0$ (see Appendix M.1 in the [August 2018](http://www.erinhengel.com/research/publishing_female20180828.pdf) version of the paper). To facilitate further scrutiny, [][AppendixMatchingNames] lists the names of economists in each pair. Matches using alternative variables (*e.g.*, minimum citation counts, mean institutional rank or fraction of articles per primary *JEL* category) and specifications (*e.g.*, propensity score matching) generate similar figures and conclusions.
Additionally, authors must be sufficiently experience at $t'$ for Assumptions 4 and 5 in [](#Corollary1) to hold. I assume this point occurs at or before authors' third top-four paper. Fifty percent of women with three or more top publications satisfy Conditions 1 *and* 2 when compared to equivalent men. Among them, $D_{ik}$ is far from zero: these women write, on average, 21 percent more clearly than equivalent men with identical experience. I believe it is unlikely that half of all female economists with three top publications---plus many more second-tier publications and substantial experience refereeing and editing themselves---make mistakes of this magnitude.
To generate the counterfactual [$\widehat R_{it}$](#equation14), I assume unobserved co-author characteristics do not partially correlate with $\text{female ratio}_{it}$, conditional on $i$'s gender and experience. To test the robustness of this assumption, [[](#tableH2_Fem100)][AppendixExclusive] replicates [](#tableH2_FemRatio) on exclusively, majority and senior female-authored papers. I have also repeated the analyses shown in [](#table8_base) and [](#figure8_base) without adjusting for [$\text{female ratio}_{it}$][AppendixSEUR] and on subsets of matched pairs in which the woman's $t=1$ and $t=3$ papers are solo- or exclusively female-authored (16), majority female-authored (20) or at least 50 percent female-authored (76). Although sample sizes for the latter three analyses are small, they also find $D_{ik}\ne0$ in about 70--75 percent of matched pairs; most of those (70 percent) indicate higher standards against the female member; the impact across all five scores also averages about 5 percent.
Moreover, experience appears to be the only $t$-varying factor driving within $i$ changes in readability. [](#tableH2_FemRatio) and additional analyses in a 2016 version of this paper[\]\[pp. 23--24][#Hengel2016] show an identical pattern despite controlling for a large array of potential confounders. In a 2017 version, I reconstructed $\widehat R_{it}$ using several $t$-varying factors (number of co-authors, institutional rank, institutional rank of the highest ranked co-author, $t$ for the most experienced co-author, publication year and dummies for each journal)[\]\[pp. 30, 61][#Hengel2017]; [][AppendixSEUJEL] adds *JEL* classification codes to [](#equation14). In [[](#tableH2_Fem100)][AppendixExclusive], I restrict [](#tableH2_FemRatio)'s analysis to solo-authored papers or those co-authored by members of the same sex. In all instances, women's readability is consistently shown to increase with $t$; when comparable results are estimated, they are similar to those presented in [](#table8_base) and [](#figure8_base).
Finally, accurate quantification requires that three additional criteria are also met. Assuming higher standards for $i$: (i) $i$'s acceptance rate is no more than $k$'s; (ii) $i$'s draft readability is at least as high as $k$'s; and (iii) $i$'s draft readability at $t=3$ is at least as high as his draft readability at $t=1$. As already discussed in [][MechanismsDescriptive], (i) rules out the possibility that $i$ is appropriately rewarded (relative to $k$) for writing more clearly. (ii) and (iii) eliminate situations in which women write more clearly during peer review in order to compensate for poorer writing---and consequently higher desk rejection rates---before peer review.
Unfortunately, my data do not perfectly identify acceptance rates nor do I have $t=1$ and $t=3$ draft readability scores for every matched pair. Nevertheless, the data I do have and prior research suggest (i)--(iii) not only hold on average, but do not exert upward bias on my estimate of $D_{ik}$, more generally. First, I reviewed the literature on gender neutrality in journals' acceptance rates in [][MechanismsDescriptive] and [][AppendixAcceptance]; women are not accepted more often than men. Results and conclusions are similar when I attempt to adjust for acceptance rates explicitly by also requiring that $T_{i}\le T_{k}$ for matched pairs in which $i$ is held to higher standards relative to $k$ (see Appendix M.4 in the [August 2018](http://www.erinhengel.com/research/publishing_female20180828.pdf) version of the paper). As shown in [][NBER], women's draft papers are indeed more readable than men's. [][IndirectEffect] provides further confirmation. [](#figure9) plots the readability of women's and men's draft and published papers over increasing $t$. Women's drafts are more readable than men's drafts at $t=3$ *and* their own drafts at $t=1$.
&lt;!--\clearpage--&gt;</Text>
            <Comments>See [\]\[pp. 30--33][#Hengel2017;] for propensity score matches from a probit model performed with replacement and using a wider array of co-variates.</Comments>
        </Document>
        <Document ID="3ED6CBCB-3CF0-4241-B5E3-CB8A9B50649F">
            <Title>[](#table7), alternative measures of an article's "gender"</Title>
            <Text>[](#tableXA), [](#tableXB) and [](#tableXC) repeat the analysis shown in [](#table7) using three alternative measures of an article's "gender". In [](#tableXA), papers authored entirely by women are compared to papers authored entirely by men. In [](#tableXB), papers are considered "female" if at least one author is female. In [](#tableXC), article gender is represented by a binary variable equal to one if at least half of all authors are female; articles below that threshold with at least one female author are excluded.
&lt;!--



\clearpage--&gt;
</Text>
        </Document>
        <Document ID="FF57AED3-6EAB-4B34-9983-D1C235D464C7">
            <Title>Estimation strategy</Title>
            <Text>In this section, I analyse readability changes that occurred during peer review by comparing abstracts pre- and post-review. My first estimation strategy simply regresses each paper's change in score on its gender composition. To understand it, note that the readability of a published paper depends on its earlier draft readability as well as factors that affect writing clarity any time after it was initially drafted:
&lt;!--\input{$HOME/Dropbox/Readability/draft/0-tex/equations/equation1}--&gt;
 where $R_{jP}$ and $R_{jW}$ are readability scores for working ($W$) and published ($P$) versions of paper $j$, respectively. $\beta_{1P}$ is the coefficient of interest and reflects the particular impact $\text{female ratio}_j$ has in peer review. $\vect X_{jP}$ and $\mu_{jP}$ are $P$-specific observable and unobservable components, respectively. $\vep_{jP}$ is $P$'s error term.
Correlation between $R_{jW}$ and $\text{female ratio}_j$ may bias OLS estimates of $\beta_{1P}$. [](#equation3) eliminates the distortion by subtracting $R_{jW}$ from both sides of [](#equation2):
&lt;!--\input{$HOME/Dropbox/Readability/draft/0-tex/equations/equation2}--&gt;
 Assuming zero partial correlation between $\text{female ratio}_j$ and $\mu_{jP}$, OLS generates an unbiased estimate of $\beta_{1P}$.
An alternative strategy based on[#Ashenfelter1994;] separately estimates gender differences in the draft and final versions of papers using generalised least squares (GLS). The contemporaneous effect of peer review is identified post-estimation by subtracting coefficients. To implement this set-up, I combine [](#equation2) with: (i) the relationship between readability scores and the gender composition of a paper before peer review; and (ii) an equation accounting for potential correlation between observable controls and version-invariant unobservables. The former is defined as:
&lt;!--\input{$HOME/Dropbox/Readability/draft/0-tex/equations/equation3}--&gt;
 where $\beta_{1W}$ reflects $\text{female ratio}_j$'s impact on readability prior to peer review; $\vect X_{jW}$ and $\mu_{jW}$ are version-invariant observable and unobservable components, respectively; $\vep_{jW}$ is version $W$'s error term. [](#equation5) then defines a general structure for potential correlation between $\mu_{jW}$ and observable variables in both [](#equation4) and [](#equation2):
&lt;!--\input{$HOME/Dropbox/Readability/draft/0-tex/equations/equation4}--&gt;
 where $\omega_j$ is uncorrelated with $\text{female ratio}_j$, $\vect X_{jW}$ and $\vect X_{jP}$. Substituting [](#equation5) into [](#equation4) generates the following reduced form representation of $R_{jW}$:
&lt;!--\input{$HOME/Dropbox/Readability/draft/0-tex/equations/equation5}--&gt;
 where $\wt\beta_{0W}=\beta_{0W}+\gamma$, $\wt\beta_{1W}=\beta_{1W}+\eta$, $\wt{\bm\uptheta}_W=\bm\uptheta_W+\bm\updelta_W$ and $\wt\vep_{jW}=\vep_{jW}+\omega_j$. $R_{jP}$'s reduced form is similarly found by substituting [](#equation6) into [](#equation2):
&lt;!--\input{$HOME/Dropbox/Readability/draft/0-tex/equations/equation6}--&gt;
 where $\wt{\bm\uptheta}_P=\bm\uptheta_P+\bm\updelta_P$ and $\wt\vep_{jP}=\wt\vep_{jW}+\vep_{jP}$. [](#equation6) and [](#equation7) are explicitly estimated via feasible GLS (FGLS). $\beta_{1P}$ is identified post-estimation by subtracting reduced form coefficients. Assuming zero partial correlation between $\mu_{jP}$ and $\text{female ratio}_j$, it also generates an unbiased estimate of $\beta_{1P}$.</Text>
            <Comments>&lt;!--\label{FootnoteColliders}--&gt;Note that zero correlation between $\text{female ratio}_j$ and $\mu_{jP}$ does not preclude biased estimates of $\beta_{1P}$ when $\mu_{jP}$ is correlated with other explanatory variables that are, in turn, correlated with $\text{female ratio}_j$ by some factor independent of $\mu_{jP}$. Unbiasedness instead requires zero *partial* correlation between $\mu_{jP}$ and $\text{female ratio}_j$.</Comments>
        </Document>
        <Document ID="13535585-A497-4FFA-B94B-30B8D25B5E5E">
            <Title>Background</Title>
        </Document>
        <Document ID="6EA6F09A-3D8E-4793-9BF9-CC0684F77E53">
            <Title>[](#table8), equality test statistics and male effects</Title>
            <Text>[](#tableC6) displays $\chi^2$ test statistics from Wald tests of [$\beta_1$](#equation1) equality across estimation results in [](#table8). 
&lt;!--\input{$HOME/Dropbox/Readability/draft/tex/generated/tableC6}--&gt;
&lt;!--\clearpage--&gt;</Text>
        </Document>
        <Document ID="3A0100A1-0A5D-47DB-B485-BA27DDE45FDC">
            <Title>Male effects</Title>
        </Document>
        <Document ID="9FF77E42-25B4-48AC-92C9-B6AC81BFE131">
            <Title>Solo-authored</Title>
            <Text>&lt;!--
\begin{vplace}[0.7]
\input{$HOME/Dropbox/Readability/draft/0-tex/generated/Table-3-FemSolo}
\end{vplace}
\clearpage
\input{$HOME/Dropbox/Readability/draft/0-tex/generated/Table-5-FemSolo}
\clearpage
\input{$HOME/Dropbox/Readability/draft/0-tex/generated/Table-6-FemSolo}
\clearpage
\input{$HOME/Dropbox/Readability/draft/0-tex/generated/Table-7-FemSolo}
\clearpage
\input{$HOME/Dropbox/Readability/draft/0-tex/generated/Table-8-FemSolo}
\clearpage
\input{$HOME/Dropbox/Readability/draft/0-tex/generated/Table-F.2-FemSolo}
\clearpage
--&gt;</Text>
        </Document>
        <Document ID="13474993-1A32-496D-A5D5-6E852669517B">
            <Title>Measuring higher standards</Title>
            <Text>[](#Theorem1) principally relies on two identifying assumptions: (i) at time $t'$,  $i$ and $k$ write papers that are identical with respect to topic, novelty and quality but potentially differ on readability; and (ii) $t'$ is sufficiently large---*i.e.*, any errors in $i$'s beliefs about $\widetilde R_i^s$ are on a path converging to zero. [](#Corollary1) assumes a more specific belief structure at $t'$ in order to (conservatively) measure discrimination's impact on readability choices.
Let $e_{it}^s$ be $i$'s time $t$ error in beliefs about $\widetilde R_{i}^s$ and define $\overline s$ as the review group in $\Sigma_{A_{it}}$ for which $i$ believes $\widetilde R_{i}^s$ is highest. If $i$ and $k$ are equivalent at $t&gt;t'$ and $e_{it}^s=e_{kt}^s$, then [](#Corollary1) shows that $R_{it}-R_{kt}$ is *smaller* in magnitude than $\widetilde R_{i}^{\overline s}-\widetilde R_{k}^{\overline s}$, which is the extent of $\overline s$'s discrimination against $i$ relative to $k$.
&lt;!--\input{$HOME/Dropbox/Readability/draft/theorems/corollary}--&gt;
[](#Corollary1) conservatively measures the impact of discrimination on $i$'s readability. It also exposes the toxic influence of a single biased $s$: $i$'s time $t$ readability choice depends on discrimination at stage 1 by the group of referees that actually reviewed his paper ($s$) as well as discrimination at stage 0 by another review group that (probably) didn't ($\overline s$).
[](#Corollary1) adds two stronger conditions to [](#Theorem1). First, $i$ and $k$ must be comparably experienced by time $t$. Second, if $s'\in\Sigma_{A_{it}}$ then $s'\in\Sigma_{A_{kt}}$. This second condition might not be satisfied if, *e.g.*, $i$'s utility of acceptance exceeds that of $k$'s so he works harder to appease the demands of a particularly tough group of reviewers. Nevertheless, $i$'s unconditional acceptance rate is not higher than $k$'s (Condition 3), so there must also exist some other $s''$ that applies higher standards to $i$'s work than it does to $k$'s (thus, $s''\in\Sigma_{A_{kt}}$ but $s''\not\in\Sigma_{A_{it}}$). However, [](#equation12) may not fully counteract the first effect relative to the second (see the proof in [][AppendixProofs]). [](#equation13) does. It therefore conservatively estimates $D_{ik}$ when $\Sigma_{A_{it}}\not\subset\Sigma_{A_{kt}}$:
&lt;!--\input{$HOME/Dropbox/Readability/draft/equations/equation13}--&gt;
 where $t''&lt;t$ is defined in Condition 2 of [](#Theorem1).</Text>
            <Comments>[](#Corollary1) actually applies under the weaker $e_{nit}^s\le e_{nkt}^s$, $n=0,1$ (see its proof in [][AppendixProofs]).
[](#equation13) does come with a cost: its conservative bias is much larger than the one generated by [](#equation12).</Comments>
        </Document>
        <Document ID="1CA292A5-32B3-4C5A-969D-EC3DBF1AE137">
            <Title>Notes</Title>
        </Document>
        <Document ID="2441C81C-90C3-4585-B581-A7FCCCC66C0D">
            <Title>Estimation strategy and results</Title>
            <Text>For more precision on gender differences in the time-cost of review---and in order to condition explicitly on quality---I build on a model by [#Ellison2002a;]:
&lt;!--\input{$HOME/Dropbox/Readability/draft/0-tex/equations/equation9}--&gt;
 where $\text{female ratio}_j$ is the ratio of female authors on paper $j$ (papers with fewer than 50 percent female authors are classified as male, see [][Gender]), $\text{mother}_j$ and $\text{birth}_j$ are binary variables equal to 1 if article $j$'s authors were all mothers to children younger than five and gave birth, respectively, at some point during peer review, $\max.~t_j$ is the number of prior papers published in a top-five economics journal by article $j$'s most prolific co-author, $\text{no. pages}_j$ refers to the page length of the published article, $\text{order}_j$ is the order in which article $j$ appeared in an issue, $\text{citations}_j$ are the asinh-transformed number of subsequent papers citing $j$, $\text{flesch}_j$ is its Flesch Reading Ease score, the dummy variables $\text{theory}_j$, $\text{empirical}_j$ and $\text{other}_j$ account for how theoretical vs. empirical a paper is, and $\vect X_j$ captures additional fixed effects.
I first estimate [](#equation16) on data from *Econometrica*. I then re-estimate it excluding readability, motherhood and childbirth controls---which I do not have for papers published in *REStud*---on the entire sample and each journal separately.
&lt;!--\input{$HOME/Dropbox/Readability/draft/0-tex/generated/Table-6-FemRatio}--&gt;
[](#table10_FemRatio) displays results for *Econometrica*. All models include editor, acceptance year and institution fixed effects. Column (1) does not control for motherhood or childbirth; (2) drops papers authored entirely by women who had children younger than five and/or gave birth during peer review; (3) controls for motherhood but not childbirth; (4) controls for childbirth but not motherhood; (5) controls for both childbirth and motherhood; (6) and (7) restrict the sample to papers published after 1990; (7) includes fixed effects for primary *JEL* categories.
Every paper published in *Econometrica* undergoes extensive review, but the consistently large and highly significant coefficient on female ratio suggests women bear the brunt of it. The average male-authored paper takes about 18.5 months to complete all revisions; papers by women need almost seven months longer.
Results pooling data from both journals and on each alone without readability, motherhood and childbirth controls are shown in [](#table11_FemRatio). Estimates from *Econometrica* (columns one and four) coincide with those shown in [](#table10_FemRatio). Women take 2--4 months longer in review at *REStud* (columns two and five). When observations from both journals are combined, female-authored papers take, on average, 3--6 months longer in peer review (columns three and six).
&lt;!--\input{$HOME/Dropbox/Readability/draft/0-tex/generated/Table-7-FemRatio}--&gt;
Remaining coefficients in [](#table10_FemRatio) and [](#table11_FemRatio) largely correspond to earlier estimates by [#Ellison2002a;]. Longer papers take more time to review, as do papers with more co-authors and (generally) those that appear earlier in an issue. Authors with an established publication history, highly cited papers and more readable papers enjoy faster reviews, although the latter effects are only noisily estimated. Papers classified as empirical take longer in review; papers classified as other spend less time under review. Finally, giving birth slows down review, but having a young child appears to have the opposite effect, at least in this particular sample.
[][AppendixQuantile] re-estimates column (5) in [](#table10_FemRatio) and the third column of [](#table11_FemRatio) using a quantile regression model. [][AppendixMotherhood] replicates [](#table10_FemRatio), column (5) altering the age-threshold on $\text{mother}_j$. The gender gap is positive and significant across the entire distribution; it also does not depend on the precise definition of motherhood.</Text>
            <Comments>See [][AppendixAlternativeYear] for results controlling for years of submission and publication, instead.
This result is consistent with [#Ginther2004;], who find that women with children are more productive than male and childless female doctoral recipients 10 years after receiving their Ph.D. I would interpret it with caution, however, given (i) counter-intuitive results, (ii) obtaining an unbiased estimate of $\beta_2$ was *not* this study's objective and (iii) $\text{mother}_j$ equals one for only a small number of articles in the sample.</Comments>
        </Document>
        <Document ID="5B7FA46D-1F81-4F7E-AF23-02266CE0366B">
            <Title>Rescue Culture, part 2</Title>
            <Synopsis>Chapter 1's model implies the following: (i) reducing bankruptcy's impact on future earnings improves financial contracting; but (ii) judicial accuracy has an ambiguous effect. I empirically test both hypotheses using data from a large, cross-country, firm-level survey. I find reorganisation's cost contributes a great deal to perceived financing constraints. Information, on the other hand, appears to have the anticipated ambiguous effect, although data limitations prevent definitively attributing it to the predictions of the model.</Synopsis>
        </Document>
        <Document ID="7DB3228B-44E4-4081-AD8B-DD93EE91A189">
            <Title>Abstract word limits</Title>
            <Text>In [][NBER], I argue that the gender gap in the changes in readability between draft and final versions of a paper likely occur because of the peer review process. Yet NBER working paper abstracts can be of any length while abstracts published in *Econometrica* and *AER* cannot---they are restricted to 150 and 100 words, respectively. Observed readability gaps could consequently result from gender differences in how authors conform to these limits.
To test this hypothesis, I replicated the analysis described [][NBERResults] (and shown in [](#table6_FemRatio)) on the subset of articles with draft abstracts below the official minimum word limit of the journals in which they were eventually published. Results are shown in [](#table6_wordlimit). Despite dropping about 40 percent of observations, coefficient magnitudes are similar to those reported in [](#table6_FemRatio); standard errors are somewhat larger.
&lt;!--
\input{$HOME/Dropbox/Readability/draft/0-tex/generated/Table-5-wordlimit}
\clearpage
--&gt;</Text>
            <Comments>Results are similar if I also include a control for the number of words in the working paper version of the abstract (available on request).</Comments>
        </Document>
        <Document ID="4CF1715E-BCE1-4286-83D4-C9323DB533BE">
            <Title>Co-variate balance</Title>
            <Text>[](#balance0) compares co-variate balance pre- and post-match. The first column displays averages for the 121 female authors with at least three publications in the data. The first column of the first panel ("Pre-match means") displays corresponding averages for the 1,554 male authors with three or more publications. The first column of the second panel ("Post-match means") displays (weighted) averages for the 108 male authors matched with a female author. Gender differences are smaller post-match; $t$-statistics are likewise closer to zero.
&lt;!--
\input{$HOME/Dropbox/Readability/draft/0-tex/generated/Table-J.1}
\clearpage
--&gt;</Text>
        </Document>
        <Document ID="5F7D2065-CFA9-4A41-8503-E5485CDF451F">
            <Title>[][Experience], supplemental output</Title>
        </Document>
        <Document ID="A4941446-2CDC-467D-8237-CB36E2E1BE3B">
            <Title>Measurement error</Title>
            <Text>Readability scores fail to capture many elements relevant to reading comprehension, including gram\\-mar---*e.g.*, active vs. passive tense[#Coleman1964,Coleman1965]---legibility---*e.g.*, typeface or layout---and content---*e.g.*, coherence, organisation and general appeal[#Kintsch1984,Kemper1983,Meyer1982,Armbruster1984]. Nevertheless, "long sentences generally correspond to complex syntactic structures, infrequent words generally refer to complex concepts, and hard texts will generally lead to harder questions about their content"[\]\[p. 222][#Kintsch1984].
Still, readability scores' low causal power raises legitimate concerns about measurement error. As long as this error does not partially correlate with the variable of interest (gender), the analytical results I present in this paper attenuate toward zero (classical measurement error). Unfortunately, they are systematically biased in an unknown direction if it does (non-classical measurement error).
Sources of non-classical measurement error are threefold: (a) grammatical, spelling and transcription errors in the textual input; (b) errors in the estimates of vocabulary complexity and sentence length introduced by automating their calculation; or (c) embodied in the jump from using these two variables to infer readability.
Conditional on accurate calculation, readability scores combine very precise estimates of vocabulary complexity with almost perfect measures of sentence length[for a discussion, see\]\[][#Chall1995]. The weighted average of these two variables is informative in much the same way that inferences about readability are. Thus, measurement error related to (c) should only shift superficial interpretation of observed gender differences---from "women are better writers" to "women use simpler words and write shorter sentences"---but leave conclusions deduced from them intact.
Nevertheless, I try to minimise measurement error from (c) by using abstracts as textual input. Abstracts are self-contained, universally summarise the research and are the first and most frequently read part of an article[#King2006]. Additionally, they follow a more standardised layout compared to other parts of a manuscript: they are generally surrounded by ample whitespace and most editorial management systems anyway reproduce them in pre-formatted cover pages. These factors suggest a relatively homogenous degree of review across journals and subject matter and limit the impact that physical layout, figures and surrounding text have on readability.
Moreover, prior research suggests authors write in a stylistically consistent manner across the abstract, introduction and discussion sections of a paper. According to an analysis of published education and psychology articles, within-manuscript correlations of Flesch Reading Ease scores range from 0.64 (abstracts vs. introductions) to 0.74 (abstracts vs. discussions)[#Hartley2003b]. [#Plaven-Sigray2017;] also found a strong positive correlation using full text articles from several scientific journals. [](#figure3) plots abstract readability against the readability of a passage from the introduction for 339 NBER Working Papers eventually published in a top-four journal. It suggests a similarly positive relationship holds in economics, as well.
&lt;!--\input{$HOME/Dropbox/Readability/draft/0-tex/fixed/Figure-D.2}--&gt;
In my opinion, non-classical measurement error from (a) and (b) poses a bigger concern to the identification mapped out in this paper. I have taken several steps to reduce it. First, abstract text is also ideal for calculating readability: 100--200 words containing few score-distorting features of academic writing---*e.g.*, citations, abbreviations and equations[#Dale1948]. Additionally, most abstracts have been previously converted to accurate machine-readable text by digital libraries and bibliographic databases, curbing errors in transcription.
Second, I carefully proofread the text in order to identify (and fix) remaining transcription errors, eliminate non-sentence-ending full stops, and replace typesetting code---typically used to render equations---with equivalent unicode characters. Readability scores were determined using the modified text.
Finally, some programs that calculate scores rely on unclear, inconsistent and possibly inaccurate algorithms to count words and syllables, identify sentence terminations and check whether a word is on Dale-Chall's easy word list[for a discussion, see\]\[][#Sirico2007]. To transparently handle these issues and eliminate ambiguity in how the scores were calculated, I wrote the Python module `Textatistic`. Its code and documentation are available on [GitHub](https://github.com/erinhengel/Textatistic); a brief description is provided in [][AppendixTextatistic].
For added robustness, I also re-calculate scores and replicate most results using the [[`R` `readability` package](https://github.com/trinker/readability)][AppendixAlternativeReadability]. Coefficients are very similar to---and (to my chagrin) standard errors universally smaller than---those presented in the body of the paper.</Text>
            <Comments>For comparison, I randomly assigned abstracts to introductions in 1,000 simulated samples. The average coefficient of correlation between abstract text readability and the readability of a passage of text from a randomly selected introduction was -0.0006 for the Gunning Fog score and 0.0007 for the Flesch-Kincaid score.
*E.g.*, words in transcribed text are often inappropriately hyphenated---typically because the word was divided at the end of the line in the original text. 
When no exact replacement existed, characters were chosen that mimicked as much as possible the equation's original intent while maintaining the same character and word counts. (Equations in abstracts generally only occur in *Econometrica* articles published before 1980.)</Comments>
        </Document>
        <Document ID="1DA036F0-69BD-4B4F-9848-2BFAF0681621">
            <Title>[][MechanismsDescriptive], supplemental output</Title>
        </Document>
        <Document ID="3EDF16EB-93A7-4221-8FCF-5D0874FAE5C2">
            <Title>Readability differences across journals</Title>
            <Text>[](#table3_journal) shows the coefficients on the journal dummies in column (2), [](#table3_FemRatio). They compare *AER*'s readability to the readability of *Econometrica*, *JPE* and *QJE*.
&lt;!--
\input{$HOME/Dropbox/Readability/draft/0-tex/generated/Table-F.1}
\clearpage
--&gt;</Text>
        </Document>
        <Document ID="DB8440E8-09C5-4474-9FA1-86524D10260C">
            <Title>[](#table4), journal and male effects</Title>
            <Text>[](#tableC3) shows the coefficients on the journal dummies in column (2), [](#table4). They compare *AER*'s readability to the readability of *Econometrica*, *JPE* and *QJE*.
&lt;!--\input{$HOME/Dropbox/Readability/draft/tex/generated/tableC3}
\clearpage--&gt;</Text>
            <Notes>&lt;!--\clearpage--&gt;</Notes>
        </Document>
        <Document ID="F3385050-0BC3-4C1D-BD5F-ADBA3AA4E758">
            <Title>English as a native language</Title>
            <Text>In this section, I conduct a primitive surname analysis to determine whether female economists are more (or less) likely to speak English natively. To proxy for native language, I construct a binary variable equal to one if an author's last name is shared with at least 100, 1,000, *etc.* people in the U.S., according to the 2000 Census. Given historical immigration to the U.S., I supplement the analysis with an analogous indicator based on popular Scottish surnames during 1975--2015 (shared by 10 or more people); data are from the National Records of Scotland.
[](#tableA1) displays correlations between the various surname popularity variables. Note the substantial overlap between Scottish and U.S. surnames shared by 1,000--100,000 people.
&lt;!--\input{$HOME/Dropbox/Readability/draft/tables/tex/tableA1}--&gt;
[](#tableA2) displays coefficients on $\text{male}_i$ from a probit regression with the surname indicators as dependant variables. In the first column, male authors are slightly more likely to have popular U.S. and Scottish last names, although figures are statistically significant for very popular American surnames, only. [](#tableA2)\'s second column includes fixed effects for the first year in which an author published in the data. Year effects are meant to control for changes in surname popularity over time---*e.g.*, due to immigration---that might be correlated with authors' gender. Indeed, their inclusion eliminates gender's impact; column two suggests men and women are equally likely to be native-English speakers.
&lt;!--\input{$HOME/Dropbox/Readability/draft/tables/tex/tableA2}--&gt;
&lt;!--\clearpage--&gt;</Text>
            <Comments>It is not clear how---or even if---native English speakers write more clearly than non-native speakers. In fact, [#Hayden2008;] found peer reviewed articles by the latter actually *more* readable, on average.
I use Scottish in lieu of U.K. data because only popular 1911 surnames are available from the latter. (British Census data are first publicly released 100 years after being collected.)</Comments>
        </Document>
        <Document ID="46171C41-FA25-46F1-A4C4-0EAC31F84F64">
            <Title>Author-level analysis</Title>
            <Text>In this appendix, I analyse readability at the author-level. To disaggregate the data, each article is duplicated $N_j$ times, where $N_j$ is article $j$'s number of co-authors; observation $j_k\in\{1,\ldots,N_j\}$ is assigned article $j$'s $k\text{th}$ author. I then estimate the dynamic panel model in [](#equation1):
&lt;!--\input{$HOME/Dropbox/Readability/draft/0-tex/equations/equationF1}--&gt;
 $R_{j_{it}}$ is the readability score for article $j$---author $i$'s $t$th top-four publication; $R_{it-1}$ is the corresponding value of author $i$'s $t-1$th top-four paper. Gender enters twice---the binary variable $\text{male}_i$ and $\text{female ratio}_j$---to account for author $i$'s sex and the sex of his co-authors, respectively (papers with fewer than 50 percent female authors are classified as male, see [][Gender]). $\vect X_j$ is a vector of observable controls. It includes: editor, journal, institution and English fluency dummies, controls for blind review and quality---citation count (asinh), $\text{max. }T$ (author prominence) and $\text{max. }t$ (author seniority)---and $N_j$ to control for author $i$'s proportional contribution to paper $j$. $\alpha_i$ are author-specific effects and $\vep_{it}$ is an idiosyncratic error. $\alpha_i$ are eliminated by first-differencing. For each time period, endogeneity in the lagged dependant variable is instrumented with up to five earlier lags[#Arellano1995,Blundell1998]. To account for duplicate articles, the regression is weighted by $1/N_j$. Standard errors are clustered at the level of the author.
[](#table4_FemRatio) displays results. Rows one and two present contemporaneous marginal effects on co-authoring with women for female ($\beta_1$) and male ($\beta_1+\beta_2$) authors, respectively. Both estimates are positive---everyone writes more clearly when collaborating with women---although statistically significant only for female authors. Marginal effects for women are up to twice as large as those shown in [](#table3_FemRatio); they suggest women write 2--6 percent better than men.
Coefficients on the lagged dependant variables are small, suggesting readability is mostly determined contemporaneously. Nevertheless, their uniform positivity and occasional significance indicate some persistence. [](#table4_FemRatio)'s second panel reports test statistics of model fit. Tests for serial correlation indicate no model misspecification. $p$-values on the overall Hansen test statistic hover between 0.79--0.96, thus failing to reject that the instruments are valid; however, their very high values suggests the model may suffer from instrument proliferations which weakens the test[for a discussion, see\]\[][#Roodman2009a]. The $p$-value on the Sargan test also fails to reject that the instruments are valid at traditional significance thresholds; but although it is not vulnerable to instrument proliferation, it does require homoskedastic errors. Additional tests (available on request) suggest results are not sensitive to including the full set of (non-collapsed) instruments or to reductions in the number of instruments. Given the possibility of instrument proliferation, however, results in [](#table4_FemRatio) should be interpreted with caution.
&lt;!--
\input{$HOME/Dropbox/Readability/draft/0-tex/generated/Table-F.2-FemRatio}
\clearpage
--&gt;</Text>
            <Comments>To reduce the number of instruments (and thanks to a high degree of correlation with editor fixed effects) year fixed effects are omitted. Results are similar when they are included [see, *e.g.*,\]\[p. 17][#Hengel2017].</Comments>
        </Document>
        <Document ID="BC5A83FE-B0B1-43B8-B751-8B8DD5CA66A5">
            <Title>[](#table10X), including Condition 1</Title>
        </Document>
        <Document ID="462F4C77-ADD6-4646-97AC-3C2648409855">
            <Title>Ideas</Title>
        </Document>
        <Document ID="ECC1FB8B-17EA-40D7-B78A-7433F847347C">
            <Title>[][Duration], supplemental output</Title>
        </Document>
        <Document ID="119B1A81-EB7C-4F6C-AE03-CB1C45C9FA1C">
            <Title>Interpretation</Title>
            <Text>A number of *tentative* conclusions about the gender readability gap can be made from [](#figure9) and [](#table9). First, inexperienced men and women seem to make similar choices in draft readability. This suggests similar initial preferences for and beliefs about the impact of writing well. In one important sense, however, men are still better informed: the standards they believe apply actually do; junior women appear to mistakenly assume similar standards apply to them, too.
Second, experienced men *and* women seem to sacrifice time upfront in order to improve their odds in peer review. By anticipating referees' demands, authors can partially insure themselves against rejection and/or excessively long review. The price is having to spend more time revising a manuscript before submitting it. Assuming choices by senior economists express optimal trade-offs with full information, [](#figure9) implies little---if any---gender differences in these preferences for insurance. Again, however, higher standards will mean that the price of that insurance is greater for women than it is for men.
Finally, [](#figure9) suggests the direct effect of peer review dominates when women have less experience; the feedback effect dominates when they have more experience. This pattern of behaviour implies that women initially underestimate referees' thresholds but learn about them over time and adapt by writing their future papers more readably prior to submission. This last observation suggests inexperienced female economists go through the toughest review, conditional on acceptance. To investigate further, I test the impact of experience on time spent in review by re-estimating [](#equation16) on sub-samples of junior and senior authors. Results are displayed in [][AppendixTimeExp]. They suggest papers by junior women do indeed take longer in review; the gender gap is significantly smaller---albeit still positive---for senior women.</Text>
        </Document>
        <Document ID="B236ED0E-EF88-4EC3-91AC-E4554A34EC5C">
            <Title>[](#table11XX), Hausman test statistics</Title>
            <Text>[](#table11AXX) displays $\chi^2$ statistics from a Hausman test between fixed and random effects. (Random effects model shown in [](#table11XX).) In all specifications, the null-hypothesis (no systemic differences between models) is not rejected.
&lt;!--\input{$PPATH/tables/tex/table11AXX}--&gt;</Text>
        </Document>
        <Document ID="D93AD7C3-BF13-47BC-AD45-73499FCC8FC5">
            <Title>Untitled</Title>
        </Document>
        <Document ID="B6599A7A-F4BD-4E4B-8210-5FB9A4C2C83B">
            <Title>Quantile regression</Title>
            <Text>In [](#table10_quantile), I re-estimate gender differences in time spent under review using a quantile regression model. The first panel replicates [](#table10_FemRatio), column (5) at the 25th, median and 75th percentiles of review times; the second panel similarly replicates the third column of [](#table11_FemRatio).
The coefficient on female ratio is positive and significant across all three percentiles. Its magnitude is greatest in the right-tail of *Econometrica*'s distribution but is similarly sized across all percentiles when estimated using observations from both *Econometrica* and *REStud*.
&lt;!--
\input{$HOME/Dropbox/Readability/draft/0-tex/generated/Table-H.4}
\clearpage
--&gt;</Text>
        </Document>
        <Document ID="01E508C0-92C1-4247-B68E-F8C7E7201694">
            <Title>[](#table11), alternative measures of an article's "gender"</Title>
            <Text>[](#table11XA), [](#table11XB) and [](#table11XC) repeat the analysis shown in [](#table11) using three alternative measures of an article's "gender". In [](#table11XA), papers authored entirely by women are compared to papers authored entirely by men. In [](#table11XB), papers are considered "female" if at least one author is female. In [](#table11XC), article gender is represented by a binary variable equal to one if at least half of all authors are female; articles below that threshold with at least one female author are excluded.
&lt;!--



\clearpage--&gt;
</Text>
        </Document>
        <Document ID="F0FEDE9C-0C3E-4643-B58B-D4C0EB21152B">
            <Title>Use in research</Title>
            <Text>Thanks to high predictive validity and ease of use, readability formulas are widely employed in education, business and government. The U.S. Securities and Exchange Commission encourages clearer financial disclosure forms benchmarked against the Gunning Fog, Flesch-Kincaid and Flesch Reading Ease scores[#Cox2007]. The formulas have also guided readability assessments of, *inter alia*, standardised test questions[#Chall1977,Chall1983], medical inserts[*e.g.*,\]\[][#Wallace2008], technical manuals[*e.g.*,\]\[][#Hussin2012,Klare1973], health pamphlets[*e.g.*,\]\[][#Foster2002,Meade1989] and data security policies[#Alkhurayyif2017].
In research, readability scores are popular proxies for "complexity". [#Enke2018;] controls for language sophistication using the Flesch Reading Ease formula in a study of moral values in U.S. presidential elections. [#Spirling2016;] employs the same score to show that British parliamentarians simplified speeches to appeal to less educated voters in the in the wake of the Great Reform Act. Legal research has found that judges are more reliant on legislative history when interpreting complex legal statutes, as measured by the Flesch-Kincaid formula[#Law2010]. And in finance, various scores have linked the clarity of financial communication materials to better firm and market financial health[#Li2008,Biddle2009,Jansen2011], larger investment and trading volume [#Miller2010,Thörnqvist2015,DeFranco2015,Lawrence2013] and lower demand for---albeit higher reliability of---outside research by sell-side analysts[#Lehavy2011].</Text>
            <Comments>[#Bischof2018;] similarly show a positive link between plain language in political speeches and a desire to appeal to voters using German political manifestos and the Björnsson’s readability index. Their study also finds evidence that voters better understand more readable political messages.came to a similar conclusion using German political manifestos and the Björnsson’s readability index. Their study also finds evidence that voters better understand more readable political messages.
[#Long2011;] investigate whether a legal brief's readability score correlates with its success on appeal, but find that they do not.
See [#Loughran2016;] for a thorough review of the use of readability measures in finance and accounting research.</Comments>
        </Document>
        <Document ID="2849EBC4-661D-43EE-8C6A-9DA357D9AD13">
            <Title>Condition 1</Title>
            <Text>A readability gap could emerge from underlying gender differences in $u_i$: ambitious authors or those strongly averse to being rejected probably work harder to secure publications. Nevertheless, they eventually learn readability's true impact on rejection, so $i$'s unconditional probability of acceptance must be higher than $k$'s for $u_k&lt;u_i$ to produce $R_{kt}&lt;R_{it}$ indefinitely.
I cannot use the data analysed in this paper to test for Condition 1. Luckily, gender's impact on acceptance rates has been extensively studied elsewhere. To the best of my knowledge, publication outcomes expose no female advantage anywhere, ever. For example, [][#Blank1991;] found that acceptance rates for male- and female-authored papers were 12.7 and 10.6 percent, respectively, at the *American Economic Review*. At *JAMA*, 44.8 percent of referees accept male-authored papers as is or if suitably revised; 29.6 summarily reject them. Corresponding figures for female-authored papers are [38.3 and 33.3][#Gilbert1994]. [][#Ceci2014;] comprehensively review other research on the subject. Their conclusion: "When it comes to actual manuscripts submitted to actual journals, the evidence for gender fairness is unequivocal: There are no sex differences in acceptance rates" [\]\[p. 111][#Ceci2014].</Text>
            <Comments>[][#Blank1991;] found that the double-blind acceptance rate for female-authored papers was 10 percent (versus 11 percent for men); the single-blind acceptance rate was 11.2 percent (versus 15 percent for men).
[][#Gilbert1994;] broke down reviewer recommendations by content reviewer and statistical reviewer. The figures presented here aggregate their responses. Note also that with respect to the actual decision, manuscripts with male corresponding authors were more likely to be summarily rejected (41.7 percent as opposed to 37.4 percent); however, there was no gender difference in overall acceptance rates.</Comments>
        </Document>
        <Document ID="1E7FDF0C-FEAE-488C-804A-8632B4A3F129">
            <Title>Results</Title>
            <Text>&lt;!--\input{$HOME/Dropbox/Readability/draft/0-tex/generated/Table-9-base}--&gt;
[](#table8_base)'s first and second panels display means and standard deviations of [$D_{ik}$](#eq:correq1) for matched pairs where one member satisfies both Conditions 1 and 2 relative to the other member. In the first panel, the female member does; in the second, it's the male member. Male scores are subtracted from female scores, so $D_{ik}$ is, by definition, positive in panel one and negative in panel two.
Results in [](#table8_base) suggest that Conditions 1 and 2 were satisfied for the same author in 65 percent of matched pairs. In two-thirds of those, the member who satisfied them was female. Moreover, $D_{ik}$'s magnitude is (on average) 1.5 times larger in matched pairs where the female member satisfied Conditions 1 and 2 compared to pairs in which the male member did. [](#figure8_base) emphasises this point. It displays the distribution of $D_{ik}$: when the evidence suggests a man is subject to higher standards, $D_{ik}$ clusters close to zero; when it suggests the woman is, $D_{ik}$ is far more spread out.
[](#table8_base)'s final panel averages $D_{ik}$ over all observations, setting $D_{ik}=0$ in matched pairs where neither member satisfied Conditions 1 and 2. Mean $D_{ik}$ is consistently positive and significant indicating that higher standards are, on average, directed at women: women write about 3 points more readably on the Flesch Reading Ease scale; according to the four grade-level scores, their writing takes around 4--9 fewer months of schooling to understand than it would if they weren't subject to higher standards. Percentage-wise, these results suggest that higher standards mean women write, on average, 5 percent more readably than they otherwise would.
I emphasis, however, that these conclusions are predicated on several strong assumptions; if any are violated, then higher standards against women cannot be inferred from [](#table8_base) and [](#figure8_base). First, and most critically, $i$ and $k$ must be identical with respect to topic, novelty and overall quality. Second, at time $t=3$ authors must have learned enough about the process of peer review for Assumptions 4 and 5 in [](#Corollary1) to hold. Finally, [](#equation14) must accurately predict the readability of $i$'s and $k$'s papers as if they had been co-authored by members of their same sex. Please see [][AppendixMatchingLimitations] where I discuss the validity and robustness of these assumptions in more detail.
&lt;!--\input{$HOME/Dropbox/Readability/draft/0-tex/fixed/Figure-5-base}--&gt;</Text>
        </Document>
        <Document ID="B8F4B6D9-7AB3-427A-8FF5-9DB01D5CD64B">
            <Title>Understanding how women respond to higher standards</Title>
            <Text>Women can respond to higher standards in two different ways: immediately (direct effect) and pre-emptively (feedback effect). As emphasised in [][SEUModel], the weight of each effect likely depends on authors' information about---hence experience with---the peer review process. In this section, I illustrate the evolution of the relative importance of each by comparing papers pre- and post-review as authors' publication counts rise. To do so, I estimate the following equation:
&lt;!--\input{$HOME/Dropbox/Readability/draft/0-tex/equations/equation14}--&gt;
 where $m=W,P$ for working papers and published articles, respectively, "female ratio" defines papers with a strict minority of female authors as male-authored; for papers with 50 percent or more female authors, it is the ratio of female authors on a paper (see [][Gender] for more details), $t_{it}$ is author $i$'s number of top-four papers at time $t$, $\vect X_{it}$ is a vector of observable controls and $\varepsilon_{it}$ is the error term. 
Results from estimating [](#equation15) are shown in [](#figure9) and [](#table9). In [](#figure9), hollow circles denote draft readability; solid diamonds reflect readability in the final, published versions of those same papers. Dashed lines trace readability as papers undergo peer review (direct effect) and correspond to estimates in the first panel of [](#table9). [](#table9)'s second panel shows the marginal effect of female ratio (papers with fewer than 50 percent female authors are classified as male) for each version of a manuscript over increasing $t$. Estimates for published articles correspond to the difference between pink and blue diamonds in [](#figure9); estimates for draft papers represent feedback effects and correspond to differences between hollow circles. Difference-in-differences are shown in the final panel of [](#table9).
&lt;!--\input{$HOME/Dropbox/Readability/draft/0-tex/fixed/Figure-6}--&gt;
[](#figure9) and [](#table9) suggest that the gender readability gap in published articles is statistically significant and relatively stable at almost every $t$. At $t=1$, it is formed almost entirely during peer review; by $t\ge4$ is is largely formed prior to submission. That is, gender differences in the direct effect of peer review start off large, positive and significant but as $t$ increases, they gradually go away. For the feedback effect, however, the pattern is reversed. Differences in draft readability do not appear to contribute to the gender gap at $t=1$, but this gap rises as $t$ increases.
&lt;!--\input{$HOME/Dropbox/Readability/draft/0-tex/generated/Table-10}--&gt;</Text>
        </Document>
        <Document ID="18029C4D-D238-4F64-9727-102F69C6CA43">
            <Title>English fluency</Title>
            <Text>To account for English fluency, most regressions include a dummy variable equal to one if an article is co-authored by at least one native (or almost native) English speaker. I assume an author is "native" if he: (i) was raised in an English-speaking country; (ii) obtained all post-secondary eduction from English speaking institutions; or (iii) spoke with no discernible (to me) non-native accent. This information was almost always found---by me or a research assistant---in authors' CVs, websites, Wikipedia articles, faculty bios or obituaries. In the few instances where the criteria were ambiguously satisfied---or no information was available---I asked friends and colleagues of the author or inferred English fluency from the author's first name, country of residence or surname (in that order).</Text>
            <Comments>Non-native speakers who meet this criteria have been continuously exposed to spoken and written English since age 18. This continuous exposure likely means they write as well as native English speakers. To qualify as an English-speaking institution, all courses---not just the course studied by an author---must be primarily taught in English. *E.g.*, McGill University is classified as English-speaking; University of Bonn is not (although most of its graduate economics instruction is in English).</Comments>
        </Document>
    </Documents>
</SearchIndexes>