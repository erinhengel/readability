<?xml version="1.0" encoding="UTF-8"?>
<SearchIndexes Version="1.0">
    <Documents>
        <Document ID="DB8440E8-09C5-4474-9FA1-86524D10260C">
            <Title>[](#table4), journal and male effects</Title>
            <Text>[](#tableC2) shows male effects from the regressions described and presented in [](#table4). Effects estimated at a female ratio of zero and observed values for other co-variates. Grade-level effects (Flesch-Kincaid, Gunning Fog, SMOG and Dale-Chall) have been multiplied by [negative one][MeasuringReadability]. [](#tableC3) shows the coefficients on the journal dummies in column (2), [](#table4). They compare *AER*'s readability to the readability of *Econometrica*, *JPE* and *QJE*.
&lt;!--\input{$HOME/Dropbox/Readability/draft/tex/generated/tableC2}--&gt;
&lt;!--\input{$HOME/Dropbox/Readability/draft/tex/generated/tableC3}
\clearpage--&gt;</Text>
            <Notes>&lt;!--\clearpage--&gt;</Notes>
        </Document>
        <Document ID="4CF1715E-BCE1-4286-83D4-C9323DB533BE">
            <Title>[][SEUMatching], co-variate balance</Title>
            <Text>[](#tableC8) compares co-variate balance pre- and post-match. The first column displays averages for the 121 female authors with at least three publications in the data. The first column of the first panel ("Pre-match means") displays corresponding averages for the 1,553 male authors with three or more publications. The first column of the second panel ("Post-match means") displays (weighted) averages for the 104 male authors matched with a female author. [](#tableC9), [](#tableC10) and [](#tableC11) compare co-variate balance when restricted to matched pairs with $\underline D_{ik}\ne0$.
Gender differences are smaller post-match; $t$-statistics are likewise closer to zero. Moreover, co-variates remain well balanced between $\underline D_{ik}&gt;0$ (discrimination against women) and $\underline D_{ik}&lt;0$ (discrimination against men) samples; both resemble averages in the matched sample.
&lt;!--\clearpage--&gt;
&lt;!--\input{$HOME/Dropbox/Readability/draft/tex/generated/tableC8}--&gt;
&lt;!--\input{$HOME/Dropbox/Readability/draft/tex/generated/tableC9}--&gt;
&lt;!--\input{$HOME/Dropbox/Readability/draft/tex/generated/tableC10}--&gt;
&lt;!--\input{$HOME/Dropbox/Readability/draft/tex/generated/tableC11}--&gt;
&lt;!--\clearpage--&gt;</Text>
        </Document>
        <Document ID="2D4500FC-774E-4302-851C-5B8EDFA71D17">
            <Title>New Folder</Title>
        </Document>
        <Document ID="D7529450-4313-4DD1-8792-5AD7C387DB51">
            <Title>History, statistical validity and use in research</Title>
        </Document>
        <Document ID="D7631746-6548-4E03-9F4E-22B795937D52">
            <Title>Alternative hypotheses?</Title>
        </Document>
        <Document ID="D2F71FD6-EE1C-49FF-938C-E65C11B8E95D">
            <Title>Extensive form</Title>
            <Text>[](#p1figureC3) is an extensive form representation of the bankruptcy game. This representation is not unique---the actions and [proofs][p1appendixproofs] are also consistent with a game in which the entrepreneur files for bankruptcy and/or the creditor proposes to liquidate or enter a workout. Complete details on assumptions and legal rationale behind actions in bankruptcy are outlined in [][p1model]; a brief description is provided in the notes.</Text>
            <Comments>\input{$PPATH/p1/figures/tex/p1figureC3}
It is still assumed the entrepreneur has all the bargaining power when negotiating a workout, thus if the creditor makes the workout offer, it would satisfy [](#lem2). For a full discussion on how the entrepreneur's bargaining power changes the game, see [][p1appendixbargainingpower].</Comments>
            <Notes>&lt;!--\end{appendices}--&gt;</Notes>
        </Document>
        <Document ID="3DEF56A7-A1F1-4651-8037-35C0D75764FB">
            <Title>Untitled</Title>
        </Document>
        <Document ID="70609C80-475F-481A-83D2-8834708978EB">
            <Title>Policy implications</Title>
            <Text>[](#prop1), [](#prop2) and [](#prop3) imply two fundamental problems caused by bankruptcy: (i) credit rationing and/or decisions that induce premature liquidation *ex ante*; and (ii) inefficient continuation *ex interim*. Problem 1 affects viable firms; Problem 2 affects non-viable firms.
Rescue culture supposedly limits liquidating viable firms. When procedures are expensive, however, the opposite occurs. Minimising Problem 1 means cutting their cost. When $Y=0$, all viable firms get credit. $K_0&lt;\ol V_0^B$; per [](#prop2)(i) and [](#p1equation4) a $D$ exists that satisfies [](#lem3). Since viable firms continue even when insolvent ([](#prop1)(i)), investment decisions are anyway efficient. Meanwhile, [](#prop2)(ii) applies to all non-viable firms; none are excluded from the lending market. With sufficiently differentiated product markets for productive assets, *ex interim* decisions are efficient as well.
The model assumes bankruptcy does not explicitly curtail creditors' right to $D$. Thus, regulations which neither reduce future earnings nor explicitly cut claims do not impact *ex ante* lending decisions. As discussed in the introduction, most rules from Chapter 11 meet these criteria---even super-senior interim financing. As shown in [][p1AppendixSuperSeniorFinancing], emergency finance prevents existing creditors from blocking new loans. Nevertheless, interim loans do not alter underlying firm value; contracts adjust upfront to account for their possibility. Lending conditions and investment decisions remain efficient.
Yet elevating the value of creditors' outside option simultaneously addresses Problems 1 and 2. Several common law countries---most notably the U.K. before 2003---include a form of bankruptcy that does just that: receivership. Receivership applies to a special class of creditor---one that holds a lien on more than just the firm's physical assets.
"Floating charge" liens apply to an entire business. In receivership, a creditor secured by one has full control of the distressed firm. He may operate it himself, sell it to a third party or liquidate its assets piecemeal. Setting $q=0$ simulates receivership: creditor and entrepreneur disagreement implies the creditor supports liquidation ([](#lem1)); without reorganisation, courts uphold his wish.
Contracts based on receivership replace a payment with an outcome precisely when that payment is particularly risky. Safer outside options are more valuable, forcing entrepreneurs to shoulder greater financial responsibility in insolvency. Since $\ol C_1^B=C_1^L$, the creditor always recovers the principal amount of his loan. Lending is riskless. The entrepreneur asks to borrow $K_0$ and repay exactly that one period later. The creditor accepts. Every project is funded. *Ex ante* and *ex interim* investment decisions are efficient ([](#prop4)).
PROPOSITION
In receivership, $D=\ul K_0$. There is no credit-rationing. Non-viable firms liquidate at time 1. Viable firms continue until time 2.
eprop
But receivership is not common and its ability to alleviate credit-rationing frequently undermined by regulation. Reformed U.K. procedures took effect in 2003. The new law limits floating charges to eight exceptional cases.
Rescue culture was the obvious rationale. Nevertheless, 60 percent of small-to-medium-sized firms in British receivership ultimately continued operating as going concerns---and most lenders that eventually liquidated genuinely tried a rescue first[#Davydenko2008,Franks2005a]. Chapter 11, on the other hand, rehabilitates only a third of them; the rest are eventually liquidated[#Baird2007a,Kahl2001].</Text>
            <Comments>As I show in a companion paper, judicial errors may also reduce credit-rationing of and self-induced premature liquidation by viable firms[#Hengel2015].
When the market for machines is insufficiently differentiated, some non-viable firms are always at risk of filing for bankruptcy when initial earnings are very low---and a fraction of those may be inefficiently continued due to judicial error. Nevertheless, sub-optimal interim decisions are infrequent. Firms opt for bankruptcy in only a narrow set of mathematical circumstances predicated mostly by dismal initial earnings. When low earnings are improbable, $D$ will be low squeezing the range of $X_1$ that make bankruptcy attractive. When they are likely, firms may get credit at some $D$ such that $\ol C_0=K_0$. In this case, however, bankruptcy is not much worse than liquidation, so either the difference between returns in liquidation and those in continuation are small or judicial likelihood of liquidation is high. If, on the other hand, no such $D$ is available, the firm only gets credit at an exorbitant rate ([](#prop2)(ii)), thus guaranteeing an efficient outcome at time 1.
Floating charges may also be more narrowly defined or co-exist with traditional secured loans.
They are: 1. capital markets; 2. public-private partnerships; 3. utilities; 4. urban regeneration projects; 5. project finance; 6. financial markets; 7. social landlords; and 8. special administration regimes (notably, transport and water). Each is further limited by several specific provisions in Enterprise Act 2002, *e.g.*, debt exceeding Â£50 million and contractual "step-in" rights (giving one party rights to "step in" the shoes of another in cases of serious breach of contract).
Findings in[#Davydenko2008;] and [#Franks2005a;] actually apply to U.K. administrative receivership procedures. The Insolvency Act 1986 altered earlier receivership---most notably by outlining statutory responsibilities of the appointed administrator---and renamed it.</Comments>
        </Document>
        <Document ID="7F311019-A771-4CA6-82C1-66C7C5C6EC4F">
            <Title>publishing_female</Title>
            <Text>LaTeX input: draft-header
My Short Title: &lt;$projecttitle&gt;
My Subtitle: &lt;$custom:Subtitle&gt;
Author: &lt;$author&gt;
My Date: &lt;$custom:Date&gt;
Address: &lt;$custom:Address&gt;
Telephone: &lt;$telephone&gt;
Affiliation: &lt;$custom:Affiliation&gt;
Department: &lt;$custom:Department&gt;
Position: &lt;$custom:Position&gt;
Email: &lt;$custom:E-mail&gt;
Thanks Bitches: &lt;$custom:Thanks&gt;
File ID: &lt;$custom:CustomID&gt;
Keywords: &lt;$custom:Keywords&gt;
Subject: &lt;$custom:Subject&gt;
JEL: &lt;$custom:JEL&gt;
My Abstract: &lt;$custom:Abstract&gt;
Word Count: &lt;$wc&gt;
Base Header Level: 3
LaTeX input: draft-begin-single
LaTeX footer: draft-footer</Text>
        </Document>
        <Document ID="0A2D3174-B366-4B3B-A1D4-777010690665">
            <Title>Indirect effect of higher standards-1</Title>
        </Document>
        <Document ID="F38B058E-3CC8-4106-B0F0-A623A794901F">
            <Title>[][SEUMatching], co-variate balance when \\(\underline D_{ik}\ne0\\)</Title>
        </Document>
        <Document ID="3F925459-8380-4EC0-991B-304A205B43D8">
            <Title>Scratch: Publishing Female</Title>
            <Text>A second issue is that readability scores are only as accurate as the underlying text fed into them. Measurement error obviously increases when they are applied to grammatically incorrect, improperly punctuated or poorly transcribed passages of text. 

Thus, non-classical measurement error from (c) affects the superficial interpretation of a difference: 
instead of identifying a gender difference in *readability* 

 even if non-classical measurement error prevents 
in the absence of errors by (a) and (b), 

A more concerning (and relevant) measurement error is that readability scores systematically mismeasure "readability" that the degree of misreporting or mismeasurement is correlated with the variable of interest (gender) conditional on observable controls. 
is correlated with the variable of interest (gender), then the results presented in this paper 
embodied within the scores themselves
the software used to calculate readability scores *or* the scores themselves intrensically embody measurement error in a way that is 
Either due to mismeasurement instroduced by the software used to calculate readability score *or* from from some inherent 
potentially suffer from a form of mis-measurement

try to discuss the means I have taken to try to reduce this error has 
Nevertheless, inferences made from non-causal predictors are necessarily biased by measurement error. For this reason, I collect a relatively large sample of 
Using the scores in empirical analysis therefore requires confronting the issue as transparently and comprehensively as possible[#Loughran2016]. I try achieve this in several ways.
While all of these components are indeed important for determining readability, the few measures developed to capture them are also highly correlated with readability scores. Nevertheless, as imperfect measures of "readability", to be interpreted as such requires confronting the issue of measurement error.
Indeed, as highlighted in [](#figure0X) are not perfectly correlated with 
Readability scores are a weighted average of vocabulary and sentence length. Inferring "readability" introduces measurement error---as shown in [](#figure0X), the scores are indeed highly correlated with alternative measures of readability, but they aren't perfectly correlated with those measures. 

Using readability scores in this manner has several important advantages. First, researcher subjectivity is avoided. Hundreds of readability scores exist, each employing slightly different methodology to analyse the readability of a piece of text. I simply choose the five most common measures. Moreover, although it is possible (probable?) that one of the underlying components of readability scores is truly driving nay observed gender differences, there is no prior research to pinpoint which one that would be. Using the overarching score, meanwhile, incorporates both measures. This will obviously come at a cost (in terms of noise) if one of the underlying components is truly driving gender differences in readability; but the benefit is reduced researcher subjectivity.

I do not indent to use readability as a proxy for the overall quality of a paper. Instead, I look at readability after controlling for other components---citations, author institution---that reflect (or are a consequence of) the quality of the underlying paper. That is, holding everything constant, what is the marginal difference in readability of male and female-authored papers? 
these readability scales only work as supplementary tools for a text where language proficiency is already confirmed through other means.
These scores are only approximations of reading difficulty and the educational/intellectual level your audience might need to understand your work. They are obviously not indicators of writing quality or how well your writing will be received by your target audience. I would say that a work with such low readability scores and such long sentences (26 for 1200 words?) is likely to come off as too obscure or elitist to the general reader, and would have a very limited audience. But, of course, such things are a matter of taste and convention. So ultimately, this is just my personal opinion.
Of course, readability formuals are not perfect predictors of readability. Evidence suggests they may not be accurate enough to adequately assess or guide development of individual documents, including legal briefs[#Sirico2007], financial disclosure documents[#Loughran2014] or school reading material[#Ardoin2005, Powell-Smith2001]. But any measure used in quantitative analysis to an extent suffers from this criticism. Unemployment status is a noisy predictor of an individual's future employment situtation, but aggregated over many individuals, it more accurately predicts how easy it is to get a job in a particular geographic unit. IQ scores are likely a noisy predictor of one person's intelligence---but are more accurate when aggregated over larger samples. The PISA score is a noisy measure of the educational atainment of a particular country. But---as with readability scores---each of these measures do correlate with the outcome of interest.
However, the use of certain quantitative measures is well established in research. Thus, the process of---and therefore trust in---data collection is more mature. Textual analysis, on the other hand, is still a developing field, with few discipline-specific norms for measuring inputs and selecting methods used to address a given empirical issue.
It has been claimed that traditional tests of reading difficulty "are dubious instruments for adequately assessing the readability of accounting narratives that are adjust oriented and specialist in nature."[p. 172\]\[][#Jones1994]. Yet this criticism was aimed at using readability scores to assess the readability of *specific* texts---*i.e.*, readability scores may not be precise enough to determine if a specific financial disclosure document is sufficiently clear or if a particular book is appropriate for certain grade-levels.
Yet while the margin for error in each individual case may be large, readability scores nevertheless *do* correlated with writing clarity, making them appropriate measures to estimate gender differences when sample sizes are sufficiently large. Just like saying the U.K. has a stronger economy than France because it has a higher estimated GDP per capita is less certain than making the more general claim that countries with higher GDP per capita have, on average, stronger economies than countries with lower GDP per capita. Similarly, the statement "Finland has a better education system because it earns a higher PISA score" is less certain than the statement

I do not indent to use readability as a proxy for the overall quality of a paper. Instead, I look at readability after controlling for other components---citations, author institution---that reflect (or are a consequence of) the quality of the underlying paper. That is, holding everything constant, what is the marginal difference in readability of male and female-authored papers? Yet, although the purpose of this paper is just to show that women are better writing, it would be nice to see that better writing in female-authored papers is associated with other positive quality outcomes. INSERT FIGURES SHOWING THIS.
; mechanically shortening sentences and using simpler words probably won't improve it. This is because the concept of readability is broader than these two components. 
 Interpreting differences in readability scores as actual differences in readability---coupled with the relatively large samples used in this paper---suggests that effects I present suffer from attenuation bias.

As discussed in [][Abstracts]
In X I discuss how remaining measurement bias may affect the results I obtain.
Readability scores omit many other aspects important for readability, such as g

---or anything indirectly correlated with gender---is not affected by 

Specifically, readability scores only measure the "surface features" of text and ignore other features such as content, organisation, coherence and human interest as well as features of the layout such as font, spacing, the prevalence of relevant figures and graphics and reader interest are all important components of reading comprehension ignored by the formulas.
Thus, adaptions and revisions of texts by mechanically lowering readability scores can result in less readable text. Also, reduced cohesion may be brought about by mechanically shortening sentences. Thus, classifying text by readability scores comes with all the same problems of classifying intelligence using IQ tests, or biases using implicit bias tests or really anything using supervised machine learning texts. Thus, applying readability scores in academic research comes with similar cautions.
Yet, despite the success of readability formulas, they remain controversial. Critics of the formulas typically concentrate on three main criticisms. First, one limitation of the formulas is that they do, indeed, contain only âsurface featuresâ of text and ignore other features like content and organisation. The research shows, however, that these surface featuresâthe readability variablesâwith all their limitations have remained the best predictors of text difficulty as measured by comprehension tests (Hunt 1965, Bormuth 1966, Maxwell 1978, Coupland 1978, Kintsch and Miller 1981, Chall 1984, Klare 1984, Davison 1984 and 1986, Carver 1990, Chall and Conard 1991, Chall and Dale 1995).

Second, some critics contend that the readability formulas were developed for children and they never were never formulated or tested with technical documents. This is actually incorrect. All five formulas were originally developed to assess the reading difficulty of adult reading material[#Dubay2004,Vieth1988;]. For example, Klare (1952) tested the Flesch Reading Ease and Dale-Chall formulas against the 16 standardised passages of reading comprehension tests developed for adult readers. Several extensive studies (Klare et al. 1955a, Klare et al. 1957, Klare and Smart 1973, Caylor et al. 1973, Kincaid et al. 1975, Hooke et al. 1979) used materials developed for technical training and regulations in the military to formulate and test several of todayâs most popular formulas such as the Flesch-Kincaid grade-level formula. A series of studies in the military (Klare et al. 1955a) examined how prior knowledge as well as the text variables affect the retention and the acceptability of technical documents. more readable versions resulted in: Greater and more complete retention. Greater amount read in a given time. Greater acceptability (preference).
Additionally, as shown in [][Abstracts], NBER abstracts---both those by men and women---change significantly in readability between draft and final version. As I show in [][NBER], drafts are typically released as NBER working papers *after* they were already submitted to peer review (and this is especially true for female-authored papers). Since post-submission, authors probably do not solicit non-referee feedback nor make non-referee requested changes. Thus, these changes in both male and female-authored papers probably do reflect the peer review process.

The Flesch Reading Ease scales from 0 (hard) to 100 (easy). In contrast, the other four scores generate grade levels estimating the minimum years of schooling necessary to confidently understand an evaluated text---and so lower scores indicate easier-to-read text. To minimise confusion, I multiply the four grade-level scores by negative one. Thus, higher numbers universally correspond to clearer writing throughout the paper.
Moreover, some authors have proposed methods they consider to be better estimates of readability. [#Benoit2017;] proposes X; [#Loughran2014;] proposes X, *etc.* While all of these studies criticise one or more traditional measures of readability---and propose new, alternative measures of the readability of complex documents---they also, universally, confirm that traditional metrics of readability do indeed correlate with the actual readability of a particular piece of text.

a number of positive---and a few negative---firm characteristics and financial market outcomes[for a thorough review of this research, see\]\[][#Loughran2016].
The role of readability is also considered important as a central or adjunct variable in assessing financial documents. The empirical results of several studies studies repeatedly document a statistical association between a traditional readability measure and other attributes of the firm. The first paper to examine the link between annual report readability and firm performance for a meaningful sample is [#Li2008;]. He measures the readability of annual reports using the Gunning Fog Index and the number of words contained in the annual report. He finds that firms with lower reported earnings tend to have annual reports that are harder to read (*i.e.*, high Fog Index values or high word counts). He also finds that companies with more readable annual reports have higher earnings persistence. Other researchers have also used the Gunning Fog Index as a measure of annual report readability. [#Biddle2009;] find that firms with high reporting quality (using the Gunning Fog Index and two other variables) are associated with greater capital investment efficiency. [#Guay2015;] find that companies with less readable annual reports (based on six different readability measures, including the Gunning Fog Index) tend to mitigate this negative readability effect by issuing more managerial forecasts of earnings per share, sales and cash flows. [#Miller2010;] finds that small investors trade significantly fewer shares of firms with high Fog Index values and word counts (*i.e.*, less readable annual reports) around the 10-K filing date. Less readable annual reports should be harder to process especially for less-sophisticated investors. Other studies have found that readable business documents are linked to lower analyst dispersion and greater earnings forecast accuracy[#Lehavy2011], higher trading volumn[#DeFranco2015] and stronger analyst reactions to both good and bad news[#Rennekamp2012;]. Other studies have shown that investors are more likely to invest in firms with shorder, more readable 10-Ks[#Lawrence2013], and firms with more readable documents have more pronouced small investor trading activity around the filing date [#Miller2010].
used them to show that ministerial speeches became easier to understand post democratisation likely because of an effort to appeal to new voters, who are poorer and less educated.
Political scientists have used them to positively link clarity of communications with better informed voters
The former tend to use readability scores to link linguistic sophistication to political outcomes[see, *e.g.*,\]\[][#Spriggs1996,Sirling2016,Owens2011,Bischof2018]. The latter use them to link clarity of communication with financial outcomes[for a comprehensive review of this research, see\[\[][#Loughran2016]. Generally, this research has found concrete benefits to clarity.
 They have gotten the widest pickup as a proxy for "complexity" in political discourse and corporate disclosures. Generally, this research has used these measures to proxy for textual complexity with an eye to link linguistic sophistication to outcomes---and a focus on the concrete benefits to clarity. [#Janson2011;], for instance, studies the reading level of communications by four central banks, equating lower reading levels of bank communication with greater clarity, which they link to positive effects on the volatility of returns of financial markets. Likewise, [#Owens2011;] and [#Spriggs1996;] examine the complexity of Supreme Court decisions, pointing to the importance of clarity in court opinions. In the context of the British parliament, [#Spirling2016;] applies readability measures to document the democratising effects of franchise reform on elite speeches. Studying post-war Austrian and German elections, [#Bischof2018;] find that simpler manifestos make for better informed voters.


This decision was made on the assumption that a gender readability gap---if it exists---is function of (i) the probability a particular portion of analysed text was written and/or revised by a female co-author; and (ii) referees' beliefs about female authors' contributions to the writing of a paper. I assume the intersection of (i) and (ii) is positively related to the ratio of female authors on a paper. This assumption is supported by prior research suggesting that co-authors---regardless of seniority---tend to share responsibility for writing and revising collaborative work[#Hart2000,Kumar2016]. [CONFIRM REFERENCE]


more readable text longer in short term memory
studied the reading efficiency and retention of 120 male aviators in a mechanics course at Chanute Air Force Base in Illinois. They used two versions of technical training materisl, hard (13th--15th grade) and easy (7--8th grade). They measured reading efficiency with an eye-movement camera with which they could determine the number of words read per second and the number of words read per fixation. The study showed that the easy text significantly improved both readign efficiency and retention. [Rothkopf1977;] showed the connection between readability and how many words a typist continues to type after the copy page is covered.
A series of studies in the military were conducted at the Sampson Air Force Base in New York and Chanute Air Force Base in illinois using 989 male Air Force enlistees in training with different versions of the same texts. They used the Flesch Reading Ease and Dale-Chall formulas to rate the texts as Easy (grade 7), present (12th grade) and hard (16th grade)[#Klare1955]. The study found that more readable versions resulted in greater and more complete retention, greater amount read in a given time and greater acceptability.

with results from multiple choice reading comprehension tests, cloze tests from 46 published studies 
with results from reading comprehension tests, from multiple choice reading comprehension tests
Early statistical validity of readability scores was established by applying the formulas to passages of text that had previously been ranked according to difficulty based on the average reader's ability to correctly answer several comprehension questions after reading it. All scores correlate highly with these tests: 0.64--0.70. Later studies confirmed their validity using the cloze procedure[for a detailed and extensive overview of this research, see\]\[][#Klare1975]. Other studies have also validated readability scores against the judgement of both expert and non-expert human judgement, oral fluency tests and other readability scores. In general, readability scores are highly correlated with other readability rescores; they also exhibit a strong association with rankings provided by human judgement. Their correlations with measures of oral reading fluency---e.g., the number of words read correctly aloud per minute---are positive, but more moderate[#Begeny2014,Ardoin2005].

 [#Bormuth1966;] found cloze scores correlated highly with all the individual comopnents of readability scores used in this analysis (syllables per word, sentence length, *etc.*). This study also provided evidence that the correlation between formula variables and comprehension do not change as a function of reading ability. These studies continued to show strong statistical correlation between readability scores and writing clarity

The constants in each formula vary widely as do the components used to rank vocabulary. The Flesch Reading Ease and Flesch-Kincaid scores rely on syllable count. Gunning Fog and SMOG total polysyllabic words (words with three or more syllables). Dale-Chall tallies words not on a pre-defined list of 3,000 so-called "easy" words. Because of these differences, the four grade-level scores rarely generate identical figures; nevertheless, all five scores produce roughly equivalent rankings(for a review of literature supporting this conclusion, see, [][AppendixMetaAnalysis]).

---*i.e.*, readers' ease in understanding a passage of text. Hundreds of formulas try to measure readability by exploiting this relationship. I concentrate on the five most widely used, tested and reliable measures for adult reading material: Flesch Reading Ease, Flesch-Kincaid, Gunning Fog, SMOG (Simple Measure of Gobbledegook) and Dale-Chall[#DuBay2004]. Each are listed in [](#table2).

In the wake of serious public concern about adult literacy post-World-War-I, Rudolf Flesch developed one of the first readability scores tailored exclusively for adult reading material. Alternative readability scores for adults soon followed: In 1948, Edgar Dale and Jeanne Chall developed the Dale-Chall formula for advanced readers. In 1952, Robert Gunning published his own formula, the Gunning Fog Index, to assist newspapers and popular magazines to improve their writing. G. Harry McLaughlin published his SMOG formula in 1969. In 1976, a study commissioned by the U.S. Navy modified the Flesch Reading Ease formula to produce a modified score known as the Flesch-Kincaid formula.
In 1953, Wilson Taylor of the University of Illinois published the cloze test. The cloze test is generally considered a better measure of difficulty because it evaluates not the diffiuclty of a wor
In Flesch's original readability study, he found the Flesch Reading Ease correlated with results from the McCall-Crabbs tests was about 70 percent. READABILITY FOR THE OTHER SCORE
CLOZE TEST

Flesch describes the test lessons as a "series of 376 passages for children which had been previously graded on the basis of ten comprehension test questions appended to each passage" (1943b, 6). The tests were supposed to teach children how to comprehend many different kinds of reading materials, how to enjoy reading, and how to be motivated and to improve expressing themselves orally. They were called "standard" test lessons because every test would show "how well the normal or typical pupil would read these same lessons" (McCall and Crabbs 1925,
184).
The estimated coefficient of correlation of his measure, the Flesch Reading Ease score, with a benchmark measure of text comprehension---the McCall-Crabbs *Standard Test Lessons*---was 70 percent.



More recent research using other benchmark measures of text comprehension have backed this earlier evidence up. One of the oldest and most commonly used formulas, the Dale-Chall has consistently proven to be a reliable formula for gauging general text difficulty for higher ability readers[#Begeny2014,Hintz2004]. [#Ardoin2005;] find a modest relationship between reading fluency and the four grade-level readability scores, and especially strong evidence in favour of the Gunning Fog readability measure.

Other studies have shown that the Flesch Reading Ease accurately---albeit noisily---gauges the readability of sophisticated political discourse[#Beniot2017;], although the correlations they reported are more muted than those found in earlier studies.

The most popular reading comprehension test was the McCall-Crabbs *Standard Test Lessons*. The Reading Ease formula correlated .70 with the McCall-Crabbs criterion
Most formulas have also been tested against other reading comprehension tests geared more toward adults.

. These lessons have been convenient statistically because there are a large number of reading passages, covering a
wide range of difficulty, resting upon extensive testing, and providing detailed grade scores.
were based on collections of text passages that had been ranked according to difficulty based on the average reader's ability to correctly answer several questions meant to test his comprehension. The underlying reading comprehension tests---including the McCall-Crabbs *Standard Test Lessons*, the Ojemann tests and the Gray and Leary tests---
standardised reading comprehension tests that had previously been administered to a . That is, readability formulas were applied to a large number of text passages which had previously been ranked according to difficulty based on the average number of correct responses 

That, a large number of readers were given a passage of text and then asked various questions about that text to determine how well it was understood. Passages of text in which a high percentage of test takers got most of the questions correct are assumed to be "easier"; passages with fewer right answers are assumed to be harder. Readability formulas were then applied to that text and 
The earliest studies showing correlation between readability scores and text comprehension were based on 
McCall-Crabbs *Standard Test Lessons*. These tests involved a series of 376 passages for children, each of which included ten comprehension test questions to evaluate how well the test-taker had understood the piece of text. Grade-level reading ability was calibrated by how many questions were answered correctly---*i.e.*, more correct answers indicated higher reading ability. Klare tested the Flesch Reading Ease and Dale-Chall formulas against 16 standardised passages of the Ojemann tests and the 48 passages of the Gray and Leary tests, all developed for adult readers.




After testing several components against benchmark measures of text comprehension, he eventually settled on a weighted average of sentence length and syllables per word.



figure out the components of writing that contributed to readability. Flesch picked twenty-one magazines and divided them into five different levels of reading difficulty: Level A was the easiest and contained *Romantic Story*; level B (*e.g.*, *Redbook*); Level C (*e.g.*, *Reader's Digest*); Level E (*e.g.*, *Fortune*); and Level E (*e.g.*, *The American Scholar* and *The Yale Review*. He then counted the number of times several components of writing which had previously been found to correlate with reading comprehension in children were found in the different texts. He found that sentence length, the number of abstract words and the number of affixes had a high predictive value.
Flesch connected several follow-up analyses using the McCall-Crabbs standardised test questions on reading materials
In response to those concerns, public librarians, sought to recommend reading material of the appropriate grade level to adults. In an effort to help the large numbers of adults who were unable to read adn to provide the needed research on the area of writing readable books, the American Association for Adult Education formed the Readability Laboratory in 1936.
In order to determine a way to produce readable material, Rudolf Flesch and the Laboratory's other researchers evaluated the exist
The earliest measures of readability were simple counts of difficult words.
Unfortunately, formulas using word lists had some definite problems when applied to adults. Most word lists had been developed for children. Adults, on the other hand, read more, had more experiences and just knew too many words. Moreover, even if a perfect and accurate word list exists, it still would not tell us which words were difficult to understand because of how they were used or because they were abstract to begin with.
In response, [#Gray1935;] statistically tested numerous additional components that could be added to word lists to improve readability measures. 
were developed for this purpose. Unfortunately, most of these measures 

Though this is not quite the same as a journalâs peer-review process, itâs not quite a personal Word document uploaded to the internet, either. Because of that, economics working papers are regularly discussed by journalists, academics, and even policymakers before theyâre formally published in a journal.
Although this might be an issue if a non-standardised repository of draft papers were used, this is not likely to have a huge confounding effect on papers released as NBER drafts. NBER Working Papers are high profile releases of draft papers before being published in order to encourage discussion and suggestions for revision before publications. However, the long list of acknowledgements on NBER working papers---and, in particular, that overlap with the acknowlegements in the actual paper---suggests that the papers have undergone a thorough form of internal peer review---by friends and colleagues of the authors---before it is released as an NBER working paper.


I worry that we cannot conclusively attribute changes in readability that occur between the NBER working paper and the published paper to the referee process. Some plausible alternative explanations are that male and female authors differ in the following ways: (i) how likely it is that they send the working paper out for comments, (ii) who they send the paper to for comments (might women send the paper to women more often, who perhaps are more likely to
focus on writing quality if they are ex-ante better writers?), and (iii) how they respond to those comments that they do receive.

long before they were released as NBER Working Papers and, combined with the evidence presented in [](#figure7), long before they were submitted to a journal. In additiona, most papers thank several economic congerences and their participants
The other thing critics brought up is that Doleac and Mukherjeeâs article is not yet peer reviewedâthey plan to submit it to journals soon. But this, too, is an important difference between economics and some other types of health research. Economists tend to put out working papers and circulate them among colleagues long before they submit to journals. For example, Doleac and Mukherjeeâs paper thanks several economic conferences and their participants in its acknowledgments. These colleaguesâsometimes in pressure-cooker-esque seminarsâask questions and make suggestions, after which the paper is revised, then submitted.

Thus, if the data from *Econometrica* is indicative of all papers, and we assume that people no longer solicit external feedback---*e.g.*, by sending the drafter version out for comments---after submitting to the journal in which it is eventually published, then [](#figure7) is strong evidence that the changes authors---and especially female authors---make to their papers is in response to referee feedback.

One could also include in that designation papers with a female first author. I therefore designate the lead author has he who was listed first in a paper in which authors were listed non-alphabetically or he was listed as the "corresponding author" or "lead author" in the acknowledgements. Unfortunately, however, this added only another 35 (0.38 percent) "female" observations.

Moreover, conducting the analysis in [][Matching] would mean restricting the sample to only those women with at least three 100% female-authored papers; there are only 22, and only an additional 26 women have two. Indeed, 26% of female authors with at least three top publications have always authored with at least one man.

with the introduction of the internet it seems unreasonable to assume that its prior success can be replicated currently. Indeed, if we drop the variables subjected to double-blind review pre-internet and include instead a dummy variable for double-blind review *post* internet, we find similar (positive) effects on gender in both the single- and double-blind samples. This evidence is consistent with the evidence presented in [](#table8a)---*i.e.*, there is less (or at least no more) evidence of gender bias under double-blind review than under single-blind review.

Single-blind review combined with wide dissemination---particularly on the internet---of NBER working papers make it likely that . 
 NBER working papers high profile (and wide dissemination)
at most journals Double-blind review---particularly before the internet---may have curbed that. To deal with the issue, [](#table7) excludes the 279 affected articles. Here, I consider discrimination in the review process through the lens of the shift from double-blind to single-blind review (or the introduction of the internet). 

, I found that the readability gap increased during the period that
papers underwent double-blind review in an analysis that only looked at the readability of published papers (and did not consider the readability of draft versions of those same periods as is done in this analysis). As shown in ??, however, those results are not robust to including time trends.


As can be seen from [](#figure0), *QJE* and, especially, *AER* implemented double-blind review over a period during which both journals experienced there was a rapid expansion in female authors published in top economics journals. Moreover, *AER* and *QJE* tend to be better written than *Econometrica* and *JPE*, according to [](#tableC3). Thus, the significantly larger effect from [#Hengel2015;] was likely due to confounding 

The gender of an author can be usually inferred from authors' given names. In the case of unisex and certain foreign first names, the high profile nature of NBER researchers and their articles probably mean most referees of these papers are aware of the gender of an author.
The one thing that might thwart this assumption is double-blind review, particularly before the internet. Two journals---*AER* and *QJE* employed double-blind review at some point during the time period covered by the data. *QJE* used double-blind procedures until 1 June, 2005. *AER*'s spell began 1 July, 1989 and ended 1 July, 2011.

---*i.e.*, the probability 
there's a 100 percent chance that solo-female-authored papers were written and/or revised by women; there's a 0 percent chance that a solo-male-authored paper was written and/or revised by women; 
the paper's "female intensity"---*i.e.*, it's proportion of female authors.
positively related to the proportion of female authors on a paper.

Yet the proportion of papers with at least one female author is much higher---18.8 percent since 1990. Because the sample sizes for 100 percent female-authored papers was so small, [#Blank1990;] used this classification strategy to define "female" papers.


 two alternative definitions---only female authors (compared to only male authors) and at least one fema

Prior papers indicate that two-thirds of co-authors describe their working relationships as "collegial"---*i.e.*, authors sharing the work as colleagues. Only a quarter of working relationships were described as "mentoring"---a senior author mentors junior authors---or "directing"---one author has primary responsibility and takes the lead in organising the research.
co-authors of the same rank tend to share the task of writing and revising the paper 

(ii) referees' beliefs about the probability papers are written by female co-authors. I assume the probability both are true is a linear function of the ratio of female co-authors.
Nevertheless
That is, if a paper's female ratio is 25 percent, then 25 percent of the time, (i) and (ii) are true. Thus, gender impacts readability in 25 percent of these papers but has no effect in the remaining 75 percent.
In mentor-mentee relationships, the tasks are predominately on the shoulders of mentees. Less likely to contribute to the writing of a paper than they would be if co-authoring with a colleague.
Significant difference between the importance of tasks performed in producing a research paper as a mentor and as a colleague.
Very important, important, less important
In mentor-mentee relationships, more important to contribute to writing and revising the paper; in colleague relationships, less important.
According to the International Committee for Medical Journal Editors, authorship credit should be based only on substantial contributions to all three of the following components: (a) conception and design, or analysis and interpretation of data; and to (b) drafting the article or revising it critically for important intellectual content; and on (c) final revision of the version to be published.
I assume authors are granted authorship credit if and only if they have participated sufficiently in the work to take responsibility for the content.
Unlike the more inclusive measure (at least one female author), female ratio captures the inherent uncertainty from both events: the probability a paper was written by a female co-author is zero or one---and referees' beliefs are presumably correct---when papers are either entirely written by men or women, respectively. Mixed gendered papers, however, are assigned a probability that (i) and (ii) are true based on the proportion of women listed as authors.

25 percent of the time when there is one female author amongst four, 50 percent of the time when half of the authors are female, *etc.* Thus, 
This strategy obviously assumes that 
 about whether a woman wrote the paper; (iii) the probability a female co-author is responsible for making editorial changes to the paper during the peer review process; and (iv) the referee's belief about whether a female co-author is responsible for 
the refereeâs perception of the contribution of a woman to a particular paper. To capture the uncertainty of his view when both men and women co-author together, I opted for the less inclusive continuous measure.


We are left with two alternative measures of a paper's gender: consider a paper "female" when at least one author is female or use the ratio of female authors to total authors as a continuous measure of a paper's gender.
Much of the analysis has been duplicated using the more inclusive measure (at least one female author) and least inclusive measure (only female-authored). The results are generally similar to those presented here.
Because the sample sizes for 100 percent female-authored papers was so small, [#Blank1990;] used this classification strategy to define "female" papers.
with only female names on the paper would be compared to papers 
The most straightforward way to deal with the gender of a co-authored paper is gender in these papers is 
In the case of single-authored papers, it is obvious how to code the gender of the paper---but 56 percent of papers are co-authored. A variety of metrics potentially deal with the gender of these papers: *e.g.*, if at least one female name is on the paper, if the primary author is female, or if only female names are on the paper.
The most straightforward way to 
according to the latter strategy would be compared to papers classified as "male. Unfortunately, however, only 4 percent of papers are predominately female-authored; even fewer (3.5 percent) are authored entirely by women. Moreover, when restricting the sample to papers after 1990 or 2000, the figures only slightly improve. Of papers published post-1990, only 5.4 percent are predominantly female-authored; even fewer (4.5 percent) are authored entirely by women. The figures hardly budge when restricting samples to papers published after 2000: 5.7 percent are predominantly female-authored; again, 4.5 percent are authored entirely by women. (See [](#figure0) for a breakdown by journal.)


Thus, higher standards in narrow dimensions that fail to contribute to the observed value of output will lower most measures of female productivity and confound gender differences in labour market outcomes. For similar reasons, common performance controls may discount discrimination in equations that relate wages (and other labour market outcomes) to gender.

For similar reasons, common performance controls may discount discrimination in equations that relate wages (and other labour market outcomes) to gender. Controlling for performance is undeniably import; yet just as important is our judgement and measurement of that performance. Higher standards in narrow dimensions that fail to contribute to the observed value of output will lower most measures of female productivity and confound gender differences in labour market outcomes.

The second panel of [](#tableAX1) displays the marginal effect of female ratio in draft and published papers. The total gap in the published article is relatively stable---between 2--3 Flesch Reading Ease points


Consistent with [](#table7), readability may actually decline during peer review. As discussed in [][NBER], this may be an artifact specific to abstracts, which are edited for length in addition to readability. Alternatively, writing (too) well upfront satisfies the review group with the highest initial readability threshold. Because referee reports reveal $s$ (and therefore $\widetilde R_i^s$), a readability decline after receiving an R&amp;R indicates that a majority of groups have laxer standards. This explanation is consistent with the theoretical model in [][SEUModel].

[](#figureA1) suggests female economists initially underestimate referees' expectations: without experience, their writing improves during peer review; with experience, they write more clearly before peer review. Women's draft readability increases between $t=1$ and $t=2$---and then again between $t=2$ and $t=3$. Consequently, women make fewer changes during peer review in $t=2$ than in $t=1$; changes shrink further by $t=3$.
Women's pattern of behaviour both resembles and differs from men's. Draft and final readability scores for male-authored papers remain relatively constant over increasing $t$. Unlike women's, men's approach does not radically change with experience: they consistently overestimate referee demands pre-peer review to minimise changes made in peer review.
This strategy mirrors women's at later $t$. Economists who anticipate demands are desk rejected less often; economists who don't enjoy more free time, all things equal. [](#figureA1) implies little---if any---gender difference in this tradeoff. Decisions by junior economists may reflect inexperience, but decisions by senior economists should not. Senior economists are familiar with peer review; their choices express optimal tradeoffs with full information (for discussion and justification, see [][SEUModel]). [](#figureA1) suggests both men and women sacrifice time to increase acceptance rates.


Matches are obviously sensitive to the choice and construction of variables in the first panel, the model used to estimate propensity scores and whether matches are made with or without replacement. Outcomes are not. After controlling for $T_i$, decade, journal and *JEL* code, matches using alternative variables (*e.g.*, minimum citation counts and minimum institutional rank) and specifications (logit and no replacement) generate similar figures and identical conclusions to those presented in [][SEUMatching]. (Not shown; analysis available on request.)


Moreover, the extent that higher standards require time investments and induce broader behavioural adjustments that reduce the quantity of female output, they may even produce superficial productivity measures that mask their effect. 
This conclusion suggests that controlling for productivity in estimates of gender earnings and promotion gaps may underestimate labour market discrimination unless those measures adjust for differences in standards.
Moreover, the extent that higher standards require time investments and induce broader behavioural adjustments that reduce the quantity of female output, they may even produce superficial productivity measures that mask their effect. 
Performance is important, 
Given women are prone to adjust behaviour in anticipation of these higher standards, even measures that appear highly objective are likely affected by them; estimates of gender gaps in career outcomes that control for performance may therefore underestimate labour market discrimination.

More broadly, these findings suggest that tougher standards in arbitrary dimensions reduce female productivity. They highlight that using performance measures to understand gender gaps in career outcomes likely underestimates---or even altogether misses---instances of gender discrimination in the labour market. Thus narrowly focusing public policies on increasing female productivity---e.g. by encouraging men to shoulder a larger portion of childcare responsibilities or promoting flexible work schedules---may not be sufficient to achieve gender equality in the workforce.
By meeting discriminatory editorial standards before submission, the readability gap between senior, productive economists could appear independent of peer review in naive estimations.

Earlier in their careers, women know less about referee expectations. [](#figureA1) suggests they *underestimate* those expectations, and, consequently, revise more during peer review. Draft readability at $t=2$ suggests 
make greater revisions during the process. suggesting women adapt behaviour and choices earlier in their career, where the learning curve is steeper. Later in their careers, when they're better informed, women's choices settle into a more stable pattern.

The difference is not significant and the sample of women with 4 or more publications is very small (see [][SEUEmpirical]). Nevertheless, a fall in readability (on average) during peer review is consistent with [](#Theorem1). Writing (too) well upfront satisfies a minority of review groups who discriminate by desk rejecting (relatively) poorly written female-authored papers. Once past desk rejection, however, the review group is "revealed" (recall that the author recognises groups that reviewed his earlier papers even though the identities of the groups' individual members remain unknown to him). In the majority of instances, this reveals that the group is non-discriminatory, so women relax their writing when making revisions. In this case, women's decline in readability between draft and published versions at $t=4\text{--}5$ and $t=6+$ could indicate that only a minority of reviewers discriminate.


implies that referees and/or editors were less inclined to demand readability edits in $t=1$ papers authored by men.
Data for publications three and up further reinforce the idea that women receive a message in peer review---write well---that men do not. 

will be desk rejected less often---and therefore accepted more frequently---than the economist who doesn't.
In the absence of bias---and assuming identical acceptance rates---senior economists would make identical decisions.
Specifically,  by female authors coping with the higher standards editors and/or referees impose on their writing.
Higher standards lead to more changes in peer review[][NBER]. They affect readability before peer review, too[][SEUModel]. 
Since women cope with demands
Having already established that women face greater referee demands than men, I now investigate how women cope with those demands.
 As a final exercise, I investigate gender differences in how this tradeoff is made.
---risking desk rejection---or anticipate demands
can choose to make changes directly in repose to referee demands *during* peer review. Or they can make changes in anticipation of those demands *before* peer review.
If I have a high probability of drawing a review group that desk rejects my paper unless it's well written, and initially laxer review groups are just as demanding during the review phase, 
 has exacting standards and an author had an especially high probability of drawing that review group, it makes more sense to meet its standards prior to peer review.
authors face a As a final exercise, I compare NBER working paper draft readability to the readability of 


The idea that women are told to write more clearly but men aren't is further reinforced at the e publications three and up, women mostly write well upfront; like men, they make very few changes during peer review. Indeed, for publications four and up, their writing gets slightly worse during peer review. Although this difference is not significant and includes only a small number of authors, this fall is consistent with [](#Theorem1). Effectively, these women write *too* well upfront because they anticipate some non-zero probability of drawing a discriminatory reviewer who desk rejects their papers if not well written. Once past desk rejection, the referee group is revealed. If the review group is not discriminatory, women relax their writing when making revisions. Thus, women's decline in readability between draft and published versions at $t=4\text{--}5$ and $t=6+$ could indicate that only a minority of reviewers discriminate.

[](#tableAX1) displays the contemporaneous marginal effect of peer review ($R_{jP}-R_{jW}$) estimated separately for men and women (first panel) and the marginal effect of female ratio in the draft paper and then in the published paper (second panel). Interestingly, without the previous context provided by [](#figureA1), the first panel of [](#tableAX1) would suggest that the readability gap is declining over increasing $t$---



Other fields---most notably social psychology---are more active on this topic[see, *e.g.*,\]\[][#Steele1995]. Economics has occasionally broached this issue, although generally within the context of identity formation[for a review of existing research, see\]\[][#Dee2014].

$e_{nit}^s-e_{nkt}^s$ converges to 0, so for large enough $t$ [](#EquationCorollary1) and/or [](#EquationCorollary2) predict the direction of $D_{ik}$ even when errors remain gender-specific. (See discussions in [][SEUModel] and the next section.)

This paper makes a curious discovery: female-authored articles in top economics journals are better written. After examining the difference, I conclude that higher standards applied by editors and/or referees are primarily to blame.

Moreover, if this effect were due entirely to women continuing to make mistakes about referee expectations at $t=3$, we should expect that a minority of men are also as sensitive as women---and so the mistakes they make are of a similar magnitude. That is, the mistakes made by the men in panel two fo [](#table10X) would be of a similar magnitude to those in panel one---just that they are made by fewer men. Yet they are not---on average, men that are discriminated against write only 20 percent more clearly than they otherwise would. This is a difference of nine percent. It seems highly unlikely that the women listed in [][AppendixMatchingNames]---a higher proportion of whom are editors and/or assistant editors at the journals in question---would persist in making errors of this magnitude relative to the men listed in [][AppendixMatchingNames].

This paper also suggests a novel technical instrument that may be used to overcome bias inherent in measures of subjective evaluation. In the presence of higher standards, subjective quality measures are themselves biased. I use readability formulas to overcome this problem. Readability formulas are transparent, objective and accurate measures of one component on which papers are evaluated during peer review--writing clarity. Similar tests to expose higher quality work may uncover higher standards in a variety of situations: successful business proposals, published "letters to the editor" or annual report introductions. Readability scores could also be employed to produce more accurate measures of labour market wage gaps---by instrumenting for unobserved bias in the subjective productivity measures themselves. Readability scores have their limitations [see][MeasuringReadability] and their use in this manner applies to just a narrow set of questions. Nevertheless, they appear to be a largely ignored, naturally occurring source of pseudo-experiments relevant to research on gender or racial bias---and differential group treatment, more generally.



- Convergence if and only if signal is informative and agent uses signal to update posterior belief. Asymptotic learning fails if the correct action is not chosen (irrationality).
- Smith and Sorenson (2000): when individuals observe all past actions and private beliefs are unbounded, information will be aggregated and the correct action will be chosen asymptotically.
- Bounded beliefs: individuals copy past actions and/or completely ignore their own signals. (Why would men's beliefs be unbounded but women's beliefs bounded?)
- Agents cannot (a) begin acting identically; (b) be subject to the same costs/rewards, (c) process signals in the same way (rationality), (d) yet end up on diverging paths. Even if we relax (a), and assume agents do not act identically---which does not correspond with what I find---assuming (b) and (c) means agents' paths remain a constant distance from one another.
- Because it does not appear that women are rewarded differently for better writing (women's acceptance rates are identical to men's and their review times longer), it must be that they have higher costs of writing poorly (again, because acceptance rates are identical to men's, the cost likely comes in more length peer review). This is equivalent to so-called "higher standards".
- There are two reasonable rewards from writing well (or, conversely, costs from writing poorly). Acceptance rates are higher and/or review times are faster. Compared to men, women do not benefit from either: numerous studies show women are not published at higher rates; my own analysis suggests women's review times are substantially longer. This latter point, especially, indicates female authors are punished more for poorer writing and/or male authors are rewarded more for clearer writing. I find some evidence of both effects. Male-authored papers that are better written do indeed enjoy faster peer review times. Given fewer changes are made to their papers during peer review, this provides a rough (although not causal) indication that better initial writing in male-authored papers shortens the time men spend in peer review. It appears to have much less of an effect for women.
Women are not rewarded for better writing's acceptance rates are identical to men's but their review times are longer. Thus suggests women are not rewarded for better writing; instead, they are punished for poorer writing.
- There is no asymptotic learning in networks with non-expanding observations (observe actions of finite subset of actors).
- When private beliefs are unbounded and the network topology is expanding, then there will be asymptotic learning.
- Note that female readability starts out identical to men's but then diverges.







++++++++++++++++++++++++++++++++++++++++++++++++
With experience, authors' beliefs about the impact of readability on acceptance rates converges to reality, a widening readability gap post experience without higher acceptance rates implies women are held to higher standards in peer review.
implies women's mistake was actually *not* in writing too clearly, but having earlier not written clear enough. When observed acceptance rates are otherwise identical between similar authors, then a widening readability gap can only be attributed to higher standards.
and with experience, authors' beliefs about the impact of readability on acceptance rates converges to reality.
if acceptance rates between men and women are identical

 writing clear enough earlier in their careers
If, with experience, the gap has widened, it must be caused by higher standards
So with experience, the readability gap should go away; if it doesn't, it must be caused by higher standards.
National Bureau of Economic Research (NBER) working papers to their final, published versions; the gap is three times larger for the latter. While both papers are exposed to many factors that impact readability, only published articles are subject to peer review. By comparing the two, influences unrelated to immediate peer review are isolated from those that are; assuming the former are not correlated with the latter's timing, a widening gap suggests a causal link---and therefore rules out some constant difference between the sexes in how well they write (holding all else equal), it would arise only during the drafting phase---that is, the time before papers are submitted to peer review. I find that this is not the case.
Second, women do not derive any apparent benefit to writing well. There are two reasonable rewards from writing well (or, conversely, costs from writing poorly). Acceptance rates are higher and/or review times are faster. Compared to men, women do not benefit from either: prior research does not suggest that journal acceptance rates are higher for women [see, *e.g.*,\]\[][#Abrevaya2012,Blank1991,Borsuk2009,Gilbert1994,Lloyd1990] nor do women enjoy speedier review times. In fact, my own analysis suggests women's review times are, on average ***six months longer***. This estimate is based on submit-accept times at *Econometrica*, is consistent across a range of specifications and, in addition to other factors, controls for motherhood and giving birth.
Third, women's writing gradually gets better but men's does not. Between authors' first and third published articles, the gap grows by 12 percent. Evidence does not suggest senior female economists co-author with more women. Nor are initially bad female writers leaving academia. Instead, women appear to get better at writing as their careers progress, whereas men donot . Given that we've already established a causal link between the readability gap and peer review, then this indicates women are receiving a form of positive reinforcement than men are not receiving. That is, men and women are receiving separate signals in peer review.
Combined with point two, this suggests gender differences in (i) biology/behaviour that manifests itself in peer review---for example, women are more sensitive to referee criticism---as well as (ii) knowledge about the advantages of writing well cannot resolve the gap. In a simple model of Bayesian updating, I show Agents cannot (a) be subject to the same costs/rewards, (b) be rational (i.e., both learn rationally and start off with the same belief sets---either both have bounded beliefs or both have unbounded beliefs) and (c) end up on paths that diverge over time. 
Thus, if we assume women are no more or less likely to systematically misinterpret signals, then observing (c) means (a) must be violated. That is, women and men are subject to different costs and rewards of writing well---i.e., they are subject to different standards. Conversely, insisting (a) is true implies women act irrationally.
Instead, women---and only women---seem to figure out that writing well makes peer review smoother; they write subsequent papers clearer from the start. 

Higher standards impose quantity vs. quality tradeoffs that characterise many instances of female output. 
undoubtedly hurting womenâs productivity and probably, as a consequence, promotion rates, too.
 explain academia âPublishing Paradoxâ: 
Higher standards impose quantity vs. quality tradeoffs that may may explain lagging female wages and productivity, more generally.

Peer review is not immune. Using five reliable measures of writing clarity, I show that female-authored articles published in top economics journals are better written than equivalent papers by men.
Academia's publishing paradox. Justifies lower pay and promotion rates. May account for small fraction of women among assistant, associate and full professors.
In this paper, I explore a possible explanation for this: women exert more effort writing their papers more clearly, and in general spend longer in peer review.
Using five reliable formulas to exploit this, I analyse 9,123 article abstracts published in the *American Economic Review* (*AER*), *Econometrica* (*ECA*), *Journal of Political Economy* (*JPE*) and *Quarterly Journal of Economics* (*QJE*).
I find female-authored abstracts are 1--6 percent more readable than those by men. Women write better despite controls for editor, journal, year and primary and tertiary *JEL* classification; that remains unchanged when proxying for article and author quality. This means the readability gap probably wasn't (i) a response to specific policies in earlier eras; (ii) caused by women writing on topics that are easier to explain; nor (iii) generated by factors correlated with gender but really related to knowledge, intelligence and creativity.
Combined with point two, this suggests gender differences in (i) biology/behaviour that manifests itself in peer review---for example, women are more sensitive to referee criticism---as well as (ii) knowledge about the advantages of writing well cannot resolve the gap. According to both interpretations, readability essentially diverges because women voluntarily accept the costs of good writing (longer drafting and revision stages) despite receiving no tangible benefit (acceptance rates do not change). Initially, this explanation is perfectly plausible; indefinitely, it is not---unless we assume senior female economists (and only senior female economists) are basically acting irrationally.
Instead, they increase. Between authors' first and third published articles, the gap grows by 12 percent. Evidence does not suggest senior female economists co-author with more women. Nor are initially bad female writers leaving academia. Instead, women---and only women---seem to figure out that writing well makes peer review smoother; they write subsequent papers clearer from the start.


Clearer sentences, less jargon and more scrutiny arenât bad things. Added attention reduces discriminatory outcomes[#Bartos2016]. It doubtless leads to a better final product: papers that are easier to understand enjoy wider, more diverse readership; closer review catches logical mistakes and leads to fewer factual errors.
Still, adding robustness checks, clarifying proofs and making sentences even marginally more readable takes time. [][AppendixModel] illustrates this idea using [#Bartos2016;]âs formative model linking discrimination and attention. The major change is a simple extension that assumes referees pass on to authors some of the cost of evaluating their papers. This prolongs peer review directly---referees spend more time evaluating womenâs papers and women spend more time responding---and indirectly---female authors take longer drafting future papers. 
I estimate the direct effect results in female-authored papers spending ***six months longer*** in peer review. This estimate is based on submit-accept times at *Econometrica*, is consistent across a range of specifications and, in addition to other factors, controls for motherhood and giving birth. It appears to dominate in papers written by early career academics. In fact, there is very little gender difference in readability in the draft version of an authors' first publication---although a large difference does emerge by the time those same articles are actually published. The indirect effect---i.e., women spending more time upfront drafting their papers---increasingly dominates as women gain experience in peer review.
In [][xxxx], I look at the indirect effect. I find the direct effect dominates among early career academics; the indirect effect dominates later in their careers. There is very little gender difference in readability in the draft version of authors' first publication---although a significant difference does emerge by the time those articles are eventually published. Initial drafts of women's second publications, however, are already much better written than men's---evidence of an indirect effect---although the still further widens during peer review.



*******************************************

Additionally, this readability gap widens precisely while papers undergo peer review.
Because better writing takes effort to compose, it likely prolongs female review time---indeed, I find female female-authored papers spend six months longer in peer review---and probably contributes to lower publishing rates.
I explore many interpretations---*e.g.*, Are women more sensitive to criticism? Is better writing due to risk aversion? Do women's papers deserve more scrutiny because they aren't as good? The only straightforward explanation consistent with the data, however, is that referees apply higher standards to womenâs writing. Because better writing takes effort to compose, higher standards prolong female review time---by six months at *Econometrica*---and likely contribute to lower publishing rates. Tougher standards applied more broadly reduce womenâs output; ignoring them undervalues female labour and may confound estimates of gender discrimination.
PARAGRAPH ON SPENDING 6 MONTHS LONGER IN PEER REVIEW
Two possible explanations can account for this gap: either referees subject female-authored papers to added scrutiny, or women choose themselves to exert more effort. Both possibilities imply women spend more time than men on a given task. On the positive side, it increases quality; on the other hand, it reduces quantity. Either could explain academia's "Publishing Paradox". Unequal time spent making revisions leads to unequal time conducting new research; as a result, women write fewer papers.
Nevertheless, I find the more plausible explanation is indeed because referees subject female-authored papers to greater scrutiny. First, women's writing gradually gets better but men's does not---meaning gender differences in (i) biology/behaviour and/or (ii) knowledge about the advantages of writing well cannot resolve the gap. According to both interpretations, readability essentially diverges because women voluntarily accept the costs of good writing (longer drafting and revision stages) despite receiving no tangible benefit (acceptance rates do not change). Initially, this explanation is perfectly plausible; indefinitely, it is not: wouldn't individual women eventually figure out there's nothing to gain by writing well---and then do it less? If so, gender differences in readability should decline as authors gain experience in peer review.
Instead, they increase. Between authors' first and third published articles, the gap grows by 12 percent. Evidence does not suggest senior female economists co-author with more women. Nor are initially bad female writers leaving academia. Instead, women---and only women---seem to figure out that writing well makes peer review smoother; they write subsequent papers clearer from the start.
Points one to three provide strong evidence that peer review is at least partially responsible for better writing in female-authored papers. But is it given that female-authored papers invite undue scrutiny? In [][alternatives], I explore several alternative hypotheses---many of which have nothing to do with bias and some that exonerate peer review, too. Among them: Are female economists disproportionately native English speakers? Are female referees the toughest critics, and, if so, are they more likely to review female-authored papers? Do manuscripts written by women deserve more criticism because they arenât as good?
As [][Alternatives] also illustrates, however, the only straightforward explanation consistent with the data is that referees are disproportionately critical of female-authored papers. Clearer sentences, less jargon and more scrutiny arenât bad things. Added attention reduces discriminatory outcomes[#Bartos2016]. It doubtless leads to a better final product: papers that are easier to understand enjoy wider, more diverse readership; closer review catches logical mistakes and leads to fewer factual errors.


Still, adding robustness checks, clarifying proofs and making sentences even marginally more readable takes time. [][AppendixModel] illustrates this idea using [#Bartos2016;]âs formative model linking discrimination and attention. The major change is a simple extension that assumes referees pass on to authors some of the cost of evaluating their papers. This prolongs peer review directly---referees spend more time evaluating womenâs papers and women spend more time responding---and indirectly---female authors take longer drafting future papers. 
I estimate the direct effect results in female-authored papers spending ***six months longer*** in peer review. This estimate is based on submit-accept times at *Econometrica*, is consistent across a range of specifications and, in addition to other factors, controls for motherhood and giving birth. It appears to dominate in papers written by early career academics. In fact, there is very little gender difference in readability in the draft version of an authors' first publication---although a large difference does emerge by the time those same articles are actually published. The indirect effect---i.e., women spending more time upfront drafting their papers---increasingly dominates as women gain experience in peer review.
In [][xxxx], I look at the indirect effect. I find the direct effect dominates among early career academics; the indirect effect dominates later in their careers. There is very little gender difference in readability in the draft version of authors' first publication---although a significant difference does emerge by the time those articles are eventually published. Initial drafts of women's second publications, however, are already much better written than men's---evidence of an indirect effect---although the still further widens during peer review.
Spending six more months in peer review may explain academia's "Publishing Paradox". Unequal time spent making revisions leads to unequal time conducting new research; as a result, women write fewer papers. 
undoubtedly hurting womenâs productivity and probably, as a consequence, promotion rates, too.
 explain academia âPublishing Paradoxâ: 
Higher standards impose quantity vs. quality tradeoffs that may may explain lagging female wages and productivity, more generally. Work that is evaluated more critically *at any point in the production process* will be systematically better (holding prices fixed) or systematically cheaper (holding quality fixed). This reduces women's wages---for example, if judges require better writing in female-authored briefs, female attorneys must charge lower fees and/or under-report hours to compete with men---and distorts measurements of female productivity---billable hours and client revenue decline; female lawyers appear less productive than they truly are.

Yet higher standards impose a quantity vs. quality tradeoff that may contribute to women's lower wages and promotion rates. Work that is evaluated more critically at any point in the production process will be systematically better (holding prices fixed) or systematically cheaper (holding quality fixed). This reduces women's wages and distorts their promotion rates. Because discrimination affects behaviour and choices, it necessarily impacts how accurately we measure their productivity. 


Stereotypes form for a variety of reasons, and discrimination exists in a plethora of forms---all of which presumably affect how victims behave. Their 

It suggests that some "feminine" traits may be more context specific than previously thought. 

This quantity vs. quality tradeoff characterises many instances of female output. According to raw numerical counts, women produce less than men. Female academics publish fewer academic journals[#Ceci2014]; female physicians see fewer patients[#Bloor2008] and submit fewer grant proposals[#Gordon2009,Klos2013]; female reporters write fewer front-page bylines[#Crozier1999]; female novelists produce less nonfiction output[#Waisbren2008]; female real estate agents list fewer homes[#Trulia2011].
When ranked by narrowly defined outcome measures, however, women generally outperform men. Female students earn better grades[#Voyer2014]; female auditors are more accurate and efficient[#Chung2001,ODonnell2001,Ittonen2013,Niskanen2011]; congresswomen secure more federal funding for their districts, sponsor more legislation and score higher on a composite measure of legislative effectiveness[#Anzia2011,Volden2013]; houses listed by female real estate agents sell for higher prices[#Seagraves2013,Salter2012]; patients treated by female physicians are less likely to die or be readmitted to hospital[#Tsugawa2016]; female pilots are involved in fewer fatal accidents[#Vail1986,Bazargan2011].
To the extent that higher standards require time investments and induce similar behavioural adjustments that reduce the quantity of female output, they may even produce superficial productivity measures that mask their effect. 

women appear to have figured out how well they need to write---and they are doing all the hard work of writing well upfront. This suggests female economists gradually anticipate higher standards *in* peer review---and consequently write future papers more clearly *before* peer review.
There is no gender difference in readability in the draft version of authors' first publication---although a significant difference has emerged by the time those articles are eventually published. Initial drafts of women's second publication, however, are already better written than men's---although the still further widens during peer review.  
how stereotypes form or 
the source of discrimination---taste-based, attention, pollution---
Indeed, many theories could explain *why* women are discriminated against in peer review---taste-based, attention, pollution, or any number of theories on how stereotypes form. The fact that it occurs, however, has important implications on how women behave. 

Its findings confirm similar conclusions in research on employee performance reviews, teaching evaluations and online comments---women receive more abusive feedback, less credit for intelligence and creativity and are expected to be more organised, prepared and clear[#Boring,2015,Correll2016,Gardiner2016)

, thereby masking the effect of discrimination. 
Indeed, irrespective of the theory behind *why* referees and/or editors discriminate against women---and there may bewomen are held to higher standards
Moreover, I find evidence that when facing discrimination, 

I find that women not only write more clearly when expected to do so---but also the way they do this can actually make these contortions resemble voluntary choice. This suggests women may modify their behaviour in ways that superficially mask the effect of discrimination by making their contortions resemble voluntary choice. 

Many of these differences are undoubtedly influenced by innate gender differences in biology and behaviour. Irrespective of their origin, however, most have intrinsic value that, under normal circumstances should be appropriately rewarded in an unbiased marketplace. Property listed by female real estate agents sell for higher prices; in the absence of higher standards, women would charge more to sell a house. They do not[#Trulia2011]. Patients treated by female physicians are less likely to die or be readmitted to hospital, in the absence of higher standards, visiting a female doctor would cost more. It does not[#XXXXXX]. Congresswomen are better legislators; in the absence of higher standards, they would be elected at higher rates. They are not[#XXXX]. Female-authored manuscripts are better written; in the absence of higher standards, women's papers would be published in academic journals at higher rates. They are not[#Ceci2014].

Or, basically, women don't expect to be treated any differently than men in peer review. Unsurprisingly, then, for the first paper, the gap in readability emerges entirely during peer review. But women are obviously learning that that their papers need to be more readable than they originally expected. So next time around, they write more clearly from the outset. 
directly---referees spend more time evaluating womenâs papers and women spend more time responding---and indirectly---female authors take longer drafting future papers. Both factors prolong peer review
The pre- and post-review analysis using NBER working papers provides evidence of the first effect. I find further evidence of the direct effect in review times at *Econometrica*, where 


But assuming authors do not *want* to be poorly informed or oversensitive, their impact on choices declines with experience. the impact of this mistake declines with experience
 these are probably not states or activities the 
with experience, those mistakes are corrected
. Holding acceptance rates constant, this implies that a widening readability gap between authors with equivalent e
women write more readably (and men possibly less so)
Thus, if women write more readably (and men even less so) after enough time has passed, 
authors who increase readability are motivated entirely by higher acceptance rates
women may miscalculate referee expectations and misconstrue their reports, but with experience they correct these mistakes. 
If an author has previously written a less readable paper and if she could obtain an identical expected acceptance rate at that same readability, she would.
Since increasing readability is motivated by an author's belief that it increases acceptance rates,
 change in readability can only be motivated by an author's belief that it increases acceptance rates. Together these factors imply that if the gap is actually wider after enough time has passed for beliefs to converge---but acceptance rate's aren't---then women because they have to. They'd be rejected, otherwise.

Earlier, we measured the direct effect, which is basically the change in readability that occurs during peer review. This is women expending effort addressing direct referee concerns during peer review. There is also an indirect effect, however, in that 
 I find the direct effect dominates among early career academics; the indirect effect dominates later in their careers. 
It appears to dominate in papers written by early career academics. In fact, there is very little gender difference in readability in the draft version of an authors' first publication---although a large difference does emerge by the time those same articles are actually published. The indirect effect---i.e., women spending more time upfront drafting their papers---increasingly dominates as women gain experience in peer review.
---and indirectly---female authors take longer drafting future papers. 
Moreover, I find evidence in the indirect effect in a supplemental pre- and post-review analysis using NBER working papers.

The data on draft papers and final papers allows us to identify these two effects over time. We see that men and women draft their papers with identical readability at the beginning of their careers. This indicates that they initially anticipate the same writing standards and they write their papers accordingly.


Unequal time spent making revisions leads to unequal time conducting new research; as a result, women write fewer papers. 
Moreover, they that I argue characterise many instances of female output---and may therefore explain lagging female productivity, wages and promotion rates, more generally.

The robustness of these conclusions is obviously sensitive to the assumptions I have made. The two primary identifying assumptions are (i) $t=3$ is large enough to eliminate gender differences in expectations and (ii) $i$ and $k$ are truly equivalent. But the accuracy of these conclusions also depends on how I have modelled authors' utility. Specifically, authors probably care about getting their papers accepted and they may care about writing well, but their marginal utility from the intersection of the two events---*i.e.*, higher utility from writing well *only* because the paper is published in a top-four journal (as opposed to field journals or second-tier general interest journals)---is assumed to be negligible. For this reason, the impact of author $i$'s personal utility from writing well is reflected only in the first stage when choosing $r_{0it}$; it is otherwise irrelevant for the second phase, when choosing $r_{1it}$. If, on the other hand, women (and not men) *only* care about readability when their papers are published in the very top journals---as opposed to field journals or second-tier general interest journals---*and* wrote initially more poorly because (i) conditional on receiving an R&amp;R, they overestimated $\widetilde R_{it}$ *but* because of risk aversion simply refused to make the changes---and were pleasantly surprised when they were accepted anyway. But then the next time they are assigned $s$, they know $\widetilde R_{it}$. And this is below some threshold; and they enjoy


Women are more risk averse[see, *e.g.*,\]\[][#Borghans2009]---could that play a role? Consider revision rounds as lotteries. An authorâs labour reveals his willingness to pay to reduce risk, so a positive link between risk aversion and better writing implies risk averse individuals exert more effort (on average) to reduce their odds of being rejected. Under these conditions, risk aversion leads to higher acceptance rates, all else equal.
So if female authors truly are more risk averse than men, their papers would---and, indeed *should*---be disproportionately represented in academic journals. This is not the case. To the best of my knowledge, publication outcomes expose no female advantage, anywhere, ever[for an overview of the literature, see, *e.g.*\]\[][#Ceci2014]. Meaning risk aversion either doesn't play a role, or conceals acceptance rates that unfairly favour men.

In [][Duration], I determine the cost of more scrutiny---at least at *Econometrica*---female-authored articles take substantially longer to complete peer review.

asymmetry from one upsets symmetric criteria applied everywhere else.

It also suggests that experience teaches women to write more readably---and men less so---thus creating or exacerbating the gender readability gap.

Assuming women do not rationally waste time writing so well---and their earlier, more poorly written papers suggest they're happy enough writing less clearly---[](#Theorem1) suggests that external forces beyond women's control is shaping decisions to write so well. To confirm this, however, requires looking at matched pairs for whom all three conditions are satisfied [simultaneously](#table10X).

 don't have to. The payoffs to writing well do not appear to be *symmetrically* high. Although women write clearer papers, they aren't published more than men---ruling out the last justification for why women exert the time women spend writing clearer papers (Condition 1, [](#Theorem1). 

As discussed in the previous section, every article examined in this paper was published in top economics journals, precluding gender analysis of acceptance rates. Nevertheless, u

Although I cannot directly determine acceptance rates with my data, mean $T$ (lifetime number of publications) do not suggest women are actually *rewarded* for their effort: on average, the women in the sample published 4.53 lar; men had 5.61. Coupled with the discussion in the previous section, thus suggests female-authored papers are accepted no more often than male-authored papers. So [](#table9X) again supports [](#Theorem1)'s conclusion that women receive harsher scrutiny or are allocated harder critics in peer review.

Men's readability scores decline as they publish more papers. For women, however, each additional paper is more readable than her last. Effects are statistically significant for four out of five scores.

First: at time $t$, $i$ and $k$ are equally accurate about referees' evaluation criteria. Second: If $s$ accepts $i$'s paper, it also accepts $k$'s paper.

First authors are those identified in the acknowledgements or listed first when authors are not ordered alphabetically. See [][Data] for more details.

Matches were made using time-invariant factors or variables averaged over $t$. The thresholds referees apply to papers may, however, depend on journal, publication year, co-author characteristics and the signal authors send with their institutional affiliation---not to mention the ratio of female authors. All of these factors vary over $t$. To account for these factors, I estimate authors' readability scores at fixed values of these [variables](#EquationMatching1).
&lt;!--\input{$PPATH/equations/EquationMatching1.tex}--&gt;
 where $\vect X_{it}$ $i$'s time $t$ number of co-authors, institutional rank, institutional rank of his highest ranked co-author, year and a dummy variable for each journal.
I separately estimated [](#EquationMatching1) on female and male authors at $t=1$ and $t=3$. 



Consider a matched pair for which Conditions 2 and 3 are satisfied, suggesting discrimination against $i$. The raw estimate of final gender difference in readability is $R_{i3}-R_{k3}$. This reflects actual discrimination only if author $i$ would otherwise voluntarily choose readability $R_{k3}$ if the probability of acceptance at that readability were identical to her probability of acceptance at readability $R_{i3}$. If $R_{k3}&gt;R_{i1}$, then this is true: $R_{i1}$ forms an upper bound on the choice of readability author $i$ would make in the absence of peer review---*i.e.*, left to her own devices, we know author $i$ would choose some $R\le R_{i1}$. If $R_{k3}&lt;R_{i1}$, however, then we do not know if $i$ would otherwise voluntarily choose readability $R_{k3}$ if the probability of acceptance at that readability were identical to her probability of acceptance at readability $R_{i3}$---we only know that if that statement were true, she would choose no higher readability than $R_{i1}$.
The purpose of Condition 2 is subtle. It rules out the possibility that women just intrinsically prefer writing more clearly than men or enjoy a lower cost of writing. Yet any such intrinsic preference or cost explanation defines the minimum readability at which a women is willing to write. Thus, *all* of her papers, including the most poorly written one, is at least this readability level. Because women will always choose a readability level that satisfies this minimum threshold dictated by their preference, any *increase* in readability must be driven by something else---*i.e.*, either their own mis-information or sensitivity or by factors outside their control.

It is also possible to generate a conservative measure of the average effect of discrimination over all matched pairs in which one member satisfies Conditions 2 and 3---and thus, determine whether the average effect leans against men or women. This is the average of the following figure: $$D_i=\bm1_i\big(R_{i3}-\max\left\{R_{i1},R_{k3}\right\}\big),$$ where $\bm1_i$ equals 1 if the author is female and -1 if the author is male.

For each author, reconstructed readability scores differ only by experience: $$R_{i3}-R_{i1}=-\mu_{i1} + \Delta\,\varepsilon_{i1}.$$ Between matched pairs, they differ on female ratio *and* preferences:  $$R_{i3}-R_{k3}=\beta_1+\Delta\,\alpha_{ik} + \Delta\,\varepsilon_{ik3}.$$ Assume both conditions are satisfied. But if $R_{i3}&gt;R_{i1}$, then $i$ $\alpha_i&lt;\alpha_k$, and $R_{i3}-R_{k3}$ underestimates $\beta_1$. If $R_{i1}&gt;R_{i3}$ then $\mu_{i1}&lt;0$. If at $t=3$ authors $i$ and $k$ enjoy identical acceptance rates at $R_{i3}$ and $R_{k3}$ and there were no bias, then they'd also enjoy identical acceptance rates at $R_{i1}&lt;R_{i3}$. Given his prior decision history, $i$ would prefer to set readability lower than $R_{i3}$. The only reason he *wouldn't* is because he *can't*---not without suffer a lower probability of acceptance. Thus, $-\mu_{i1}&gt;0$ is a conservative estimate of $\beta_1$.
&lt;!--\input{$PPATH/tables/tex/table11NEW}--&gt;


[](#Theorem1) does not require that authors are perfectly accurate at $t=3$ nor that by that point, they are (on average) unbiased about $\widetilde r_{0it}$ and $\widetilde R_{it}$. It requires only that the bias is decreasing over time for all $t$ past $t=3$. [](#Corollary1), however, does require $e_{nit}\le e_{nkt}$ to ensure [](#EquationCorollary1) and [](#EquationCorollary2) do not overestimate $D_{ik}$---although they will still consistent predict the *direction* of discrimination for large enough $t$. (See [](#footnote76).)


The second assumption in [](#Proposition1) is that the states in which $i$ is accepted is a subset of the states in which $k$ is accepted. This is stronger than---and indeed, supplants---Condition 1 in [](#Theorem1). As shown in Appendix X, however, if we relax this assumption and assume only that the weaker Condition 1 is true, then, assuming $i$ is the one held to higher standards, 

It reflects tasted-based discrimination, either by review group $s$ in stage 1 *or* the review group $\overline s\in\Sigma_{A_i}$ with the strictest stage 0 standards.

$\delta_{ik}=\widetilde R_i^s-\widetilde R_k^s$ measures differing evaluation criteria applied to different authors: review group $s$ subjects author $i$ ($k$) to higher readability standards when $\delta_{ik}&gt;0$ ($\delta_{ik}&lt;0$).
When $i$ and $k$ are arbitrary authors, a positive $\delta_{ik}$ could reflect legitimate evaluation criteria related to the topic, novelty and quality of $i$'s work.
If $i$ and $k$ are arbitrary authors,  applied to their work. Between equivalent authors, $\delta_{ik}$ measures discrimination: against $i$ when $\delta_{ik}$ is positive; against $k$ when $\delta_{ik}$ is negative.


is the difference in standards applied to $i$'s papers compared to $k$'s. When $i$ and $k$ are equivalent authors, then a positive $\delta_{ik}$ reflects discrimination against $i$; a negative $\delta_{ik}$ suggests discrimination against $k$.

$\widetilde R_i^s$ depends on the topic, novelty and quality of papers written by author $i$. Assuming $i$ and $k$ write equivalent papers,
&lt;!--\input{$PPATH/equations/equationMatching4.tex}--&gt;
where $\delta_{ik}$ is the the degree to which referees and/or editors apply higher standards to $i$ or $k$ that have nothing to do with the topic, novelty or quality of the papers written by either author. A positve $\delta_{ik}$ indicates discrimination against $i$; a negative $\detla_{ik}$ means $k$ is discriminated against.


&lt;!--\input{$PPATH/equations/equationMatching3.tex}--&gt;
and $R_i^\star$ is the $R$ that solves $\phi_i'(R)=c_i'(R)$---\textit{i.e.}, the readability level $i$ would choose in the absence of earning any utility from acceptance; as shown in the proof of~\autoref{Theorem1}, it forms a lower bound on $r_{0it}$ and $R_{it}$ for all $t$. 

Testing [](#Theorem1) is simply a matter of testing each condition individually on equivalent pairs. Discrimination exists whenever Conditions 1--3 are satisfied. If the number of women who satisfy all three conditions is significantly larger than the number of men, then women are discriminated against more when compared to equivalent men.

These results strongly suggest a causal interpretation of peer review's impact on the gender readability gap. According to all five scores, the readability of female-authored papers grew relative to male-authored papers precisely when these papers were undergoing peer review; the difference is highly significant for two scores and weakly significant for a third. When explicitly controlling for papers' draft readability, the readability gap persists and is highly significant for all five scores. Although it is premature to specify the exact mechanism that induces the gap's formation, these results nevertheless provide strong evidence that the mechanism emerges during peer review.

Determining whether authors are native English speakers would be time-consuming, yet feasible; however, it is not clear how or even if native speakers write clearer than those that are non-native but highly fluent---and most of the authors in the data would fall in one of these two categories. Nevertheless, fluency is presumably related to clarity. Unfortunately, I do not know how to determine author fluency at the time articles were submitted to journals without access to a time machine, a lot of money and the willingness of 7,241 economists to sit a TOEFL exam. But I'm open to suggestions.

Extra scrutiny leads to better papers but isn't without cost. Revisions are expensive, time consuming and and disrupt other research[for a discussion, see\]\[][#Gans1994]. 

 [#Gans1994;] also point out that styles change: George Akerlof found it difficult to publish his 1970 paper "The Market for 'Lemons'" due to its "readable" style because "mathematical rigour" was then in fashion. Nevertheless, despite sometimes preferring mathematics, journals have probably always frowned on any unjustifiably complicated prose that is present.

Of course, these estimates establish only correlations; causality requires referee reports---which I do not have. Nevertheless, but by process of elimination, I show that the most straightforward (and plausible) interpretation is that female-authored papers *are* subject to more scrutiny in that 
Working paper readability is unsurprisingly highly correlated with a published article's readability, but explicitly controlling for it has little impact on gender differences---women's papers remain 2--4 percent better written.

Although gender has no apparent impact on journal acceptance rates, my paper suggests it nevertheless has an important impact on referee feedback. 

While the question of gender differences in peer review feedback may be new, the general question of whether women receive harsher feedback is not. The findings in this paper confirm conclusions from recent research on employee performance reviews and teaching evaluations.
The contribution of this paper is threefold. Its primary contribution is to shed light on the peer review *process*. To the best of my knowledge, my study is the first not only to suggest bias could exist in the process despite its absence in the outcome, but also to document such evidence.

First study of process. to the best of my knowledge, it is the first study to document evidence of gender bias in the peer review process. As mentioned earlier, gender has no apparent impact on journal acceptance rates. But a gender neutral outcome does not preclude a gender neutral *process*---and to the best of my knowledge, my paper is the first to suggest it. Determining whether female economists are unfairly scrutinised in referee reports suffers from several identification issues, chief among them the fact that accessing those reports is impossible thanks to the privacy ensconced in peer review. To circumvent this limitation, my paper notes that *if* women are subject to greater scrutiny, and that added scrutiny.

I concentrate on the third hypothesis: do reviewers require clearer, more concise writing from women than they do men? For, if referees hold female- and male-authored papers to identical standards, the latter should be just as coherently written as the former. My identification strategy is effectively a process of elimination. I first establish that the readability gap exists. I next establish a causal link between the timing of peer review and formation (or widening) of the gap. Finally, I document numerous stylised facts about the gap that a coherent theory would need to account for.

Given the well-documented human tendency to discount female competency, it should certainly not be used as proof that the wider system is completely fair.
Given the well documented human tendency to discount female competency, it is reassuring to know that at least journal acceptance rates are gender neutral. But as this paper illustrates, a gender neutral outcome does not preclude bias in the process---and it should not be used as a definitive proof that the wider system is completely fair. Instead, what it highlights is a neutrality precisely where neutrality is easy to measure but non-neutrality where it isn't. Other research has illustrated that neutrality is quickly and easily achieved when measurement is easy and monitoring is frequent. The findings in this paper support those conclusions---and suggests that to really root out bias, we need to improve monitoring and transparency in the peer review process, possibly by adopting open review, as discussed in [][p3openreview].

I believe it is the first to identify group differences using readability scores. As discussed in [][p3measuring_readability], these scores are highly correlated with reading comprehension, cheaper than audit studies and randomised controlled trials and arguably more objective than survey data. Using readability in this context obviously applies to a narrow set of research questions and cannot replace these traditional methods; they nevertheless reflect a complementary research tool that untaps a largely ignored, naturally occurring source of pseudo-experiments. A similar approach may (or may not) uncover similar group differences in business proposals, letters to the editor printed in newspapers, student essays, etc.

Only 13 papers were written by women with young children, nine of whom gave birth during peer review

Six more months finding male co-authors---at least in the individual (and therefore anecdotal) recent experience of two women instructed by their referee to "find one or two male biologists to work with (or at least obtain internal peer review from, but better yet as active co-authors)" to prevent the paper from "drifting too far away from empirical evidence into ideologically biased assumptions"[#Bernstein2015].

Unbiased decisions may be inversely related to the decision maker's belief in his own neutrality.
In several surveys female academics report bias from their superiors, peers and students[see, *e.g.*,\]\[][#Williams2015]. 
In fact, there is no significant gender difference in abstract readability for authors' first publication in the data---papers initially composed without prior exposure to top journals' refereeing process. This finding further supports a causal link to peer review. Since readability scores are more gender neutral for first time publications, the gap is unlikely fully explained by either the residual impact of stricter writing standards for women at an earlier stage in their education or inherent gender differences in writing clarity. For, if either were true, a significant persistent gender difference should be present regardless of previous publication counts---and especially in the one with the largest sub-sample.

&lt;!--\label{fn3}--&gt;Applying [](#p3equation1) to several increasing, convex transformations of the female ratio and restricting the sample to only male authors supports this conclusion: coefficients on the transformed variables are generally highly significant and as large (or larger) than the equivalent effect for women (see Online Appendix).


Given the average number of words per sentence decreases during peer review but as a fraction of total word count, syllables, polysyllabic words and difficult words rise, peer review has an uncertain impact on four out of five readability scores. Here I find a pos
Other assessments of peer review's impact on papers' readability scores have found a positive impact; for the Flesch Reading Ease and Dale-Chall scores, [](#p3table7) suggests a negative effect---at least for men. The Flesch-Kincaid, Gunning Fog and SMOG scores are positive. 
For women, however, a combination of their larger fall in words per sentence coupled with greater cuts in hard words relative to hard words means during peer review, readability scores universally increase for female-authored papers; that increase is significant for the majority of scores.
Because women cut more hard words than men, readability scores for female-authored papers universally increase, and that increase is 


significant for the majority of scores.

---and again the drop is steeper for women. The hard word count to total word count ratio actually increases: total abstract word count falls more than hard word counts do, *i.e.*, simpler words are more likely to be cut than are hard words. Because women delete proportionally more 
Because total abstract word count falls more than hard word counts do, however, the hard word count to total word count ratio increases.
---but actually increases hard word count to total word count. This occurs because total abstract word count falls more than hard word counts do.
and may reflect the fact that abstracts are edited also for length.
Peer review unambiguously shortens sentences---although again, the drop is steeper for women. For both men and women, however, it may actually increase the three estimates of "hard" words to total words in panel two. This likely occurs because abstracts are edited not just for readability, but also for length. Because the fall in total word count is greater than the fall in the number of hard words, these ratios increase during peer review. Because women are still deleting a larger proportion of difficult words relative to men, 
although, again the drop is steeper for women.
Papers written by women, however, undergo the most changes. Women's sentence, character and word counts fall by 16--18 percent more than do men's; syllables, polysyllabic and difficult words fall by 6--16 percent more.
The fact that paper's don't universally improve is due to 
Abstracts may be extensively edited during peer review, but readability scores adjust by somewhat less than the textual components used to calculate them. This likely reflects the particular nature of abstracts, which are edited not only for readability, but also for length. When the fall in 
The fall in syllables and so-called "difficult" words is proportionately less than the fall in word count. Because four of the five scores are functions of "difficult" words per total word count, the aggregate effect is ambiguous. According to [](#p3figure2), poorly written draft papers emerge from peer review with higher readability scores (above the 45 degree line); abstracts that were already well written may come out with slightly lower scores (below the 45 degree line). Nevertheless, [](#p3figure2) also highlights that female-authored published papers are consistently more readable than they were as working papers relative to male-authored papers.

Female academics are less likely to make tenure, take longer when they do and earn much less than their male peers[#Bandiera2016,Ceci2014,Ginther2004,Sarsons2015,Weisshaar2014]. One culprit is productivity. 

More generally, this study augments recent research linking gender pay and promotion differentials to productivity by showing men are held to lower evaluation standards. As expected, women adjust to this favouritism by raising the quality of their output, thereby increasing the time required to complete a task. If the marginal cost of this adjustment exceeds its benefit, female productivity will decline. 

These statistics are uncomfortable, but their causes are myriad: lower publishing rates, career choices, motherhood and, probably, bias. In lab experiments women are subject to tougher standards. Their qualifications and ability are underestimated[#Foschi1996,Grunspan2016,Moss-Racusin2012,Reuben2014]. Female-authored manuscripts are evaluated more critically[#Goldberg1968,Krawczyk2016,Paludi1983]; when collaborating with with men, women are given less credit[#Heilman2005,Sarsons2015].
Peer review is not immune. Using five reliable measures of writing clarity, I show that female-authored articles published in top economics journals are better written than similar papers by men; the simplest interpretation is that editors and referees expect clearer, more direct writing from women. Because better writing takes effort to compose, higher standards prolong female review times---by six months at *Econometrica*---and may be a fundamental factor behind lower publishing rates.

Policy has responded by encouraging more flexible work hours, better paid and longer maternity leave and greater nursery care options. These policy responses are important; but the analysis of submit-accept times suggests women without any family constraints take the longest in peer review. This suggests current policy interventions may have very little impact on the gender and earnings gap in academia. To the extent that women face similar higher standards in other high-skilled occupations, these findings therefore suggest such policy interventions will do little to increase female representation in senior positions. This emphasis on schedules and child-rearing has shifted focus from cases of actual discrimination witnessed heretofore mainly in the lab---women's qualifications are underestimated, they are evaluated more critically and when collaborating with men, women are given less credit---to external factors independent of the firm. To the extent that it has highlighted the challenges and responsibilities hoisted on the typical working mother, it may even have unintentionally reinforced the gender wage gap by attributing pay differentials caused by discrimination to external factors beyond the firm's controls, effectively creating a convenient justification for lower wages that may actually have been caused by discrimination.

The gap forms dynamically at both the author- and article-levels---it grows between first draft and final publication and over the course of women's careers---preventing factors fixed in either direction from entirely resolving the difference. This rules out inborn advantages and one-off improvements in response to external circumstances unrelated to peer review.
Financing obstacles decline. When the "correct" decision is liquidation, however, judicial mistakes have precisely the opposite effect: more accurate judges correspond to more lenient lending conditions.
that the most informative signal is not always the ideal signal
To do

1. Space at beginning of paragraph =&gt; text substitution for &lt;!--\noindent--&gt;
2. Format proofs

2. Devise term for potentially cash flow/balance sheet insolvent firms.
3. Ensure changes to W, V and Y are consistent throughout text.
4. Create an annex explaining gory detail of equations.

5. Fix proofs.
6. Rewrite introduction.
7. Rewrite literature review.
8. Create the following extensions:
	a. Over borrowing if Z&lt;0
	b. Balance sheet insolvent: D-(X+K2)
	c. Long term contract.

More generally, the impact higher standards has on gender productivity gaps suggests caution in interpreting any estimates of wage discrimination stemming from equations relating earnings to gender, even (perhaps especially) with a large set of variables designed to control for differences in productivity---or, indeed, in any estimates of labour market discrimination.

This raises concerns about ignoring higher standards in wage equations that control for productivity.
More generally, the impact higher standards has on gender productivity gaps suggests caution in interpreting any estimates of wage discrimination stemming from equations relating earnings to gender, even (perhaps especially) with a large set of variables designed to control for differences in productivity. 
 six-month gap in peer review times *still* underestimates the cost to women for higher standards.
 Here, this finding suggests higher standards increase how much time women spend writing papers even before having to spend six more months in peer review. Thus, a six-month gap in review times may *still* underestimate the time tax women pay for higher standards.


lengthen the time between a paper's inception and publication beyond inducing a six-month longer 
higher standards reduce female productivity beyond the 
a six-month gap in review times *still* underestimates how much the impact higher standards have on 

productivity measures are further distored as women pre-emptively alter behaviour in response to hightened expectations.
This suggests productivity measures are further distorted as women pre-emptively alter behaviour in response to heightened expectations. Combined with women's surprising sensitivity to higher standards, 

women address higher standards before peer review even starts.
higher standards also impact how well women women allocate time and effort outside of peer review.
a portion of the effort women put into addressing higher standards occurs before their papers are even submitted.


My evidence also indicates higher standards have subtle, indirect effects on female productivity. I find peer review induces women to draft future papers more clearly. This suggests not only that women are highly sensitive to referee criticism, but also that they adjust behaviour indirectly to deal with them
also indicates that past experience with peer review induces women to draft future papers more clearly from the start. This finding suggests women 


even in a very controlled and highly scrutinized environment, these can be biased against minorities.  within a high-skilled, professional environment while cataloging a mechanism---higher standards---for why this occurs.
Disadvantaged players threw pitches that required less discretion to interpret; advantaged playersâs needed more. Because the latter category are harder to hit, advantaged pitchers gave up fewer runs per game---a common metric of pitching performance. Thus disadvantaged (usually minority) pitchers appeared less skilled than they actually were. An important contribution of this paper is to confirm this conclusion in the context of gender discrimination within a high-skilled, professional environment.
They found calls by home plate umpires expressed 
home plate umpires made racially biased calls in low-scrutiny settings; disadvantaged pitchers respond to these situations by throwing more conspicuous strikes. An important contribution of this paper is to confirm both conclusions in the context of gender discrimination in a previously unmonitored, high-skilled, professional environment.

 and reduces research output, academia's benchmark for pay and promotion. Thus while performance may be an important to explaining the underrepresentation of women in economics, at least equally important is how we judge that performance. 
Moreover, women's permanent behavioural adjustment accumulates at a rate of X% per paper, meaning the gender readability gap mirrors wage dynamics in many high-skilled occupations[#Bertrand2009]. Higher standards may therefore contribute to women's general underrepresentation in senior positions.



Moreover, my results suggest a possible link between higher standards and the gradual increase of the gender wage gap with age. I find the readability gap does not decline with seniority---*i.e.*, women are subject to higher standards throughout their careers. By limiting the rate at which women accrue publications, they generate a wage and promotion gap that widens as economists age.
This limits the rate at which women accumulate publications, and leads to a widening gap in accrued publications Because academic pay is generally determined by researchersâ aggregate publication counts, this phenomena may explain why the gender wage and promotion gap is widest between well-published, senior economists.
potentially explains why academiaâs publication gap is widest between well-published, senior economists. Because pay in academia is generally determined by publications accumulated over an academicâs career
This potentially explains both why the 
 and generates the widest publication gap between well-published, senior economists.
gender productivity gaps are widest between well-published, senior economists, because differences in standards prevent women from accumulating more publications.
 women must contend with higher standards throughout their careers---and that translates into a widening
This indicates that female economists are subject to a constant difference in writing standards over the course of their careers---which translates into a a widening 
, then the gap between male and female productivity will widen
If higher writing standards mean female economists take twice as long to write a paper, then their wage growth will be twice as slow as men, unless pay and promotions fairly compensate for both publication quantity and writing quality.
As I show in [][p3Experience], the readability gap grows by X% for each new paper a woman writes because women accumulate permanent improvements in how well they write. This gradual progression indicates that women incur *constantly* higher standards throughout their careers---*i.e.*, they do not benefit *at all* from lower standards as they gain experience. If constantly 
Frustration may also lead women to drop out of the labour market
itâs widest between well-published, senior economists---mirroring wage dynamics in many high-skilled occupations
mirrors wage dynamics in many high-skilled occupations[#Bertrand2009]---the gap is particularly wide between well-published, senior economists---suggesting their accumulative impact plays a role in womenâs âglass ceilingâ.

permanently improve their writing in response to referee criticism---and that accumulates at a rate of X% per paper. The analyses in [][p3NBER] and [][#p3Experience] suggest female-authored drafts are already better written before peer review even starts; the gap is particularly wide between well-published, senior economists. This finding suggests women write better in anticipation of higher standards yet to come. Thus higher standards may prolong peer review indirectly as women write more clearly prior to submitting. This increases the time required to write a paper. Female productivity again declines.

My findings suggest that gender discrimination can crop up in surprising and subtle ways. It also suggests that until we no longer see this type of discrimination in the lab, we should assume it still exists, it has just manifested itself in areas we have yet thought to look in.

 re suffers from a form of attention discrimination in so-called "lemon-dropping" markets---*i.e.*, markets in which the average candidate is acceptable. ---in a recent model by[#Bartos2016;]. find applicants of a particular sex or ethnic group are more heavily scrutinised either because employers dislike them (simple discrimination), believe they are less productive (statistical discrimination) or cannot as accurately interpret their signals (screening discrimination). The results from my field experiment supports these conclusions. I extend their work by suggesting an employers' tende

With respect to the mechanisms through which discrimination emerges, this study most closely resembles work by[#Parsons2011;] on Major League Baseball umpires. They found home plate umpires made racially biased calls in low-scrutiny settings; disadvantaged pitchers respond to these situations by throwing more conspicuous strikes. An important contribution of this paper is to confirm both conclusions in the context of gender discrimination in a previously unmonitored, high-skilled, professional environment.

My findings suggest these gaps will endure in every profession unless standards for women align with men's or women are fairly rewarded for raising them.
Higher writing standards improve quality but also lengthen the time between a paper's inception and publication, thereby limiting the number of papers its author can write. 
My evidence also indicates women write better in anticipation of higher standards yet to come, indicating performance m
Performance is important, yet it is also important how we judge performance. If higher standards for female output persists in narrow dimensions without contributing to the value of that output, this will lower female productivity
Given women are prone to adjust behaviour in anticipation of these higher standards, even measures that appear highly objective are likely affected by them; estimates of gender gaps in career outcomes that control for performance may therefore underestimate labour market discrimination.
Longer reviews limit the number of papers a woman can write in her lifetime.
This increases time between a paper's inception and its publication and limits the number of papers a woman can write. Assuming research productivity measures do not commensurately reflect higher standards, female productivity will remain artificially low. This may resolve academia's "Publishing Paradox".
This paper sheds light on gender differences in productivity.
Lower productivity due to higher standards provides a novel explanation for women's underrepresentation in economics departments. Mirroring most professions, female economists are less likely to make tenure, take longer when they do and earn much less than their male peers[#Bandiera2016,Ceci2014,Ginther2004,Sarsons2015]. These patterns persist despite appropriate controls for individual and work characteristics, and for at least a decade, they nave not declined. 
I find papers by both sexes are altered during peer review; the changes made to female-authored papers, however, are far more substantial and are positively correlated with the length of time spent in peer review
The results demonstrate that a gender neutral outcome does not preclude a gender neutral *process* that can still bias the outcome in unintended ways.
This conclusion indicates that instances of gender discrimination may have been underestimated in the past.

This conclusion suggests that controlling for productivity in estimates of gender earnings and promotion gaps may underestimate labour market discrimination unless those measures adjust for differences in standards.
The readability formulas used in this analysis provide a transparent, objective and accurate measure of higher standards---albeit in only the narrow dimension of writing clarity.
This increases the time between a paper's inception and publication and decreases the number of papers women can write. Assuming expending effort on writing well is not commensurately rewarded in measures of research productivity, female productivity will appear lower than it is.
If higher standards for female output do not increase its value---*e.g.*, better writing does not impact tenure and pay decisions---female productivity will decline. 

Unless women are accurately compensated for this tax on their time and resulting quality of their output, they will appear less productive---an obvious disadvantage when employers rely on imperfect measures to make hiring and promotion decisions. 

Alternatively, if clients tolerate fewer grammatical errors in female-authored legal briefs but do not pay more for those briefs, female lawyers will either reduce their fees or bill fewer hours than they actually worked, both key performance indicators in the legal industry.

Referee criticism prompts clearer drafts of future papers

While performance is important, what's also important is how we judge performance. The readability formulas used in this analysis are transparent, objective and accurate measures of writing clarity. An important advantage of readability formulas is that they establish an objective measure of quality that changes with referee subjectivity. Publication in a top economics journal is a transparent and comprehensive indicator of performance. Combining these measures therefore produces a productivity measure that transparently reflects the *actual* productivity of women. Such a measure may prove a potent instrument to correct for estimates of gender gaps in career outcomes that control for performance.

Finally, this paper contributes to research that women are held to different---usually higher---standards than men as well as broader empirical work on the negative implications of stereotypes.
This research also adds to other recent studies that have shown that women are held to different, usually higher, standards than men in the workplace. Although writing standards are the focus here, prior research suggests women and men are held to separate standards beyond this narrow dimension.
This research also contributes to the recent research on the impact of stereotypes. Its findings confirm similar conclusions in research on employee performance reviews, teaching evaluations and online comments---women receive more abusive feedback, less credit for intelligence and creativity and are expected to be more organised, prepared and clear[#Boring2015,Correll2016,Gardiner2016].
These findings also emphasise the role transparency and monitoring play in reducing discrimination. Journal acceptance rates are easy to measure and frequently auditing; both factors foster accountability, which in turn encourages gender neutrality[#Foschi1996]. Indeed, prior investigations suggest acceptance decisions are genuinely bias-free [see, *e.g.*,\]\[][#Abrevaya2012,Blank1991,Borsuk2009,Gilbert1994,Lloyd1990]. The content of referee reports however, is subject to much less scrutiny; referees have fewer incentives to suppress bias.

My findings suggests these gaps will endure unless standards for women align with men's or women are fairly rewarded for raising them. 

Women in many high-skilled professions appear less productive than men[#Ceci2014,Xie2005,Ecklund2011,Azmat2016]. Unfortunately, there are relatively few studies that provide compelling explanations for these productivity differences: productivity gaps remain despite accounting for children, responsibilities and hours worked.
Instead, this paper suggests that academia's primary yardstick of productivity---the number of articles published in top journals---is distorted during peer review. Using several objective measures of writing quality, I show that editors and referees in top economics journals expect clearer, more direct writing from women. As a consequence, two factors emerge to reduce female productivity: (i) female-authored papers spend more time in peer review; and (ii) women start writing subsequent papers clearer upfront in anticipation of referee criticism.

More broadly, these findings suggest that tougher standards in arbitrary dimensions reduce female productivity. They highlight that using performance measures to understand gender gaps in career outcomes likely underestimates---or even altogether misses---instances of gender discrimination in the labour market. Thus narrowly focusing public policies on increasing female productivity---e.g. by encouraging men to shoulder a larger portion of childcare responsibilities or promoting flexible work schedules---may not be sufficient to achieve gender equality in the workforce.
women may modify their behaviour in ways that superficially mask the effect of discrimination by making their contortions resemble voluntary choice.

Indeed, under these conditions---properly addressing referee concerns increases one's acceptance odds, and women more consistently and accurately address those concerns---female-authored papers *should* enjoy higher acceptance rates.  Thus *either* ignoring referee concerns has no consequence
Yet comparisons between 
Indeed, if women are better at addressing referee concerns, their papers *ought* to be accepted at higher rates. Since there is no evidence that 
 Failing to address referee concerns is effectively a lottery: I benefit by saving time, but lose by suffering a higher probability of being rejected during a revise and resubmit
Responding to referee requests can be seen as a lottery: if I address those concerns, I increase the probability my paper is eventually accepted but also lose time in addressing those concerns. A female strategy 

Instead, women---and only women---apparently figure out that better writing makes peer review smoother; they write subsequent papers clearer from the start. This evidence is consistent with higher standards.

There is no evidence selection effects are behind the [trend][p3ExperienceExplanation]. Senior female economists do not co-author with more women and senior male economists do not co-author with more men. (In fact, senior economists regardless of gender publish more often with members of the opposite sex than they did as juniors.) It is also not true that initially bad female writers (or initially good male writers) disproportionately leave academia: the readability of an author's first paper has zero predictive power on how often he (or she) eventually publishes in a top four economics journal.




This third point is consistent with the possibility that higher standards are causing the gap. 

 learn that writing well is not worth the effort---*i.e.*, readability declines (or at a minimum, remains constant) over the course of their careers. 
women's writing gradually gets better but men's does not. 
With no benefit to better writing, however, they cannot account for the relationship actually observed: women most familiar with how peer review works write the clearest.


update posterior beliefs about the benefits and/or expectations of readability or the expectations of referees after successive rounds of peer review. Thus (ii), readability declines over the course of women's careers; or (i)
point to constant or declining readability as women update believes about the payoffs to clear writing after successive rounds of peer review
 in order to write more clearly although they do not actually generate any tangible benefit (acceptance rates do not change)
differences in behaviour, biology and/or information about the costs of writing well as grounds for explaining the gender readability gap. 
These interpretations imply women accept the cost of writing well (longer drafting and revision stages) despite getting no benefit (acceptance rates do not change)
behavioural, biological and or misinformation grounds for explaining the readability gap. These explanations 
explanations that imply women accept the cost of writing well (longer drafting and revision stages) despite getting no benefit (acceptance rates do not change). Bayesian learning applied to these theories point to constant or declining readability after successive rounds of peer review. 
with greater exposure to peer review. Without any benefit to better writing, however, readability should not improve---yet exactly those women who write the clearest are also the women most familiar with how peer review works.
These explanations are motivated either by (i) fixed biological differences between the sexes; or (ii) misinformation about the costs of writing well. In the latter case, women update posterior believes about its benefits after successive rounds of peer review---*i.e.*, readability declines over the course of their careers. In the former, readability may decline---as women adapt to environmental expectations---or remain constant. Neither explanation accounts for the relationship actually observed: the clearest female authors are also the women most familiar with how peer review works.

women most familiar with how peer review works write the clearest.
female economists who write the clearest are precisely those women most familiar with how peer review works.


onsider first the author's choice of $r_{1it}$ given $r_{0it}$of $R_{it}$---*i.e.*, To do that, I restrict attention to the "revise and resubmit" (R&amp;R) decision---*i.e.*, I analyse the author's choice of $r_{1it}$, given $r_{0it}$. Section X investigates the relationship between $r_{0it}$ and $r_{1it}$, and correspondingly discusses in more detail the initial screening process of papers and author's original choice of $r_{0it}$.
The author submits the initial draft of his paper to a journal. There, it is assigned to the group of reviewers $e\in\mathcal E$, where $\mathcal E$ is the set of all review groups. Given the focus of the analysis, we have assumed that review group $e$ has already decided to invite the author to revise and resubmit his manuscript. At the same time, they have collectively decided that the paper should be published if and only if after revising the document, the paper's readability is at least $\widetilde R_{it}^e$. $\widetilde R_{it}^e$ is a threshold on readability specific to the review group $e$; it depends on the characteristics of the paper---*e.g.*, topic, novelty and quality---journal and author; it may be prejudiced by the review group conditional on any of these factors.
Unfortunately, the concept of readability is complex and ambiguous, meaning members of the review group cannot convey $\widetilde R_{it}^e$ directly. Instead, they compile a "referee report" from which the author forms expectations about $\widetilde R_{it}^e$ by assigning subjective probabilities $\mu_{1it}^e(R)$ to all $R\in\mathbb{R}$. $\mu_{1it}^e$ depends on characteristics of the paper, journal and review group and may be biased by the author's interpretation of the referee report or his mis-information about peer review.
The author's subjective expected utility from choosing readability $r_{1it}$ given his initial choice $r_{0it}$ is $$\mathrm{\widehat M}_{1it}^e(R_{it})\,u_{i}-c_{1i|r_{0it}}(r_{1it}),$$ where $\mathrm{\widehat M}_{1it}^e(R_{it})$ is the sum of all $\mu_{1it}^e$ such that $\widetilde R_{it}&lt;R_{it}$, $u_{i}$ is the author's utility of having his paper accepted and $c_{1i|r_{0it}}(r_{1it})$ represents the cost of making changes $r_{1it}$ given the paper's initial readability $r_{0it}$. $c_{1i}$ is increasing and convex in $r_{0it}$ and $r_{1it}$. Marginally increasing $R_{it}$ generates proportionally less utility but requires proportionally more effort when the paper is already well written.
$u_i$ is strictly positive and independent of $R_{it}$. Authors probably care about getting their papers accepted and they may care about writing well, but their marginal utility from the intersection of the two events---*i.e.*, higher utility from writing well *only* because the paper is published in a top-four journal---is assumed to be negligible. For this reason, the impact of author $i$'s personal utility from writing well is reflected only in the first stage when choosing $r_{0it}$ (see Section X, where it is included as a specific element in the author's objective function); it is otherwise irrelevant for the second phase, when choosing $r_{1it}$.
&lt;!--\input{$PPATH/theorems/lemma}--&gt;
In other words, women chose to make their papers more readable because they think it will improve the probability their papers are accepted. This does not imply that utility or risk aversion or whatever doesn't factor into women's choice of $R_{it}$. As long as writing more clearly means a higher probability of acceptance, a higher $u_j$ will indeed induce authors to write more clearly. Women may also be more responsive to referee criticism (reflected in a more pessimistic $\mu_{j0}^e$ for all $\widetilde R_j^e$), but it has to be accompanied by a higher probability of acceptance. Absent that, SEU rational agents will not choose $R_{j}&gt;R_{j}'$.
Do women get an advantage from writing more clearly? Theorem X is conditional on having received an R&amp;R. That is, all women who receive an R&amp;R and still write more clearly than men during the revision process expected a strictly higher probability of acceptance.
This can explain a wide variety of behaviours. For example, women are more sensitive to referee criticism not because they enjoy being sensitive but because they *think* it is rewarded with a higher acceptance rate. Similarly, women may indeed be more risk averse---reflected in X by a higher instantaneous utility of acceptance---but if they choose to write more clearly, again, it was only because they *thought* it would improve their probability of acceptance.

**Corrollary**. Assume $R_{kt1}$ and $R_{it1}$ satisfy Assumption (3) and $\widehat\mu_{kt1}(R_{kt})=\widehat\mu_{it1}(R_{it})$, where . Then there exists a $\widehat\Delta_{ik}(R_0)&lt;0$ such that $$\lim_{t\rightarrow\infty}\Delta_{ikt}(R_0)\longrightarrow\widehat\Delta_{ikt}(R_0)$$ almost surely.
Yet readability *does not* improve the probability that women's papers are accepted. Although women's papers are indeed more readable, to my knowledge no prior investigation of gender bias in peer review has previously shown that women's papers were accepted at any *higher* rate than men's.
Women might be more likely to choose a higher R if their utility or acceptance was higher or marginal costs were lower. But what Theorem X establishes is that that they don't do that for the hell of it---they are only going to write more clearly if comes with an additional benefit.



Groups classified together share, *inter alia*, a disposition toward the paper, attachment to the author, attention to detail and range of conscientiousness. 



; it is independent of $\widetilde r_{0it}^e$ and likewise conditional on characteristics of the paper, journal, review group and author. If $R_{it}&lt;\widetilde R_{it}^e$, the paper is rejected.

$\rho_{it}^e$ reflects the quality and clarity of referee reports written by group $e$. $\widetilde r_{0it}^e$ and $\widetilde R_{it}=\widetilde r_{0it}+\widetilde r_{1it}^e$ are readability thresholds imposed by group $e$ for granting author $i$'s $t$th paper a "revise and resubmit" (R&amp;R) and accepting it for publication, respectively. That is, the author is given an R&amp;R if $$\widetilde r_{0it}&lt;r_{0it}.$$ But then after granting the author an R&amp;R, review group $e$ is willing to accept it only if the revised manuscript is at least $$\widetilde R_{it}^e&lt;R_{it}.$$
make two separate decisions. The first decision is whether to invite the author a "revise and resubmit" (R&amp;R); the second decision is whether 
are distinguished by two rejection thresholds on readability imposed at 
minimum readability threshold they impose on author $i$'s $t$th paper, denoted by $\widetilde r_{0it}$ and $\widetilde r_{1it}$, and the quality and clarity of their referee report, denoted by $\mathcal r\in\mathcal R$, where $R$  is the set of all referee reports.

 each of which is distinguished by the quality and clarity of its referee report. Depending on the characteristics of the journal, paper---*e.g.*, topic, novelty and quality---and author---*e.g.*, institutional affiliation and gender---review group $e$ grants author $i$ a "revise and resubmit" (R&amp;R) if $\widetilde r_{0it}^e&lt;r_{0it}$, where $\widetilde r_{0it}^e$ is a readability threshold specific to review group $e$.

denote the publications threshold on readability at time 1. In the even an R&amp;R is granted, review group $e$ will still reject the revised paper at time 1 if $$R_{it}&lt;\widetilde R_{i}^e,$$ where $\widetilde R_{i}^e=\widetilde r_{0i}^e+\widetilde r_{1i}^e$,
where $\widetilde r_{1i}$ reflects the difference between the initial threshold for receiving an R&amp;R and the threshold for publication.
time 1 threshold on readability specific to review group $e$ and author $i$---*i.e.*, the 
n the event an R&amp;R is granted, review group $e$ will still reject the revised paper at time 1 if $$R_{it}&lt;\widetilde R_{i}^e,$$ where $\widetilde R_{i}^e=\widetilde r_{0i}^e+\widetilde r_{1i}^e$, $R_{it}=r_{0it}+ r_{1it}$, $r_{1it}$ are the author's revisions and $\widetilde r_{1i}$ reflects the difference between the initial threshold for receiving an R&amp;R and the threshold for publication. $\widetilde r_{0i}$ and $\widetilde r_{1i}$ are assumed to be independent.
The concept of readability is complex and ambiguous, meaning members of the review group cannot convey it directly. Instead, $e$ tries to convey this information in a referee report, denoted by $\rho_i^e$. The author uses the report $\rho_i^e$ to form expectations about $\widetilde R_i^e$ by assigning subjective probabilities $\widehat\mu_{1it}^{\rho_i^e}(R)$ to all $R$. $\widehat\mu_{1it}^{\rho_i^e}$ may be biased by the author's interpretation of the referee report or a skewed perception of peer review.
 Authors, in turn, misconstrue even excellent reports when they are, *inter alia*, over- or under-confident in their paper's other qualities or excessively sensitive to low-probability events. Thus, the author's interpretation of the report is imprecise; the expectations he forms about $\widetilde R_i^e$ can only be represented in the subjective probabilities $\widehat\mu_{1it]^e(R)$ he assings to all $R$ after reading the report.

It *also* implies that if author $i$ choses a higher readability than another author with an otherwise equivalent paper, then author $i$ anticipated a higher probability of acceptance, *ex ante*.


that unless author $i$ chooses the same $r_{0it}$ and $R_{it}$ every single time period, 
Yet when he chooses to *change* his readability from one time to the next, that decision is driven by his desire to amp up his probability of acceptance.
[](#theorem1) shows that if at time $t$ author choses $r_{0it}$ and $R_{it}$ but at some other time $t'$ he chose $r'&lt;r_{0it}$ and $R'&lt;R_{it}$, then the higher readability at time $t$ is driven exclusively by the author's desire to attain a higher acceptance rate. 
 author $i$'s *ex ante* subjective probability of being accepted given his optimal readability choices $r_{0it}$ and $R_{it}$ is never equal to (and strictly greater than) his subjective probability of acceptance at every other $r'&lt;r_{0it}$ and $R'&lt;R_{it}$.


[](#theorem1) also implies that an increase in readability between time $t'$ and time $t$ was driven by the expectation of being rewarded for it with a higher acceptance probability. It *also* implies that if author $i$ chose a higher readability than another author with an otherwise equivalent paper, then author $i$ anticipated a higher probability of acceptance, *ex ante*.
the optimal choice of readability is always linked to the probability of acceptance: although many reasons may factor into a woman's choice of $r_{0it}$ and $R_{it}$, but absent a higher acceptance rate, women do not increase
women chose to make their papers more readable because they think it will improve the probability their papers are accepted. This does not imply that utility or risk aversion or whatever doesn't factor into women's choice of $R_{it}$. As long as writing more clearly means a higher probability of acceptance, a higher $u_i$ will indeed induce authors to write more clearly. Women may also be more responsive to referee criticism (reflected in a more pessimistic $\pi_{0it}^s$ for all $\widetilde R_{it}^s$), but it has to be accompanied by a higher probability of acceptance. Absent that, SEU rational agents would prefer to choose a smaller $R$.
This can explain a wide variety of behaviours. For example, women are more sensitive to referee criticism not because they enjoy being sensitive but because they *think* it is rewarded with a higher acceptance rate. Similarly, women may indeed be more risk averse---reflected in X by a higher instantaneous utility of acceptance---but if they choose to write more clearly, again, it was only because they *thought* it would improve their probability of acceptance.

But I'm also relatively certain that if I could get away with writing less readably, then I would.


This does not imply that $u_i$, $\phi_i$ and $c_i$ do non influence $r_{0it}$ and $R_{it}$. It does imply, however, that their influence should not be reinforced over timeFor their influence on readability to be *reinforced* however, they must bolster other *explicit* objectives. For example, sensitive authors do not become more sensitive unless that sensitivity is useful for achieving the explicit objectives in [](#equationX1) or [](#equationX2). Thus, Indeed, if at any point he chooses to write his current paper more clearly than some previous paper, this choice was motivated by a higher ex ante subjective expected probability of acceptance. Absent that advantage, author $i$ prefers the smaller readability chosen previously.
$u_i$, $\phi_i$ and $c_i$ remain constant over time. Thus, changes to $r_{0it}$ and $R_{it}$ imply changes in the subjective probabilities author $i$ forms about $\widetilde r_{0it}$ and $\widetilde R_{it}$. Thus, when authors write current papers more readably than past or future papers, they do so because they expected to be rewarded for it with a higher probability of acceptance. Changes to the optimal choices $r_{0it}$ and $R_{it}$ are motivated entirely by a desire to achieve his other objective of getting his paper accepted into a prestigious academic journal.
[](#theorem1) indirectly implies that if author $i$ choses a higher readability than another author with an otherwise equivalent paper, then author $i$ anticipated a higher probability of acceptance, *ex ante*. Of course, that doesn't mean author $i$ is right---it is perfectly plausible that he's misinformed and there is no difference in their probabilities of acceptance. As $t\rightarrow\infty$ in a fixed state $s$ with perfect recall of his past histories or in any state with access to his peers' histories, author $i$ eventually figures this out.


(For an overview of the literature, see, *e.g.*, [#Ceci2014;].) Thus, if female author's improve both improve their writing over time *and* persistently write more clearly then men, then [](#Theorem1) implies that female authors suffer from a subtle form of discrimination. Either referee assignment is biased in favour of men---*i.e.*, journal editors disproportionately refer female-authored papers to the toughest critics---or referees are more critical of female-authored work.



This suggests two simple empirical tests can establish if editorial standards and/or referee assignment beyond authors' control is driving the readability gap, or, instead, it's being driven by gender differences in biology/behaviour and/or knowledge about referee expectations.
The first test 

[](#Theorem1) relies on two principle identifying assumptions: (i) [](#equationX1) and [](#equationX2) accurately reflect authors' objective functions; and (ii) men and women are accurately sorted into appropriate equivalence classes. If so, then The first identifying assumption is discussed at length in [](#Robustness). For the moment, I assume it is satisfied and focus instead on Conditions (1)--(3) under conditions in which the topic, novelty and quality of a paper is independent of gender.

Thus, 
For either phenomena to cause a *persistent* readability gap, however, then female-authored papers will be disproportionately represented in academic journals.
Under these conditions, female-authored papers should enjoy higher acceptance rates, all things equal.
If they don't---*i.e.*, if Condition 1 is satisfied---then diligently addressing every referee concern has no apparent upside---acceptance rates are unaffected---and a very clear downside---constant redrafting takes time. In that case, women may indeed have a higher utlity or acceptance or simply be more risk averse
In the absence of other reason to keep readability so high, rational women re-examine initial beliefsâ¦ and then start 
When Condition 1 is satisfied, however, this behaviour cannot persist long-term. If women care more about being accepted, 
Persistent effort in the absence of any tangible reward requires systematic failure among women to efficiently allocate resources
If women's hard work solely behind long-term readability differences, however, they must be rewarded with higher acceptance rates.
For such gender differences in preferences to persist long term, however, 
 write more clearly in peer review. A gender readability gap---even one formed *during* peer review---could reasonably reflect gender differences in preferences. For example, more risk averse individuals or those with higher utility of acceptance might exert more effort (on average) to reduce their odds of being rejected. They expect to be rewarded, however, with higher acceptance rates, all else equal. As a consequence, their papers would---and, indeed *should*---be disproportionately represented in academic journals. So gender differences in preferences lead to appropriate gender differences in outcomes.
If better written female-authored papers are accepted at higher rates than more poorly written papers by otherwise equivalent male authors, then women are being rewarded for the time and effort they put into writing more clearly. The gender readability gap could reasonably reflect gender differences in the utility of acceptance. For example, women are more risk averse[see, *e.g.*,\]\[][#Borghans2009]. Consider revision rounds as lotteries. An authorâs labour reveals his willingness to pay to reduce risk, so a positive link between risk aversion and better writing implies risk averse individuals exert more effort (on average) to reduce their odds of being rejected. Under these conditions, risk aversion leads to higher acceptance rates, all else equal. So if female authors truly are more risk averse than men, their papers would---and, indeed *should*---be disproportionately represented in academic journals. 


As for Condition 1: readability gap could emerge from underlying gender differences in $u_i$: ambitious authors or those strongly averse to being rejected probably work harder to secure publications. Nevertheless, they eventually learn readability's true impact on rejection, so $i$'s unconditional probability of acceptance must be higher than $k$'s for $u_k&lt;u_i$ to produce $R_{kt}&lt;R_{it}$ indefinitely.

[1] First authors are those identified in the acknowledgements or listed first when authors are not ordered alphabetically. See [][Data] for more details.</Text>
            <Comments>When these issues are also correlated with factors that are in turn correlated with the variable of interest, they can potentially bias results.
And in any case, readability scores are perfect (or almost perfect) predictors of sentence and word length. Thus, the figures presented in this paper will always capture differences in the weighted averages of these two measures. And if women write shorter sentences and use simpler words, that's informative, on its own. See [][Alternatives] for a detailed discussion on interpreting gender differences in readability scores as actual gender differences in readability as opposed to simply gender differences in sentence and word length.
Note that readability measures more accurately capture a weighted average of vocabulary and, especially, sentence length. Measurement error is only introduced when we extrapolate from these to components and infer "readability".
It's worth nothing that none of the alternative readability measures perfectly capture "readability" either. Even the presumed gold standard---human judgement---itself suffers from a high degree of inconsistency[SOURCE]. Some research has even suggested that the cloze procedure, reading comprehension tests and even readability scores themselves are more accurate at gauging readability than humans in fact are[SOURCE].
At a bare minimum, no study (to my knowledge) has ever shown that any of the five scores used here are significantly inversely related to reading difficulty. Evidence from [#Begeny2014;] suggests the four grade-level readability scores, and particularly the SMOG and Dale-Chall scores, are more accurate for higher ability readers. (The study did not assess the Flesch Reading Ease score.)
Similarly, intelligence tests predict intelligence but studying for intelligence tests does not actually create intelligence. Implicit bias tests predict biases but learning to perform well on one is unlikely to  cause one to be less biased.
Please see [#Loughran2016;] for a detailed review of this literature.
When the co-authoring relationship is between a senior and junior colleague, the junior tends to write the manuscript while the senior puts more effort into revising it. Since both writing the original draft and revising the manuscript influence readability, co-authors in both relationships likely influence the readability of a paper. [CONFIRM REFERENCE]
The cloze randomly deletes selected words in a particular passage of text and ranks texts by readers' ability to correctly enter the missing words.
Specifically, 3,000 words understood by 80 percent of fourth-grade readers (aged 9-10).
Although several readability studies and measures already existed at the time, most relied exclusively on word lists and were developed for (and tested on) children. Interestingly, the earlist recorded attempt to quantify readability was from a group of Talmudists in 900 A.D.
The Dale-Chall formula is intended for adults and chlidren above the 4th grade reading level. The Dale-Chall formula was updated in 1995. I use this updated formula throughout this paper.
Flesch and Gunning's work had an enormous impact on journalism. Together, they helped bring down the reading grade level of front-page newspaper stories from the 16th grade level (*i.e.*, university-level) to the 11th grade level (*i.e.*, high school level).
Alternative measures using alternative criteria to estimate word difficulty and/or sentence length have also been developed. They generally fall into two camps: those that use noisier criteria for these two components (but are easier to calculate), such as the Coleman-Liau index and those that use more conventional criteria to gauge reading difficulty but are more difficult to calculate. It has been repeatedly shown, however, that after controlling for sentence length and word complexity, adding additional features of the text does not improve the predictive value of scores. Moreover, the evidence on character counts as a proxy for readability is limited, and the evidence that does exist suggests it is a much noisier indicator than syllable counts and other more sophisticated factors meant to approximate word difficulty.
[#Ardoin2005;] found that all four scores were negatively correlated with their designated proxy for reading difficulty---words read correctly per minute---although the effects in the Flesch-Kincaid and Dale-Chall scores were not significant. 
[#Beniot2017;] ask readers to compare two snippits of text and decide which is easier to read. They found that the Flesch Reading Ease ranking corresponded to the human comparison 56.8 percent of the time---*i.e.*, it performed about 6.8 percent better than chance. Note, however, that the snippits they use were very close in readability to one another. As a precondition for participating in the survey, participants had to correctly identify the more readable text between two snippits with much wider dispersion of readability scores. Thus, their lower accuracy is predominately a function of the nearness in readability in the two comparison texts. Given their setup, it is obvious that readability scores are much more accurate when comparing two different texts with a more noticable difference in readability.
Syllables per word was not originally meant to capture word difficulty, per se, but instead to capture how abstract a word was, on the assumption that the more abstract a word or idea is, the more difficult it is to comprehend. Flesch originally measured word abstraction by counting affixes. Later, he switched to syllables, which were easier to count and tended to correlate highly with the original mesure
In fact, the use of word lists to classify the difficulty of written material in a scientific manner goes back as far as 900 A.D. to a group of Talmudists. They counted the words and individual ideas so that they could know how many times each word appeard in the scroll and how frequently each word appeared in an unusual sense. Among the reasons for the elaborate counting of the Torah were clarification of unusual meaning and the devision of the reading of the weakly portions into approximately equivalent comprehension units.
Of second order importance is the probability female authors are responsible for making editorial changes during peer review.
Standard errors are generally higher for the less inclusive measure and effect sizes tend to be smaller for the more inclusive measure.
Moreover, conducting the analysis in [][Matching] would mean restricting the sample to only those women with at least three 100% female-authored papers; there are only 22, and only an additional 26 women have two. Indeed, 26% of female authors with at least three top publications have always authored with at least one man.
One could also include in that designation papers with a female first author. I therefore designate the lead author has he who was listed first in a paper in which authors were listed non-alphabetically or he was listed as the "corresponding author" or "lead author" in the acknowledgements. Unfortunately, however, this added only another 35 (0.38 percent) "female" observations.
Assuming no gender difference in acceptance rates at $t=3$ and given evidence that women are held to higher standards documented earlier, [](#figureA1) suggests---but does not prove---that manuscripts by junior female economists are disproportionately rejected.
Consistent with [](#table7), readability may actually decline during peer review. As discussed in [][NBER], this may be an artifact specific to abstracts, which are edited for length in addition to readability. Alternatively, writing (too) well upfront satisfies the review group with the highest initial readability threshold. Because referee reports reveal $s$ (and therefore $\widetilde R_i^s$), a readability decline after receiving an R&amp;R indicates that a majority of groups have laxer standards. This explanation is consistent with the theoretical model in [][SEUModel].
&lt;!--\label{Footnote132}--&gt;Alternatively, if desk rejection rates are gender neutral, authors subject to higher standards will undergo more arduous peer review. Greater scrutiny would therefore replace higher desk rejection rates when editors (or even referees) monitor and implement a policy of gender neutral acceptance rates.
Recall that the author recognises groups that reviewed his earlier papers even though the identities of the groups' individual members remain unknown to him.
Many thanks to Kevin Schnepel for suggesting this idea.
A possible exception is *Behavioral Ecology*, which increased its number of female first-authored papers after switching to double-blind review in 2001[#Budden2008a]. Whether that increase was due to bias or the universal upward trend in female authorship, however, has been somewhat controversial[#Budden2008b,Budden2008c,Webb2008,Whittaker2008].
Predictably, giving birth slows down poor review. The coefficient on motherhood, however, is consistently negative (indicating a productivity boost) and almost always highly significant when subjected to several robustness checks. This result is provocative and discussed in [][Duration]. I encourage interpreting it with caution, however, given (i) counterintuitive results; (ii) the analysis did not intend to measure motherhoodâs impact on review times; and especially (iii) only a small number of mothers with young children are published in *Econometrica*.
Readability scores are highly correlated across an article's abstract, introduction and discussion sections[#Hartley2003a]. See [][Data] for further discussion.
For a discussion on the reliability of readability formulas, see [#DuBay2004;] and [][MeasuringReadability]. A sixth commonly used measure is the Lexile Framework. Because its formula and software are proprietary, I do not include it in the analysis.
Any female behaviour that also improves acceptance rates would result in female-authored papers being disproportionately represented in academic journals. They are not[#Ceci2014]. See [][alternatives] for further discussion on this point in the context of risk aversion.
This study investigates the revision process conditional on passing initial screening, at which point most manuscripts are ultimately accepted[#Duflo2016]. These conditions resemble [#Bartos2016;]'s "lemon-dropping" market---*i.e.*, average candidates are acceptable. In contrast, initial screening by scholarly journals more likely resembles "cherry-picking" markets, in that most manuscripts are not selected. Attention and labour market discrimination are inversely related in lemon-dropping markets, only.
While 1--6 percent seems small, it is based on a single paragraph. Assuming a similar standard applies to every paragraph in a paper and improving each one takes slightly more time, the accumulated impact may be substantial.
The underlying assumption driving the result is that womenâs papers are more costly to evaluate. As discussed in [][AppendixModel], higher evaluation costs can be motivated by any of the three prominent theories of discrimination---*i.e.*, taste-based[#Becker1957], statistical[#Phelps1972,Arrow1973] or screening[#Cornell1996].
Predictably, giving birth slows down poor review. The coefficient on motherhood, however, is consistently negative (indicating a productivity boost) and almost always highly significant when subjected to several robustness checks. This result is provocative and discussed in [][Duration]. I encourage interpreting it with caution, however, given (i) counterintuitive results; (ii) the analysis did not intend to measure motherhoodâs impact on review times; and especially (iii) only a small number of mothers with young children are published in *Econometrica*.
Any female behaviour that also improves acceptance rates would result in female-authored papers being disproportionately represented in academic journals. They are not[#Ceci2014]. See [][alternatives] for further discussion on this point in the context of risk aversion.
At a bare minimum, women would not be expected to write *more* clearly with greater exposure to peer review.
This study investigates the revision process conditional on passing initial screening, at which point most manuscripts are ultimately accepted[#Duflo2016]. These conditions resemble [#Bartos2016;]'s "lemon-dropping" market---*i.e.*, average candidates are acceptable. In contrast, initial screening by scholarly journals more likely resembles "cherry-picking" markets, in that most manuscripts are not selected. Attention and labour market discrimination are inversely related in lemon-dropping markets, only.
While 1--6 percent seems small, it is based on a single paragraph. Assuming a similar standard applies to every paragraph in a paper and improving each one takes slightly more time, the accumulated impact may be substantial.
The underlying assumption driving the result is that womenâs papers are more costly to evaluate. As discussed in [][AppendixModel], higher evaluation costs can be motivated by any of the three prominent theories of discrimination---*i.e.*, taste-based[#Becker1957], statistical[#Phelps1972,Arrow1973] or screening[#Cornell1996].
Predictably, giving birth slows down poor review. The coefficient on motherhood, however, is consistently negative (indicating a productivity boost) and almost always highly significant when subjected to several robustness checks. This result is provocative and discussed in [][Duration]. I encourage interpreting it with caution, however, given (i) counterintuitive results; (ii) the analysis did not intend to measure motherhoodâs impact on review times; and especially (iii) only a small number of mothers with young children are published in *Econometrica*.
Indeed, the discrimination documented in this paper could be described by a variety of theories: LIST.
[#Bloor2008;]'s analysis considers only full-time (or maximum part-time), salaried physicians in the U.K. Similar results are found in Canada and the U.S., where physicians are paid on a per-service basis[#CICH2005,Benedetti2004].
[#Seagraves2013;] find that normal houses (*i.e.* homes not sold under special sales conditions, such as foreclosures, fixer-uppers, corporate-owned properties, transfers and estate sales) sell at a significantly higher price when listed by a female real estate agent. While the authors do find buyers pay less if they are represented by a male agent, the effect is only present for homes sold under special sales conditions. In an earlier study, [#Turnbull2007;] did not find any significant gender difference in selling performance for listing and selling agents.
The evidence on general accident rates (including non-fatal accidents) is mixed. [#McFadden1996;] found no difference in female vs. male accident rates after adjusting for pilot experience and age. [#Walton2016;] found female accident rates were *higher* among inexperienced pilots but *lower* among experienced pilots.
See also [][Discussion] where I more thoroughly address the possibility that female-authored manuscripts are systematically lower quality than male-authored papers.
These findings are not universal. [#Schubert1999;] find evidence that men are more risk averse when lotteries are framed as losses.
That is, risk averse authors thoroughly respond to referee reports---losing time but enjoying better acceptance odds; risk neutral ones donât---saving time but at greater risk of being rejected. Although it is not always true that risk averse individuals pay more to reduce risk[see, *e.g.*,\]\[][#Eeckhoudt1997,Langlais2005], this directionality is needed (on average) to claim risk aversion induces better writing in female-authored papers.
Please refer to the previous section for a more thorough review of the (substantial) prior research on gender neutrality and journals' acceptance rates. It too finds no female advantage in journals' acceptance rates.
Prior research has found peer review slightly improves the readability of articles and abstracts
Differences between men's and women's differences (columns 3 and 6) divided by men's difference.
A possible exception is *Behavioral Ecology*, which increased its number of female first-authored papers after switching to double-blind review in 2001[#Budden2008a]. Whether that increase was due to bias or the universal upward trend in female authorship, however, has been somewhat controversial[#Budden2008b,Budden2008c,Webb2008,Whittaker2008].
Any female behaviour that also improves acceptance rates would result in female-authored papers being disproportionately represented in academic journals. They are not[#Ceci2014]. See [][p3alternatives] for further discussion on this point in the context of risk aversion.
These explanations are motivated either by (i) fixed biological differences between the sexes; or (ii) misinformation about the costs of writing well. In the latter case, women update posterior believes about its benefits after successive rounds of peer review---*i.e.*, readability declines over the course of their careers. In the former, readability may decline---as women adapt to environmental expectations---or remain constant. With no benefit to better writing, however, it does not increase. See [][p3Alternatives] for a fuller discussion.
Members *within* a review group do not necessarily share these traits. For example, assume review groups are composed of two people and classified only by how well they know the author. Then every review group in which one member knew the author but the other didn't would be classified together.
That is, risk averse authors thoroughly respond to referee reports---losing time but enjoying better acceptance odds; risk neutral ones don't---saving time but at greater risk of being rejected. Although it is not always true that risk averse individuals pay more to reduce risk[see, *e.g.*,\]\[][#Eeckhoudt1997,Langlais2005], this directionality is needed (on average) to claim risk aversion induces better writing in female-authored papers. These findings are not universal. [#Schubert1999;] find evidence that men are more risk averse when lotteries are framed as losses.
These findings are not universal. [#Schubert1999;] find evidence that men are more risk averse when lotteries are framed as losses.
That is, risk averse authors thoroughly respond to referee reports---losing time but enjoying better acceptance odds; risk neutral ones don't---saving time but at greater risk of being rejected. Although it is not always true that risk averse individuals pay more to reduce risk[see, *e.g.*,\]\[][#Eeckhoudt1997,Langlais2005], this directionality is needed (on average) to claim risk aversion induces better writing in female-authored papers.</Comments>
        </Document>
        <Document ID="87E870E4-D4E5-4835-89D8-06540851AC32">
            <Title>[](#table4), alternative quality/productivity controls</Title>
            <Notes>&lt;!--\clearpage--&gt;</Notes>
        </Document>
        <Document ID="CCCB92C0-578B-44AD-B46C-C1BDC7A531C1">
            <Title>Article</Title>
        </Document>
        <Document ID="8BCB8FD2-8ED9-4A76-8887-A627F4E9F7AE">
            <Title>Proofs</Title>
            <Text>Each proof is restricted to only those results not otherwise shown in the text of the previous section. No separate proof is needed for p[](#prop3).</Text>
            <Comments>\input{$PPATH/p1/proofs/proofs.tex}</Comments>
        </Document>
        <Document ID="C68D3686-72FF-47AB-BF44-37CC33B9B14C">
            <Title>Comparing abstracts pre- and post-review.</Title>
            <Text>[](#table4) establishes a gender readability gap for abstracts published in top economics journals. [](#table5) suggests it primarily forms contemporaneously. A possible contemporaneous cause is peer review---specifically referee and/or editor demands for more revisions by female authors.
In this section, I show that peer review does indeed cause (or exacerbate) the gender readability gap. To do so, I analyse papers before and after review by comparing published articles to their draft versions. Assuming peer review is the sole gender-related factor to affect abstract readability between versions, a larger increase in women's readability relative to men's is evidence of causality.</Text>
        </Document>
        <Document ID="A1E6ECD6-F16F-40ED-B48A-7CE5AB1C41BF">
            <Title>Mean \\(\widehat R_{k3}\\) (men)</Title>
        </Document>
        <Document ID="226CD660-FC2A-44F5-8B0C-3B825C5A41D8">
            <Title>349 (alastairâs macbook's conflicted copy 2016-10-06)</Title>
            <Text>[](#p3table8) suggests the readability gap grew precisely while papers where being reviewed. FGLS estimates suggest female-authored working papers and published articles are better written. The latter's gender readability gap, however, is substantially larger. The Flesch-Kincaid, Gunning Fog and SMOG scores imply immediate peer review accounts for 60--70 percent of the gap; the Flesch Reading Ease and Dale-Chall scores indicate a smaller impact (30--40 percent). OLS estimates on the change in score draw similar conclusions.
Assuming non-peer review factors are independent of its timing---discussed in [][p3Alternatives]---the significant increase in the readability gap *ex post* establishes the desired causal link.
</Text>
        </Document>
        <Document ID="B574767E-5F2C-433A-8F56-2F2577254B6C">
            <Title>Double-blind review</Title>
            <Text>In an earlier version of this paper[#Hengel2015], I compared readability gaps in published papers subjected to blind (double-blind review before the internet) and non-blind review (single-blind review or double-blind review after the internet). Blind review appeared to exacerbate the gender readability gap.
These findings, however, were not robust to including fixed effects for year of publication. [](#tableC6b) repeats the analysis from [\]\[Table 3.9 (first panel), p. 65][#Hengel2015;] including these effects. Figures reflect the marginal effect of female ratio in non-blind ($\beta_{1P}$) and blind ($\beta_{1P}+\beta_{3P}$) review from OLS estimation of [](#equationC6b).
&lt;!--\input{$HOME/Dropbox/Readability/draft/equations/equationC6b}--&gt;
 The gender readability gap is positive when papers are not blindly evaluated in peer review---although significant or weakly significant for only three out of five scores. Under blinded review, the gender gap is negative for three scores and positive for two; none, however, are significantly different from zero. Difference-in-differences are generally positive---indicating (in contrast to results in[#Hengel2015;] but consistent with results in [](#table8)) that the readability gap declined during blinded peer review---but standard errors are large relative to the size of the effect.
&lt;!--\input{$HOME/Dropbox/Readability/draft/tex/generated/tableC6b}--&gt;
[](#table8) suggests double-blind review may have successfully reduced peer review's impact on the gender readability gap *before* the internet. Unfortunately, it has been less effective *after* the internet. I dropped observations subjected to double-blind review pre-internet and re-estimated [](#equation3a), but with $\text{Blind}_j$ equal to 1 if an article $j$ was subjected to an official policy of double-blind review after the internet. The results, presented in [](#tableC6a), suggests a positive gender readability gap in both samples.
&lt;!--\input{$HOME/Dropbox/Readability/draft/tex/generated/tableC6a}--&gt;
</Text>
        </Document>
        <Document ID="63CB6D77-211B-489C-AE1C-02E856B6D37A">
            <Title>Revisions</Title>
        </Document>
        <Document ID="01E508C0-92C1-4247-B68E-F8C7E7201694">
            <Title>[](#table11), alternative measures of an article's "gender"</Title>
            <Text>[](#table11XA), [](#table11XB) and [](#table11XC) repeat the analysis shown in [](#table11) using three alternative measures of an article's "gender". In [](#table11XA), papers authored entirely by women are compared to papers authored entirely by men. In [](#table11XB), papers are considered "female" if at least one author is female. In [](#table11XC), article gender is represented by a binary variable equal to one if at least half of all authors are female; articles below that threshold with at least one female author are excluded.
&lt;!--



\clearpage--&gt;
</Text>
        </Document>
        <Document ID="E3D6BD99-A9C6-4E73-B83C-5854E795B7EB">
            <Title>Time.</Title>
            <Text>Receivership-like legislation may succumb to lengthy appeals or other delay tactics that restrain creditor action in the event of default. If so, it differs little from traditional bankruptcy. Thus, a final prerequisite is that the procedures themselves not take too much time.
Receivership permits prior contracting of control rights. By design, it should quickly transfer them in the event of default irrespective of the host country's level of development or institutional strength. When it doesn't, bankruptcy involves a significant period during which the firm continues operating under judicial supervision---*i.e.*, reorganisation. Creditors take this into account when issuing and pricing loans.
I define the threshold as two years or less based on Doing Business's "time required to recover debt" indicator. This indicator measures the number of years lenders need to recover their credit from a defaulting firm; it is based on responses to a hypothetical scenario involving a financially distressed hotel unable to make payments on a bank loan collateralised by its entire business.
The nine countries that possess the legislative and time components of receivership are: Albania (2007, 2013), Barbados (2010), Belize (2010), Fiji (2009), Ghana (2013), Jamaica (2010), Montenegro (2013), Nigeria (2007, 2014) and St. Lucia (2010). Each country, the years in which it was surveyed and whether it employed receivership or not are listed in [](#p2table5).</Text>
            <Comments>Even if the firm is shut down during this period, the fact that creditors are delayed in liquidating its assets, selling it to a third party or even operating it themselves reduces the value of their claim in a manner analogous to reorganisation.
If a country records no instance of an official debt enforcement procedure within the previous five years, Doing Business  assumes debt is never recovered (time to recover debt is recorded as "no practice"). Correspondingly, I assume these countries do not meet the two-year threshold.
\input{/Users/erinhengel/Dropbox/Thesis/p2/tables/tex/p2table5}</Comments>
        </Document>
        <Document ID="325BC625-9B21-4A5F-A4EE-2544A74B6BCB">
            <Title>Solvency</Title>
            <Text>Presume first $D\le X_1$; the project is solvent. The entrepreneur decides whether to operate another period or liquidate. Should he continue, the project generates a second period's cash flow, $X_2$, after which it comes to the natural end of its life. It is shut down and the machine sold for $K_2$. Gross project value is
$$V_2^C=X_1+X_2+K_2.$$
Should he liquidate, all service and employment contracts are voided, business operations cease and the machine is sold for its full market value. Gross project value is instead
$$V_1^L=X_1+K_0.$$
In both scenarios, the creditor is repaid in full; his returns are $D$. The entrepreneur keeps what's left: $E_2^C=V_2^C-D$ if the project is continued and $E_1^L=V_1^L-D$ if it's liquidated.</Text>
        </Document>
        <Document ID="8D97AAB2-10CC-429C-9080-177F0B18C0FD">
            <Title>[][NBER], suplemental output</Title>
        </Document>
        <Document ID="FEECFE95-BE8C-47D4-9AFF-717E472573AE">
            <Title>Rescue Culture</Title>
            <Synopsis>In a comprehensive debt-financing model, I show ``rescue''-centric bankruptcy anticipates motives creditors do not have. Although out-of-court settlements dominate and formal procedures are rarely used, insolvent borrowers nevertheless extract concessions from creditors unless supervised reorganisation is 100 percent free. They pay for it, of course, with inefficient continuation and liquidation, expensive debt and credit-rationing. Ironically, rescue culture hits hardest the firms it purports to save yet benefits creditors, whom it claims to check. Firms with long expected lifespans are either the first denied credit or the first to make investment choices that guarantee premature liquidation. Creditors, on the other hand, enjoy a profitable niche lending market in an otherwise perfectly competitive industry.</Synopsis>
        </Document>
        <Document ID="15013BDF-CC07-41E3-9FFA-FD763B85A9C2">
            <Title>Untitled</Title>
        </Document>
        <Document ID="05289442-8D6A-45D1-9417-0B6549C0EDB2">
            <Title>Double-blind review</Title>
            <Text>Gender bias is possible only when authors' identity is known or can be reasonably guessed. Two factors make that feasible: non-blind review and Google. Do either exacerbate the readability gap between male- and female-authored abstracts? No and Yes. While the gap has grown with search engines' popularity and accuracy, double-blind review does not seem to dampen it. And it may make things worse.
Two journals---*QJE* and *AER*---employed double-blind review at some point during the time period covered by the data. *AER*\'s spell began 1 July, 1989 and ended 1 July, 2011. *QJE* used double-blind procedures until 1 June, 2005. *Econometrica*, *JPE* and *AER Papers &amp; Proceedings* have never blinded referees to papers' authors.
The first panel in [](#p3table9b) displays marginal effects of double-blind review by gender from [](#eq:p3eq5):where $\text{double-blind review}_j$ equals 1 if article $j$ was evaluated under double-blind refereeing and $\text{era}_j\text{ effect}$ splits data into three time periods: 1950--1969, 1970--1989 and 1990--2015. Male effects for male authors co-authoring with no females $(\beta_2)$; female effects for female authors co-authoring with no males $(\beta_2+\beta_3)$.
In the first panel, switching to double-blind review does not significantly affect the readability of female-authored abstracts. For whatever reason, however, it does affect men. When blindly evaluated, male-authored abstracts are up to !!DoubleBlindMan percent less clear. The effect is weakly significant for two out of five scores.
The last row of panel one displays gender differences. Because men write slightly sloppier under double-blind review but women do not, all statistics agree that a double-blind refereeing policy exacerbates the readability gap; three are significant.
The second panel of [](#p3table9b) re-estimates [](#eq:p3eq5) with primary *JEL* effects on *AER* abstracts published on or after 1990. The data therefore cover a single journal during periods of both double- and single-blind review. Figures support conclusions drawn from the first panel. Again, all readability scores---four of which are significant---confirm double-blind review exacerbates gender differences. On the source of that difference, four agree that male-authored abstracts are less readable (the Dale-Chall score suggests the opposite) while all five newly affirm female-authored abstracts are more readable when referees are blinded to authors' identities.
Gender differences in writing standards grow under double-blind review. This result dovetails with a finding in[#Blank1991;]. After a two-year controlled experiment whereby manuscripts submitted to *AER* were randomly subjected to either single- or double-blind review she found referees under the latter system rate female-authored articles more harshly. Although her result lacks significance it suggests referees are easier on women's papers as long as they know authors' gender. By one logical extension, female-authored manuscripts deserve more criticism because they are poorer quality.
A combination of lower quality research and less scrutiny of papers written by women may drive significance here. If women's manuscripts are not as good, referees, blinded or not, *should* peruse them more carefully---a byproduct of which could be better written papers after-the-fact or more attractive prose compensating for structural weaknesses before it. That only the blinded referees do might explain why they are harsher on male-authored manuscripts but easier on female-authored ones when gender is definitely known.
But I'll leave that question to physical scientists and instead offer an alternative explanation. I mean, who wants to believe biological features over which one has no control determine whether research is good or not? Not I, said the woman!
When blinded to authors' identities, reviewers may be more susceptible to "snap judgements" based on gender stereotypes than when they aren't. Referees that know authors' identities have a lot more information at their disposal when reviewing a paper---previous publications, current employment, education, etc. They also have the freedom to query colleagues on reputation, work ethic and general competency. All of this information impacts how papers are assessed---for example, "the author's identity might signal to a referee whether it is necessary to double-check technical details in a paper with a great deal of mathematical analysis"[\]\[p. 1042][#Blank1991].
Double-blind review makes overt information-gathering illegal but it does not stop referees from guessing authors' identities---which they did with surprising accuracy before the internet[#Blank1991], and presumably perfect accuracy after it---meaning superficial qualities are known but not complemented by more reliable indicators of reputation. Assuming referees are about as evolved as the rest of humanity, they underrate women and overrate men without extra evidence of the former's competency[#Foschi2000]. The end result may be harsher referee evaluations---and consequently better written text---of female-authored articles under double-blind review.</Text>
            <Comments>Specifically, single-blind review (the author is blinded to the referee's identity but not the reverse) or review under which the identities of both parties are mutually known.
&lt;!--\label{fnm:p3fn32}--&gt;From the beginning of May 1987 to the end of May 1989 *AER* subjected half of submitted papers to double-blind review, half to single-blind review[see\]\[][#Blank1991]. Because I do not know the process accepted manuscripts were reviewed under, all estimation results in this section omit these data.
*Econometrica* and *JPE* have always employed single-blind refereeing. Papers are chosen for *AER Papers &amp; Proceedings* by *AER*\'s president and/or American Economic Association committees. Neither are blinded to authors' identities.
\input{$P3/equations/eqn5.tex}
$\text{double-blind review}_j$ is 1 for articles published during an official policy of double-blind review. A final publication date, however, may substantially lag the actual review date[for an illustration and discussion, see\]\[][#Blank1991]. Because only *AER* articles published post May 1989 are included in the analysis (see &lt;!--\autoref{fnm:p3fn32}--&gt;) and all *QJE* articles published before June 2005 were submitted to double-blind review, the number of articles subjected to non-blind review incorrectly classified as having undergone blind review should be small. The second panel of [](#p3table9b) further limits misclassification by including only *AER* articles published post 1990.
Year effects were replaced due to multicollinearity between $\text{double-blind review}_j$ and the combination of year and journal dummies.
\input{$P3/tables/tex/table9b.tex}
Unless journals have an explicit policy of publishing female-authored research over male work of equal quality---which may be the case---because all prior evidence points to gender neutral *acceptance rates* and there appears to be no gender difference in citations per article except in Norway[#Ceci2014,Aksnes2011], women's research is probably just as good as men's.
I use "snap judgement" to refer to quick decisions made without deep investigation or logical deliberation. It is akin to choices made with "System 1" in[#Kahneman2011;].</Comments>
        </Document>
        <Document ID="0B4EE5DC-9826-447C-83FE-33CBE62B4D6C">
            <Title>Exclusively female-authored</Title>
            <Text>In the following tables, papers are considered "female" if at least one author is female.
&lt;!--\input{$HOME/Dropbox/Readability/draft/tex/generated/tableC4a}--&gt;
&lt;!--\input{$HOME/Dropbox/Readability/draft/tex/generated/tableC5c}--&gt;
&lt;!--\input{$HOME/Dropbox/Readability/draft/tex/generated/table8XC}--&gt;
&lt;!--\input{$HOME/Dropbox/Readability/draft/tex/generated/table11XA}--&gt;
&lt;!--\input{$HOME/Dropbox/Readability/draft/tex/generated/tableXA}--&gt;
&lt;!--\clearpage--&gt;</Text>
        </Document>
        <Document ID="E4E41B8C-CCB7-4324-AD59-21BB8D756BBE">
            <Title>[][SEUMatching], Mahalanobis matching</Title>
            <Text>[](#table9XA), [](#table10XA) and [](#figure4XA) replicate the analysis in [][SEUMatching] but instead of using propensity score matching to generate matched pairs, it uses a Mahalanobis matching procedure. The names of the economists in each matched pair are displayed in [](#tableC14XA). The variables used to match authors are identical to those used for the matches based on propensity scores, outlined in [][SEUMatching].
&lt;!--\input{$HOME/Dropbox/Readability/draft/tex/generated/table9XA}--&gt;
&lt;!--\input{$HOME/Dropbox/Readability/draft/tex/generated/table10XA}--&gt;
&lt;!--\input{$HOME/Dropbox/Readability/draft/tex/fixed/figure4XA}--&gt;
&lt;!--\input{$HOME/Dropbox/Readability/draft/tex/generated/tableC14XA}--&gt;
&lt;!--\clearpage--&gt;</Text>
        </Document>
        <Document ID="B3E6EB0F-D83F-4D2A-878C-6177EF23C8E5">
            <Title>[](#table11NEWXX), including Condition 1</Title>
            <Text>To remain consistent with a similar regression in [](#table11), [](#table6) does not include author productivity effects. [](#tableD12) reproduces [](#table6) including these effects. Coefficients and standard errors vary little from those in [](#table6).
&lt;!--\clearpage--&gt;</Text>
        </Document>
        <Document ID="75C4F160-4DF9-41FD-9C13-DA0A6A57037F">
            <Title>Strategic default</Title>
            <Text>In this section, I illustrate that strategic default does not affect lending when reorganisation is costless. In a minority of cases, judicial error may lead to premature liquidation.
Let $Y=0$. Absconding with more than $X_1$ requires surreptitious liquidation that is a serious breech of contract and  probably constitutes fraud. I assume its punishment is enough to deter it entirely. Additionally, $X_1&lt;D$ triggers insolvency and if the creditor desires, court supervision implied by bankruptcy. Thus, strategic default is possible only up to $D\le X_1$.
Consider non-viable firms. Since $Y=0$, $\ol V_1^C&lt;\ol V_1^B$; per [](#prop1), insolvency implies liquidation or bankruptcy. For all $D\le X_1$, however, $\ol E_1^B&lt;E_1^L$. The firm does not strategically default.
Consider now viable firms. As discussed in [][p1Insolvency], for some $X_1$ entrepreneurs are better off in a workout than they would be continuing while solvent---creating motive for strategic default.  Nevertheless, creditors expect at least $K_0$ even in reorganisation when $X_1$ is naught. Since reorganisation is by definition court-supervised and the judiciary assumed ethical, worst-case-scenario creditors turn to it for future oversight. Regardless, viable firms always get credit. If bankruptcy guarantees reorganisation when creditors favour it or if entrepreneurs cannot steal time 2 earnings, firms efficiently continue. Otherwise, judicial error may cause prematurely liquidating a minority of them.</Text>
            <Comments>In an alternative scenario, the entrepreneur could steal all of $X_1$. Using analogous arguments, non-viable firms always obtain credit, whereas viable firms did anyway per [](#prop1)(ii).
$Y=0$ implies no corruption, *i.e.*, an ethical judiciary.</Comments>
        </Document>
        <Document ID="DDB754E7-DD00-4DA3-82D3-4534B0CD8CA0">
            <Title>Proofs</Title>
            <Text>Only [](#prop5) is proved. [](#cor3) follows directly from [](#prop4).
&lt;!--\input{$PPATH/p2/proofs/proofs}--&gt;</Text>
        </Document>
        <Document ID="ADCA6B09-EEDD-4F90-A5FB-F7910D66AD57">
            <Title>Settlement.</Title>
            <Text>Bankruptcy isn't the only option. Creditors and entrepreneurs can always deal with insolvency on their own. One scenario involves a mutual decision to wind-up business operations. In another, both parties agree to a workout.
Monetary or in-kind transfers between borrower and lender are common components of workout agreements; in liquidation, however, they are usually forbidden. Paying a director to voluntarily wind up his insolvent firm qualifies as a "preference payment"---*i.e.*, a payment that "has the effect of putting [its recipient] into a position which, in the event of the company going into insolvent liquidation, will be better than the position he would have been in if that thing had not been done"[\]\[Â§239 (4)(b)][#GreatBritain1986]. Preference payments are not allowed in either the U.S. or the U.K.
Voluntarily liquidating an insolvent firm differs very little from compulsory liquidation. The process is overseen by a third-party who acts in the creditors' interests. One of his tasks is to inspect the firm's financial records and recover preference payments---which I assume he does efficiently and accurately. Without preference payments, neither side has the power to "bribe" the other to wind up operations; combined with the earlier assumption that liquidation does not erode project value, the process and proceeds are identical to those in compulsory liquidation.
In a workout, the bankrupt debtor negotiates a revised debt contract with his creditors outside the judicial system. Workouts, when allowed, follow a similar script. In the U.S., managers propose a plan to restructure the debt; creditors then vote on it. If unanimously accepted, the plan is implemented without requiring court intervention. Time spent in bankruptcy is reduced or eliminated, making them cheaper and less stressful than formal reorganisation[#McConnell1991]. I assume they are costless.
The debt renegotiation game is very simple; I assume the entrepreneur makes a take-it-or-leave-it offer to replace the original debt $D$ due at time 1 with a new one $\wt D$ due at time 2. If accepted, projects operate another period; gross earnings are identical to $V_2^C$. Under these circumstances, the entrepreneur offers the smallest $\wt D$ the creditor will accept: one which equates the latter's expected earnings in a workout, $\ol C_1^W$, with those of his outside option---bankruptcy ([](#lem2)).
LEMMA
In a workout, the entrepreneur offers the creditor the smallest $\wt D$ such that $\ol C_1^B=\ol C_1^W$. Such a $\wt D$ exists if and only if $\,\ol C_1^B\le\ol V_1^C$.
elem
Assuming his proposal is accepted, the entrepreneur's expected earnings from a workout are
$$\ol E_1^W=\ol V_1^C-\ol C_1^B.$$
 Let $\ol E_1^C$ be the time 1 expected value of $E_2^C$. Since $\ol C_1^B$ is only ever at most $D$, $\ol E_1^C\le\ol E_1^W$ for values of $X_1$ within a sufficiently small neighbourhood of $D$. The upshot? Entrepreneurs are better off in a workout than they would be if continuing while solvent---armed with the threat of bankruptcy, they demand revised terms of credit at lenders' expense.</Text>
            <Comments>Preference payments include most transfers to company directors (and connected persons) one (U.S.) or two (U.K.) years prior to insolvency.
For a discussion on voidable preferences in the U.K., see [\]\[][#Hill2014;]. In the U.S., paying directors to liquidate may also fall under[\]\[Â§152(6)][#US18;]---otherwise know as the "bankruptcy bribery" statue. It states that "a person who knowingly and fraudulently gives, offers, receives or attempts to obtain any money or property, remuneration, compensation, reward, advantage or promise thereof for acting or forbearing to act in the case under title 11; [...] shall be fined under this title, imprisoned not more than 5 years, or both". (See also [\]\[Â§727(a)(4)(C)][#US11;].)
In the U.K., all liquidations, regardless of solvency, are overseen by a liquidator with this responsibility. In the U.S., a liquidator is appointed only if the firm undergoes Chapter 7 liquidation or state-governed "Assignment for the Benefit of Creditors" (ABC) procedures. Yet, even if the borrower and creditor negotiate a settlement and liquidate assets outside official procedures, creditors may nevertheless recover preferences in bankruptcy court after concluding the sale. (For a discussion of this issue specific to ABC procedures---a generally weaker mechanism for recovering preferences---see[#Thorne2007;].)
Although altering the financial terms of a debt contract outside bankruptcy requires the unanimous consent of creditors[#UnitedStates1939], if accepted by at least a supra-majority, the firm can file for a pre-pack bankruptcy. Pre-pack bankruptcies fast-track approval of (or approve by default) workout agreements supported by a certain majority of creditors in each class.
&lt;!--\label{p1footnote3}--&gt;This assumption effectively grants the entrepreneur all bargaining power in bankruptcy. It is made for tractability--- without further constraints, multiple equilibria are possible. As shown in [][p1AppendixBargainingPower], however, as long as entrepreneurs extract some surplus during debt renegotiations, all conclusions in this paper hold. Additionally, granting the entrepreneur full power is most consistent with the original motivation of rescue culture. It is also very likely to hold for viable firms---*i.e.*, those that had originally planned to operate two periods and more susceptible to the stigma of failure and side effects of sudden unemployment[see, *e.g.*,\]\[][#Linn1985,Fay2002].
This discrepancy creates motive for strategic default [see\]\[][#Hart1998,Bolton1990]. As shown in [][p1appendixstrategicdefault], however, strategic default is only harmful when coupled with expensive reorganisation. Otherwise, it has no effect on the lending market and probably no effect on interim investment decisions.</Comments>
        </Document>
        <Document ID="908E044B-E27B-4AEC-934D-FE4BF8C2FC1D">
            <Title>[](#table10), male effects</Title>
            <Text> [](#tableC17) shows $\widehat R_{k3}$ for men in the matched sample. Grade-level effects (Flesch-Kincaid, Gunning Fog, SMOG and Dale-Chall) have been multiplied by [negative one][MeasuringReadability].
&lt;!--\input{$HOME/Dropbox/Readability/draft/tex/generated/tableC17}</Text>
        </Document>
        <Document ID="46171C41-FA25-46F1-A4C4-0EAC31F84F64">
            <Title>Author-level analysis</Title>
            <Text>I next analyse readability at the author-level. To disaggregate the data, each article is duplicated $N_j$ times, where $N_j$ is article $j$'s number of co-authors; observation $j_k\in\{1,\ldots,N_j\}$ is assigned article $j$'s $k\text{th}$ author. I then estimate the dynamic panel model in [](#equation1):
&lt;!--\input{$HOME/Dropbox/Readability/draft/equations/equation1}--&gt;
 $R_{j_{it}}$ is the readability score for article $j$---author $i$'s $t$th publication; $R_{it-1}$ is the corresponding value of author $i$'s $t-1$th paper. Gender enters twice---the binary variable $\text{male}_i$ and $\text{female ratio}_j$---to account for author $i$'s sex and the sex of his co-authors, respectively. $\vect X_j$ is a vector of observable controls. It includes: editor, journal, year, journal $\times$ year, institution and English fluency dummies; quality controls---citation count and $\text{max. }T_j$ fixed effects; and $N_j$ to account for author $i$'s proportional contribution to paper $j$. $\alpha_i$ are author-specific effects and $\vep_{it}$ is an idiosyncratic error. $\alpha_i$ are eliminated by first-differencing; endogeneity in the lagged dependant variable is instrumented with earlier lags[#Arellano1995,Blundell1998]. To account for duplicate articles, the regression is weighted by $1/N_j$. Standard errors are adjusted for two-way clustering on editor and author.
&lt;!--\input{$HOME/Dropbox/Readability/draft/tex/generated/table5}--&gt;
[](#table5) displays results. Rows one and two present contemporaneous marginal effects on co-authoring with women for female ($\beta_1$) and male ($\beta_1+\beta_2$) authors, respectively. Both estimates are positive---everyone writes more clearly when collaborating with women. Marginal effects for women are highly significant and at least twice as large as those in [](#table4)---women write 2--6 percent better than men. When men write with women, however, marginal effects are smaller and less precise.
Men and women co-authoring together experience an identical rise (or fall) in readability, so the effect for one should mirror the other. Yet, [](#table5) suggests they don't. While the interaction terms ($\beta_2$) are insignificant---*i.e.*, the observed disparity is plausibly due to chance---the difference may reveal an increasing, convex relationship between female ratio and readability. Men's smaller effect potentially reflects their disproportionate tendency to co-author exclusively with other men---precisely where the marginal impact of an additional woman is low.
Tests for serial correlation indicate no model misspecification. Coefficients on the lagged dependant variables are small, suggesting readability is mostly determined contemporaneously. Nevertheless, their uniform positivity and significance indicate modest persistence.</Text>
            <Comments>Assigning equal weight to all observations results in quantitatively and qualitatively similar results[see\]\[pp. 44--45][#Hengel2016].
Quotient of $\beta_1$ divided by the total effect for men co-authoring with no women (female ratio of zero) estimated at other co-variatesâ observed values (see [][AppendixAuthorMale]).
On average, the female ratio for men is 0.04 (0.05 excluding solo-authored papers). When excluding articles written entirely by men, their average ratio is still only 0.39. By default, women always author with at least one woman---themselves; the average female ratio of their papers is 0.6 (0.46 and 0.53 excluding articles written entirely by women and solo-authored papers, respectively).</Comments>
        </Document>
        <Document ID="959D7027-8576-46E3-A7A1-19AD065607C6">
            <Title>Alternative measures of an article's "gender"</Title>
            <Text>The following tables repeat the principal analyses from the paper using several alternative measures of of an article's "gender".</Text>
        </Document>
        <Document ID="F9E2FC29-2F12-4D32-8B72-A2BECBF61F58">
            <Title>Meta-Data</Title>
            <Text>The wider gap post-peer review suggests causality with peer review. In this section, I investigate if and how higher standards for women are a contributory factor. To help organise the discussion, I develop a stylised model of readability's marginal impact on the editorial decision making process.</Text>
        </Document>
        <Document ID="64F0F44E-6C57-4044-BED3-AEFDC3A4AD3F">
            <Title>[](#table8), alternative measures of an article's "gender"</Title>
            <Text>[](#table8XA), [](#table8XB) and [](#table8XC) repeat the analysis shown in [](#table8) using three alternative measures of an article's "gender". In [](#table8XA), papers authored entirely by women are compared to papers authored entirely by men. In [](#table8XB), papers are considered "female" if at least one author is female. In [](#table8XC), article gender is represented by a binary variable equal to one if at least half of all authors are female; articles below that threshold with at least one female author are excluded.
&lt;!--


\clearpage--&gt;</Text>
        </Document>
        <Document ID="6B416027-752E-4EE0-AADB-3EEA6C218A10">
            <Title>Controls</Title>
            <Text>For every article I recorded authors' institutional affiliations. Individual universities in U.S. State University Systems were coded separately (*e.g.*, UCLA and UC Berkeley) but think tanks and research organisations operating under the umbrella of a single university were grouped together with that university (*e.g.*, the Cowles Foundation and Yale University). Institutions linked to multiple universities are coded as separate entities (*e.g.*, Ãcole des hautes Ã©tudes en sciences sociales).
In total, 1,039 different institutions were identified. I create 64 dummy variables, each of which represents one or more institution(s); groupings reflect counts of distinct articles in which an institution was listed as an affiliation. Specifically, institutions listed in 59 or fewer articles were grouped in bins of 10 to form six dummy variables: the 751 institutions mentioned in 0--9 articles were grouped to form the first dummy variable, the 92 mentioned in 10--19 articles were grouped to form the second, *etc.* Fifty-eight institutions were affiliated with 60 or more articles; each is assigned its own dummy variable. When multiple institutions are associated with an observation, only the dummy variable with the highest-rank is used, *i.e.*, the highest-ranked institution per author when data is analysed at the author-level and the highest-ranked institution for all authors when data is analysed at the article-level.
I control for article quality and author productivity in several ways. First, I use article citations from the [Web of Science](https://login.webofknowledge.com/error/Error?Error=IPError&amp;PathInfo=%2F&amp;RouterURL=https%3A%2F%2Fwww.webofknowledge.com%2F&amp;Domain=.webofknowledge.com&amp;Src=IP&amp;Alias=WOK5) database. Second, I generate 30 dummy variables that group authors by career-total publication counts in the four journals. For example, Daron Acemoglu and Jean Tirole form one group (each published 45 articles as of December 2015); Alvin Roth, Elhanan Helpman and Gene Grossman form another (27 articles). In [][NBER] and [][Duration], I additionally control for the number of prior top-four papers (at time of publication). For co-authored articles, only the data corresponding to the most prolific author is used.
To account for English fluency, most regressions include a dummy variable equal to one if an article is co-authored by at least one native (or almost native) English speaker. I assume an author is "native" if he: (i) was raised in an English-speaking country; (ii) obtained all post-secondary eduction from English speaking institutions; or (iii) spoke with no discernible (to me) non-native accent. This information was almost always found---by me or a research assistant---in authors' CVs, websites, Wikipedia articles, faculty bios or obituaries. In the few instances where the criteria were ambiguously satisfied---or no information was available---I asked friends and colleagues of the author or inferred English fluency from the author's first name, country of residence or surname (in that order).
I create dummy variables corresponding to the 20 primary and over 700 tertiary *JEL* categories to control for subject matter. The *JEL* system was significantly revised in 1990; because exact mapping from one system to another is not possible, I collected these data only for articles published post-reform---about 60 percent of the dataset. Codes were recorded whenever found in the text of an article or on the websites where bibliographic information was scraped. Remaining articles were classified using codes from the American Economic Association's Econlit database.
To control for editorial policy, I recorded editor/editorial board member names from issue mastheads. *AER* and *Econometrica* employ an individual to oversee policy. *JPE* and *QJE* do not generally name one lead editor and instead rely on boards composed of four to five faculty members at the University of Chicago and Harvard, respectively. Editor controls are based on distinct lead editor/editorial boards---*i.e.*, they differ by at least one member. In total, 74 groups are formed in this manner.
The matching exercise in [][SEUMatching] pairs authors using various factors, including their fraction of first-authored papers. First authors are those identified in the acknowledgements or listed first when authors are not ordered alphabetically.
To control for motherhood's impact on revision times, I recorded children's birth years for women with at least one 100 percent female-authored paper in *Econometrica*. I personally (and, I apologise, rather unsettlingly) gleaned this information from published profiles, CVs, acknowledgements, Wikipedia, personal websites, Facebook pages, [intelius.com](https://www.intelius.com) background checks and local school district/popular extra-curricular activity websites. Exact years were recorded whenever found; otherwise, they were approximated by subtracting a child's actual or estimated age from the date the source material was posted online. If an exhaustive search turned up no reference to children, I assumed the woman in question did not have any.</Text>
            <Comments>[#Blank1991;] ranks institutions by National Academy of Science departmental rankings. Those and similar official rankings are based largely on the number of papers published in the journals analysed here.
&lt;!--\label{fn7}--&gt;This quality/productivity control has several limitations: (i) it relies on publication counts---not necessarily an accurate measure of "quality"; (ii) it discounts current junior economists' productivity; and (iii) it generates somewhat inconsistent groupings---for example, two authors have published 45 articles, but only one author has published 37 (Andrei Shleifer).
In [\]\[p. 42 and p. 44][#Hengel2016;], I experiment with another measure of quality---the order an article appeared in an issue. It has no noticeable impact on the coefficient of interest or its standard error.
Non-native speakers who meet this criteria have been continuously exposed to spoken and written English since age 18. This continuous exposure likely means they write as well as native English speakers. To qualify as an English speaking institution, all courses---not just the course studied by an author---must be primarily taught in English. *E.g.*, McGill University is classified as English-speaking; University of Bonn is not (although most of its graduate economics instruction is in English).
I also conducted a primitive surname analysis [see\]\[pp. 35--36][#Hengel2016]. It suggests that the female authors in my data are no more or less likely to be native English speakers.
In recent years, *JPE* has been published under the aegis of a lead editor.
While the information I found was publicly available, I apologise for the obvious intrusion.
In several instances, I obtained this information from acquaintances, friends and colleagues or by asking the woman directly. Given its sensitive nature, children's birth years are not currently available on my website (unlike other data in this paper).</Comments>
        </Document>
        <Document ID="3FC7E572-E16E-49F4-8CD6-0F3C88A18D78">
            <Title>Untitled</Title>
        </Document>
        <Document ID="13474993-1A32-496D-A5D5-6E852669517B">
            <Title>Measuring discrimination</Title>
            <Text>[](#Theorem1)'s three conditions confirm the presence of discrimination. They principally rely on two identifying assumptions: (i) $i$ and $k$ are equivalent; (ii) $t'$ is sufficiently large---*i.e.*, any errors in $i$'s beliefs about $\widetilde r_{0i}$ and $\widetilde R_i$ are on a path converging to zero. By assuming a more specific belief structure at $t'$, [](#Corollary1) proposes a conservative measure of discrimination's impact on readability choices.
When making revisions, authors choose $R_{it}$ to maximise [](#equation8). As shown in [][AppendixProofs], $R_i^\star\le r_{0it}$ where $R_i^\star$ is the $R$ that solves $\phi_i'(R)=c_i'(R)$. Since $R_i^\star$ is $i$'s optimal readability in the absence of peer review and $R_i^\star\le r_{0it}$, $i$ prefers $R_{it}&gt;r_{0it}$ only if $r_{0it}&lt;\widetilde R_i^s + e_{1it}^s$, where $e_{1it}^s$ is his time $t$ error in beliefs about $\widetilde R_i^s$. So $i$ revises only when required---and even then, no more than a comfortable minimum to placate referees.
A similar logic governs $i$'s choice of $r_{0it}$---now picked to maximise [](#equation9). $i$ opts for $r_{0it}&gt;R_i^\star$ only if $R_i^\star&lt;\widetilde r_{0i}^s+e_{0it}^s$ for at least one $s$ in $\Sigma_{A_{it}}$, where $e_{0it}^s$ is the time $t$ error in $i$'s beliefs about $\widetilde r_{0i}^s$. Thus
&lt;!--\input{$HOME/Dropbox/Readability/draft/equations/equation10}--&gt;
 where $\overline s$ is the review group in $\Sigma_{A_{it}}$ for which $i$ believes $\widetilde r_{0i}^s$ is highest---*i.e.*, $\overline s\in\Sigma_{A_{it}}$ satisfies $\widetilde r_{0i}^s+e_{0it}^s\le\widetilde r_{0i}^{\overline s}+e_{0it}^{\overline s}$ for all $s\in\Sigma_{A_{it}}$.
Define $\delta_{0ik}^s$ and $\delta_{1ik}^s$ as the difference in readability standards applied to authors $i$ and $k$ by review group $s$ in time $t$ at stage 0 and 1, respectively: $$\delta_{0ik}^s\equiv\widetilde r_{0i}^s-\widetilde r_{0k}^s\quad\text{and}\quad\delta_{1ik}^s\equiv\widetilde R_i^s-\widetilde R_k^s.$$ When $\delta_{0ik}^s\ne0$ and/or $\delta_{1ik}^s\ne0$, $s$ employs asymmetric evaluation criteria to $i$ and $k$'s work. Dissimilar authors may call for asymmetric benchmarks---but if $i$ and $k$ are equivalent, they're a form of discrimination. Unfortunately, $\widetilde r_{0i}^s$ and $\widetilde R_i^s$ are not known to the researcher and $R_{it}$ inconsistently estimates [them](#equation10). As [](#Corollary1) shows, however, $R_{it}-R_{kt}$ is *smaller* in magnitude than the true value of stage 1 discrimination by $s$ or stage 0 discrimination by $\overline s$.
&lt;!--\input{$HOME/Dropbox/Readability/draft/theorems/corollary}--&gt;
[](#Corollary1) identifies a conservative measure of discrimination's impact on $i$'s readability. It also exposes the toxic denouement of one biased $s$. $i$'s time $t$ readability choice depends on discrimination at stage 1 by the group of referees that actually reviewed his paper ($s$) as well as discrimination at stage 0 by another review group that (probably) didn't ($\overline s$).
Such is the first externality from even one rotten apple. From $i$'s perspective, $\overline s$ spoils the bunch. Bias from $\overline s$ destabilises $s$'s attempt to treat $i$ and $k$ fairly. Either $i$ is rejected when assigned to $\overline s$ or discrimination by $\overline s$ affects $i$'s readability even when $i$ is reviewed by referees who do not discriminate.
Moreover, offsetting unfairness with fairness only works when *everyone* is fair. Asymmetry from one upsets symmetric criteria applied everywhere else, creating endless imbalance when some people just will not be fair. If culture and/or behaviour predicate bias against $i$ and restrain comparable bias against $k$ then, sans intervention, we permanently and unjustly take from $i$ and give to $k$.
[](#Corollary1) adds two stronger conditions to [](#Theorem1). According to the first, $i$ and $k$ must be comparably experienced by time $t$. [](#Corollary1) actually applies under the weaker $e_{nit}^s\le e_{nkt}^s$, $n=0,1$ (see its proof in [][AppendixProofs]), but $R_{it}-R_{kt}$ may overestimate $D_{ik}$ if $e_{nkt}^s&lt;e_{nit}^s$ for all $t&gt;t'$. Nevertheless, $e_{nit}^s-e_{nkt}^s$ converges to 0 as $t$ tends to infinity, so $R_{it}-R_{kt}$ consistently predicts the *direction* of $D_{ik}$ for large enough $t$.
The second condition precludes $s'$ such that $s'$ is in $\Sigma_{A_{it}}$ but not in $\Sigma_{A_{kt}}$---*e.g.*, because $i$'s utility of acceptance exceeds that of $k$'s. Of course, $i$'s unconditional acceptance rate is not higher than $k$'s (Condition 3), so $s'$ necessarily offsets some other $s''$ such that---because $s''$ discriminates against $i$---$s''$ is in $\Sigma_{A_{kt}}$ but not in $\Sigma_{A_{it}}$. But $R_{it}-R_{kt}$ may not fully counteract the first effect; [](#equation12) does---providing a conservative estimate of $D_{ik}$ under [](#Theorem1)'s weaker Condition 3.
&lt;!--\input{$HOME/Dropbox/Readability/draft/equations/equation12}--&gt;</Text>
            <Comments>&lt;!--\label{Footnote68}--&gt;As shown in [](#Theorem1)'s [proof][AppendixProofs], $i$'s beliefs about $\widetilde r_{0i}^s$ and $\widetilde R_i^s$ converge from above. Coupled with Jensen's inequality, this means $\widetilde r_{0i}^s+e_{0it}$ and $\widetilde R_i^s+e_{1it}^s$ may exceed $i$'s time $t$ expectations of $\widetilde r_{0i}^s$ and $\widetilde R_i^s$, respectively. At the limit, however, $e_{0it}^s$ and $e_{1it}^s$ converge to 0---so as $t$ increases, this "comfort buffer" declines.
The asymmetry's direction is captured in the sign: positive if $s$ is tougher on $i$; negative otherwise.
That is, if cultural and/or behavioural factors mean that $\delta_{nik}^s&gt;0$ for at least one $s\in\Sigma$, *and* there is no comparable offsetting bias against $k$ *and* education and/or time cannot eliminate $\delta_{nik}^s$, then $i$ is at a permanent disadvantage relative to $k$.
See also the discussion in [](#Footnote68) and [][SEUMatching].
Although [](#equation12) counteracts the impact of any $s'$ such that $s'$ is in $\Sigma_{A_{it}}$ but not in $\Sigma_{A_{kt}}$, it comes at a cost: [](#equation12)'s attenuation bias is much larger than the one generated by [](#equation11).</Comments>
        </Document>
        <Document ID="55655E1F-9EA6-414E-8EE8-2928EDC52CD2">
            <Title>Google</Title>
            <Text>[](#eq:p3eq6) investigates the effect internet search---and Google, specifically---has on readability:where $\text{Google search}_j$ is one of two statistics: a dummy variable equal to 1 for articles published after Google incorporated (September 1998) and the natural log of total annual searches on [Google.com](http://www.google.com).
[](#p3table10b) presents results. It suggests the gender readability gap has grown with the popularity of search engines. The first panel shows the marginal effect for annual search numbers. The ratio of female authors is weakly significant for four out of five scores: each time internet search traffic doubled---which it has 19 times since 1998---women's Flesch Reading Ease score increased by !!GoogleLogFlesch points; their Flesch-Kincaid, Gunning Fog and SMOG scores declined by roughly !!GoogleLogThree points.
Men experienced a muted version of this trend. Doubling the number of Google searches made annually is correlated with a !!GoogleLogFleschMan point rise in their Flesch Reading Ease score and !!GoogleLogKincaidMan point fall in their Flesch-Kincaid score. Differences between the sexes are significant only for the latter.
[](#p3table10b)'s second panel re-estimates [](#eq:p3eq6) with the Google incorporation dummy. Marginal effects are more pronounced---male and female estimates are !!GoogleManPTwo and !!GoogleWomanPTwo times higher, respectively---although the precision of those estimates has not improved. The Flesch Reading Ease suggests Google improved readability of female-authored abstracts by five points; according to the others, it cut schooling needed to understand abstracts by up to a year and a quarter. Men likewise improved, albeit to a lesser degree. Differences between the sexes are newly significant for the Gunning Fog and SMOG scores; the Flesch-Kincaid remains weakly significant. All suggest Google's existence is correlated with a !!GoogleSexCompare percent greater increase in the readability of female-authored abstracts relative to abstracts by men.</Text>
            <Comments>\input{$P3/equations/eqn6.tex}
\input{$P3/tables/tex/table10b.tex}
Ratio of second panel coefficient to first panel coefficient multiplied by the logarithm of $2^{19}$.</Comments>
        </Document>
        <Document ID="F0FEDE9C-0C3E-4643-B58B-D4C0EB21152B">
            <Title>Use in research</Title>
            <Text>Thanks to high predictive validity and ease of use, readability formulas are widely employed in education, business and government. The U.S. Securities and Exchange Commission encourages clearer financial disclosure forms benchmarked against the Gunning Fog, Flesch-Kincaid and Flesch Reading Ease scores[#Cox2007]. The formulas have also guided readability assessments of, *inter alia*, standardised test questions[#Chall1977,Chall1983], medical inserts[*e.g.*,\]\[][#Wallace2008], technical manuals[*e.g.*,\]\[][#Hussin2012,Klare1973], health pamphlets[*e.g.*,\]\[][#Foster2002,Meade1989] and data security policies[#Alkhurayyif2017].
In research, readability scores are popular proxies for "complexity". [#Enke2018;] controls for language sophistication using the Flesch Reading Ease formula in a study of moral values in U.S. presidential elections. [#Spirling2016;] employs the same score to show that British parliamentarians simplified speeches to appeal to less educated voters in the in the wake of the Great Reform Act. Legal research has found that judges are more reliant on legislative history when interpreting complex legal statutes, as measured by the Flesch-Kincaid formula[#Law2010]. And in finance, various scores have linked the clarity of financial communication materials to better firm and market financial health[#Li2008,Biddle2009,Jansen2011], larger investment and trading volume [#Miller2010,ThÃ¶rnqvist2015,DeFranco2015,Lawrence2013] and lower demand for---albeit higher reliability of---outside research by sell-side analysts[#Lehavy2011].</Text>
            <Comments>[#Bischof2018;] similarly show a positive link between plain language in political speeches and a desire to appeal to voters using German political manifestos and the BjÃ¶rnssonâs readability index. Their study also finds evidence that voters better understand more readable political messages.came to a similar conclusion using German political manifestos and the BjÃ¶rnssonâs readability index. Their study also finds evidence that voters better understand more readable political messages.
[#Long2011;] investigate whether a legal brief's readability score correlates with its success on appeal, but find that they do not.
See [#Loughran2016;] for a thorough review of the use of readability measures in finance and accounting research.</Comments>
        </Document>
        <Document ID="60D4D28A-20D7-4895-A8AB-649C1D39F501">
            <Title>269 (alastairâs macbook's conflicted copy 2016-05-08)</Title>
            <Text>[](#p3table3) displays average characters, words, syllables, polysyllabic words and difficult words per sentence, broken down by sex. According to all measures, women write shorter, simpler sentences---they contain fewer characters, fewer syllables, fewer words and fewer "hard" words. All differences are highly significant.
[](#p3table4b) presents the estimated coefficients from an ordinary least square regression of an article's ratio of female co-authors on each of the five readability scores. Column (1) includes only the four journal dummies. It indicates abstracts written only by women score 1.27 points higher on the Flesch Reading Ease scale and take between 1--6 fewer months of schooling to understand according to the other four measures. Percentage-wise, women write between 1--3 percent better than men.
Column (2) includes 63 year dummies; column (3) adds another 182 journal and year interaction dummies; column (4) introduces the 98 institutional dummies. Controlling for time reduces gender's impact on the first four scores by 20--40 percent but exacerbates it by a third for the Dale-Chall score. Adding the institution dummies has little to no effect on the estimates. Coefficients and standard errors are very similar across columns (2), (3) and (4).
The coefficients on the journal dummy variables from (2) are presented in [](#p3table5).They contrast the readability of *Econometrica*, *JPE* and *QJE* with *AER* and provide a useful check on the reliability of the five scores. As intuitively anticipated, all five scores agree that *Econometrica* is harder to read; the Flesch Reading Ease and Dale-Chall scores suggest *JPE* and *QJE* are easier.
Columns 4 and 5 add controls for 20 primary *JEL* codes. Articles are usually assigned multiple codes---the average paper in the data has !!AvgJEL linked to it. To avoid subjectively choosing only one or randomly deleting data, each article is duplicated $n_j$ times, where $n_j$ is the number of primary *JEL* codes assigned to article $j$. Observation $j_k\in\{1,\ldots,n_j\}$ is allocated article $j\text{'s }k\text{th}$ *JEL* code and weighted by $1/n_j$.
Since classifications and institutions are only available for a subset of articles, columns 3--5 estimate [](#eq:p3eq1) on just 45, !!JELSubset and 16 percent of the data, respectively. Nevertheless, coefficients are roughly the same as those in column 2, albeit slightly higher---female-authored abstracts may be !!RangeColC percent better written.
[](#p3table5b) displays $\beta_c$ from estimating [](#eq:p3eq2)where $\text{score}_{j_k}^s$ is the $k\text{th}$ copy of readability score $s$ for article $j$, and subscript $c_{j_k}$ denotes article $j\text{'s}$ $k\text{th}$ *JEL* code.
According to at least one readability score, gender differences are less pronounced in subfields B (history of economic thought, methodology and heterodox approaches), F (international economics), H (public economics), M (business administration and business economics; marketing; accounting), O (economic development, technological change and growth) and R (regional, real estate and transportation economics).
Abstracts from subfield O are still better written when authored by women according to three of the five scores; marginal effects for B, F, H and R are not significantly different from zero. Weak evidence points to men as the better writers in subfield M: its Flesch-Kincaid score suggests female-authored abstracts take !!FleschKincaidM more years of schooling to understand (standard error !!FleschKincaidse); effects for the other scores are insignificant.
Papers classified as D (microeconomics), N (economic history) or Q (agricultural and natural resource economics; environmental and ecological economics) have more pronounced gender differences in readability---female-authored abstracts read !!DBetter, !!NBetter and !!QBetter times better than the average, respectively. All readability scores agree, are significant on their own and most are significantly higher than the mean effect.
In contrast to my expectation, more women in a field may not necessarily be an antidote to gender differences in readability. While D, N and Q are about average in terms of total female co-authors, they vary widely in how frequently their papers name only female co-authors. Q is in the !!QFemOnly, D the !!DFemOnly, N the !!NFemOnly. Admittedly, !!NWomen percent is itself pretty low; economic history papers are still overwhelmingly---as in !!NMen percent---penned just by men. But given the readability gap is present in subfields with both above- and below-average rates of sole female authorship, women may need to be better writers even where more of them publish.</Text>
        </Document>
        <Document ID="D93AD7C3-BF13-47BC-AD45-73499FCC8FC5">
            <Title>Untitled</Title>
        </Document>
        <Document ID="DC0D39EB-E0F9-4045-8936-1BEAC5BC35C7">
            <Title>Estimation strategy</Title>
            <Text>Holding acceptance rates constant, [](#Theorem1) rules out confounding factors---*e.g.*, sensitivity to criticism and individual preferences---by comparing readability between equivalent authors experienced in peer review (Condition 1) and within authors before and after gaining that experience (Condition 2).
I consider authors "experienced" by $t=3$. Authors with one or two top-four publications are probably tenured and well-established in their fields. By publication three, all frequently referee (and some edit) prestigious economics journals. I assume this accumulated experience means equivalent authors are equally accurate about $\widetilde r_{0i3}$ and $\widetilde R_{i3}$, so remaining errors are no longer gender specific: $e_{ni3}^s=e_{nk3}^s$, [$n=0,1$](#Corollary1).
To account for equivalence, I matched every female author with three or more publications (121) to her closest male counterpart (1,553). Matches were made based on the probability of treatment (female) from a probit model performed with replacement using the following co-variates: (1) $T_i$; (2) minimum order in an issue; (3) fraction of papers first-authored by $i$; (4) maximum citation count; (5) maximum institutional rank; (6) fraction of papers published per decade; (7) fraction of papers published by each journal; and (8) number of articles per primary *JEL* category. Fractions, minimums and maximums were calculated over $T_i$. Co-variate balance pre- and post-match are shown in [][AppendixMatchingBalance]. [][AppendixMatchingNames] lists each matched pair. Results using a Mahalanobis matching procedure are shown in [][Appendix Mahalanobis]. Results from alternative matching algorithms are available on request (&lt;!--\href{mailto:erin.hengel@gmail.com}{\texttt{erin.hengel@gmail.com}}--&gt;).
$\widetilde r_{0i}^s$ and $\widetilde R_i^s$ may be influenced by factors that vary with $t$: female ratio, journal, year, co-author characteristics and stereotypes about authors' institutions. $\widehat R_{it}$ accounts for this. It reconstructs $R_{it}$ at female ratio equal to 1 for women, 0 for men and median $t=3$ values of other co-variates---number of co-authors, institutional rank, institutional rank of the highest ranked co-author, $t$ for the most experienced co-author, publication year, dummies for each journal---using relevant coefficients and residuals from four separate time- and gender-specific regressions on readability. (See [][AppendixReconstruction] for regression output.) Throughout the next section (and appropriate appendices), standard errors adjust for the degrees of freedom lost when generating $\widehat R_{it}$.</Text>
            <Comments>Recall that $e_{nit}^s-e_{nkt}^s$ converges to 0, so for large enough $t$ [](#equation11) and/or [](#equation12) predict the direction of $D_{ik}$ even when errors remain gender-specific. (See discussions in the next section.)
Propensity scores were estimated using the entire sample (771 female authors; 6,105 male authors). Each female author with at least three publications was then matched to her nearest male neighbour (*i.e.*, the man with the closest propensity score, conditional on having at least three publications in the data). Matches were generated in Stata using `psmatch2` [#Leuven2003].
I eschewed means in favour of minimum order in an issue, maximum citation count and maximum institutional rank on the assumption that an author's "quality" is principally a function of his best paper.
That is, a dual-authored paper published in 2008 in the *American Economic Review* where $t=3$ for $i$ and his co-author and their institutions rank 48 and 54, respectively.
Specifically, standard errors are inflated by a factor of 1.2.</Comments>
        </Document>
        <Document ID="7E9CC279-AE5B-4ACD-AF4D-A61459822F40">
            <Title>Untitled</Title>
        </Document>
        <Document ID="1BD4AF95-951B-446B-8D6F-BA28F0445598">
            <Title>Online appendices</Title>
        </Document>
        <Document ID="A93AC44A-82E5-4444-B6C0-9ADC63CB55A5">
            <Title>[](#table10), male effects, [](#equation12) and Condition 1</Title>
            <Text>[](#tableC15) estimates $D_{ik}$ with [](#equation12). [](#tableC16) estimates $D_{ik}$ with a rough attempt to control for acceptance rates---it requires $T_i\le T_k$ or $T_k\le T_i$ before categorising matched pairs as discrimination against $i$ or $k$, respectively. Conclusions from both tables are are similar to those presented in [][SEUMatching].
&lt;!--\input{$HOME/Dropbox/Readability/draft/tex/generated/tableC15}--&gt;
&lt;!--\input{$HOME/Dropbox/Readability/draft/tex/generated/tableC16}--&gt;
--&gt;</Text>
        </Document>
        <Document ID="1DA036F0-69BD-4B4F-9848-2BFAF0681621">
            <Title>[][SEUMatching], supplemental anlysis</Title>
        </Document>
        <Document ID="26299AAC-A568-45A1-8572-2C03439D7261">
            <Title>Insolvency</Title>
            <Text>Suppose now $X_1&lt;D$; the project is insolvent: "unable to pay its debts as they fall due"[\]\[Â§123(2)][#GreatBritain1986]. No legal restrictions prevent the entrepreneur and creditor from settling matters on their own: renegotiating the terms of their loan contract or jointly agreeing to liquidate. Formal bankruptcy is their fallback when they fail to do so.</Text>
            <Comments>Technically it is "cash flow insolvent". Another test is "balance sheet insolvency" whereby the firm's liabilities exceed its assets. Usually, balance sheet insolvent firms are also cash flow insolvent; rarely, however, a firm is *currently* able to pay its debts but clearly won't be able to in the future. In this scenario, creditors (or shareholders) can petition the court to declare the company insolvent. Since every balance sheet insolvent firm must eventually be cash flow insolvent, the entrepreneur has no opportunity to misuse company funds and the time value of money is zero, I disregard balance sheet insolvent firms (without loss of generality).</Comments>
        </Document>
        <Document ID="5F7D2065-CFA9-4A41-8503-E5485CDF451F">
            <Title>[][Experience], supplemental output</Title>
        </Document>
        <Document ID="73A9C97A-D69D-4052-A3E9-67629AAC9D50">
            <Title>dissertation</Title>
            <Text>LaTeX input: dissertation-header
Title: &lt;$projecttitle&gt;
Author: &lt;$author&gt;
Email: &lt;$custom:E-mail&gt;
Affiliation: &lt;$custom:Affiliation&gt;
Abstract One: &lt;$synopsis&gt;
Abstract Two: &lt;$synopsis&gt;
Abstract Three: &lt;$synopsis&gt;
Thanks Bitches: I would like to thank Jeremy Edwards, Pramila Krishnan, SÃ¶nje Reiche, Melvyn Weeks and, especially, my supervisor Christopher Harris. Their thoughtful input has made this dissertation much better. I am also grateful to my dissertation examination committee, Prof. Hamish Low and Prof. Leonardo Felli for instructive comments.
My Disclaimer: This dissertation is the result of my own work and includes nothing which is the outcome of work done in collaboration. It is not substantially the same as any that I have submitted or is being concurrently submitted for a degree or diploma or other qualification at the University of Cambridge or any other University or similar institution. I further state that no substantial part of my dissertation has already been submitted or is being concurrently submitted for any such degree, diploma or other qualification at the University of Cambridge or any other University or similar institution.
My Word Count: This dissertation does not exceed the prescribed word limit for the Faculty of Economics Degree Committee.
Base Header Level: 2
LaTeX input: dissertation-begin
LaTeX footer: dissertation-footer</Text>
        </Document>
        <Document ID="15BED637-BE8D-43E5-BEB7-7D57E209101E">
            <Title>[](#table6), including quality/productivity controls</Title>
            <Text>To remain consistent with a similar regression in [](#table11), [](#table6) does not include author productivity effects. [](#tableD12) reproduces [](#table6) including these effects. Coefficients and standard errors vary little from those in [](#table6).
&lt;!--\input{$PPATH/tables/tex/tableD12}--&gt;</Text>
            <Notes>&lt;!--\clearpage--&gt;</Notes>
        </Document>
        <Document ID="FA3C24A8-E42C-42CC-A1CE-FEED91EE1CCD">
            <Title>Bankruptcy.</Title>
            <Text>If the entrepreneur and lender cannot see eye-to-eye on whether to restructure their debt or liquidate the project, each petitions the court to force the other to accept his desired result---reorganisation or compulsory liquidation. A judge adjudicates.
Reorganisation---administration in the U.K.---is a supervised version of continuation. The entrepreneur formulates a restructuring plan; a court-appointed administrator approves and oversees its implementation.
As discussed in the introduction, reorganisation is expensive. Legal and accounting fees, lost customers, suppliers and employees, added bureaucracy and even theft from fraudulent administrators add up. The upshot is delayed production, asset depreciation and lower profits. $Y\in(0,1)$ captures this. It measures the fraction of project value reorganisation wastes. What's left is only
$$V_2^R=X_1+(1-Y)\l(X_2+K_2\r).$$
Compulsory liquidation is the second option. In theory, compulsory liquidation is more transparent, straightforward and faster than reorganisation. Service and employment contracts are immediately voided and business operations cease. Secured creditors repossess their liens; a court-appointed administrator auctions off remaining assets and distributes the proceeds to unsecured creditors. Any excess is settled on equity.
Winding up a business is less ambiguous than rescuing it---giving corruption, bureaucracy and legal fees less scope to eat away a firm's eventual liquidation value. And since that value is independent of unrealised future earnings, bankruptcy's impact on business reputation is especially irrelevant. Compulsory liquidation, therefore, probably wastes a smaller fraction of a firm's time 1 liquidation value than reorganisation wastes of its time 2 continuation value.
To incorporate this idea without adding unnecessary complexity, I assume compulsory liquidation incurs no added cost and is instantaneous. Gross returns are therefore identical to those in voluntary liquidation.
Whether reorganised or liquidated, the creditor is no longer guaranteed the full face value of his loan. Given $X_1&lt;D$, neither $V_1^L$ nor $V_2^R$ is necessarily large enough to cover $D$. When it isn't, the creditor takes home the entirety of the project's gross value; otherwise, he earns $D$. His gross returns in liquidation and reorganisation are, respectively
$$C_1^L=\min\{D,V_1^L\}\eqOr C_2^R=\min\{D,V_2^R\}.$$
As before, the entrepreneur keeps whatever remains: $E_1^L=V_1^L-C_1^L$ if the project is liquidated and $E_2^R=V_2^R-C_2^R$ if it's reorganised.</Text>
            <Comments>In Germany and France, control cedes to a court-appointed administrator; in the U.S., it remains with existing managers. In most countries payments cease on outstanding loans and an automatic stay---lasting anywhere from three months in Germany to over a year in France---is applied to secured claims. The firm can usually obtain new financing, often at terms more favourable than existing debt. After a certain period---at least four months in the U.S. and more than 18 in France---a plan is proposed to restructure debt and reorganise the firm. In the U.S., this period can be extended indefinitely by the bankruptcy court. Creditors vote on it and the court approves it; in some jurisdictions equity holders also have a say. In the U.S. a judge can impose a plan already rejected by creditors. In France, creditors have no vote; only the court decides whether the plan is implemented.
Generally, these proceeds are distributed according to legally defined absolute priority rules. For example, in the U.S., administrative and legal fees incurred during proceedings are paid first; next, statutory claims, including unpaid taxes and wages; finally, unsecured debt. Evidence from the U.S. suggests this ordering is very rarely violated when firms are liquidated in Chapter 7[#Bris2006].
It is empirically difficult to disentangle the cost of reorganisation from the cost of liquidation. Evidence from the U.S. suggests that legal and accounting fees are roughly eqivalent[#Ferris1997,Ferris2000], but recovery rates in Chapter 7 are lower than those in Chapter 11[#Weiss1990,Bris2006]. (Because these studies cannot compare Chapter 7 and Chapter 11 recovery rates for the same firm, however, the data are not especially informative on the actual cost of each regime. Additionally, recovery rates in Chapter 7 partially incorporate losses incurred in Chapter 11 given most firms that are eventually liquidated previously attempted a reorganisation.) In contrast, empirical work conducted in Italy and the U.S. after each country introduced or significantly expanded reorganisation procedures suggest creditors anticipate lower returns when reorganisation is more likely[#Rodano2012,Scott1986]. Indeed, emerging markets with weak institutions and severe restrictions on reorganisation appear far more capable of recovering creditors' claims than their peers with more generous procedures[#Djankov2008].
&lt;!--\label{p1footnote2}--&gt;In general, however, a nominal court fee may be due---Â£200 in the U.K.---and both sides undoubtedly incur legal and administrative fees they otherwise wouldn't be subject to if the liquidation were voluntary. For example, in the U.K., the party petitioning the court for compulsory liquidation must advertise in the press a number of days before the process takes place.</Comments>
        </Document>
        <Document ID="5195FD07-F29D-42B9-9513-3C1467650578">
            <Title>Theory</Title>
            <Text>
From [][rescue_culture], the degree of credit rationing in any economy is largely determined by how many insolvent firms would be reorganised in bankruptcy and the amount of money wasted if they were. Inefficient reorganisation makes getting a loan unequivocally more difficult---an increase in $Y$ decreases the value of creditors' outside option and weakens their bargaining position during workouts. The cost of lending rises, as does credit-rationing ([](#prop5)(i)).
PROPOSITION
(i) When $Y$ increases, credit-rationing increases. (ii) When $p$ increases, more viable but fewer non-viable firms are credit-rationed.
eprop
The impact of $p$ is not so straightforward. $p$ affects creditor earnings in different ways depending on whether an insolvent project is viable or not. For the non-viable, increasing judicial accuracy makes it more likely bankrupt projects are liquidated. The value of creditors' outside option rises; entrepreneurs accept greater financial responsibility during workouts; credit rationing falls. For viable firms, an increase in $p$ has precisely the opposite effect. Judicial error actually makes it easier for these firms to get credit ([](#prop5)(ii)).
Judicial error simultaneously expands and contracts credit rationing. For non-viable firms the direction is obvious: it increases insolvency costs and stifles lending. When judges make mistakes, however, viable firms are sometimes liquidated. This decision is legally inaccurate and possibly even economically suboptimal. Nonetheless creditors do not lose money in liquidation so their expected returns in insolvency are higher. They are more willing to lend.
COROLLARY
In receivership, $Y$ and $p$ have no impact on credit-rationing.
ecor
But by replacing payments with outcomes---and eliminating judicial interference entirely---a perfectly efficient outcome prevails. Firms liquidate and continue when they should, debt is cheap and credit-rationing unheard of. In its purest form, receivership---*i.e.*, procedures similar to those found in the U.K. prior to 1986---achieves both. As shown in [](#prop4) ([][rescue_culture]), receivership forces entrepreneurs to shoulder maximum financial responsibility during insolvency; every such project is funded. $Y$ and $p$ have no impact on [credit-rationing](#cor3).</Text>
            <Comments>Proved in [][p2Proofs].</Comments>
        </Document>
        <Document ID="1D724DCE-7C3F-4B21-928C-A6E7D99D4CA0">
            <Title>Results</Title>
            <Text>[](#table7) presents results from OLS estimation of [](#equation2), FGLS estimation of [](#equation6) and [](#equation7) and OLS estimation of [](#equation3). Since gender bias is possible only when authors' identities are known or can be reasonably guessed, estimates exclude the 279 articles subjected to double-blind review at the *AER* and *QJE* before the internet. In the next section, I include these observations and consider their impact.
In order to maximise the sample size, estimates in the first three columns of [](#table7) do no include primary *JEL* controls. Estimates including these controls are very similar to those presented in [](#table7) and are reproduced in [][AppendixNBERfield]. As discussed in [](#Footnote121), estimates in the final column implicitly control for field, already.
Results in [](#table7) strongly indicate the readability gap grew precisely while papers were being reviewed. The first column displays $\beta_{1P}$ from OLS estimation of [](#equation2). According to all five scores, women's readability gains outpace men's between versions. Estimates additionally confirm published readability is correlated with draft readability: coefficients on $R_{jW}$ (shown in [][AppendixDraftCorr]) are positive and significant---but only about 0.8. A less than unit value suggests $\mu_{jP}$ exerts downward pressure on $R_{jW}$'s coefficient, thereby artificially inflating first column figures (see previous section).
&lt;!--\input{$HOME/Dropbox/Readability/draft/tex/generated/table7}--&gt;
[](#table7)'s remaining columns present results from both strategies meant to deal with this bias. Columns 2--4 display FGLS estimates. Coefficients on $\text{female ratio}_j$ from [](#equation6) ($\wt\beta_{1W}$) and [](#equation7) ($\wt\beta_{1W}+\beta_{1P}$) are shown in columns two and three, respectively. Female-authored working papers and published articles are both better written---but the readability gap is substantially larger in the latter. Flesch-Kincaid, Gunning Fog and SMOG scores imply immediate peer review accounts for 40--60 percent of the total (biased) effect of female ratio in [](#equation7); Flesch Reading Ease and Dale-Chall scores indicate a smaller proportion (30 percent). Column four displays their difference ($\beta_{1P}$); it is positive and significant for all five scores.
OLS estimates of $\beta_{1P}$ from [](#equation3) are shown in [](#table7)'s final column. Magnitudes are close to FGLS estimates---confirming earlier conclusions---standard errors are slightly higher. Moreover, given non-bias related factors specific to sub-field that may affect readability in a published paper will similarly affect the readability of a working paper, these estimates already implicitly control for sub-field. Thus, the similarity between the two estimation strategies in [](#table7) confirms the findings in [][ArticleLevel] that, conditional on the other explanatory variables, the gender readability gap is basically independent of field. See [][Appendix NBERfield] for evidence further corroborating this finding.
Both strategies show a significant increase in the gender readability gap *ex post*. Assuming non-peer review factors are always independent of either its timing or gender, this establishes the desired causal link. Threats to this identification strategy are considered next.</Text>
            <Comments>&lt;!--\label{Footnote51}--&gt;Excluding these observations does not noticeably impact results or conclusions[for estimates based on the full sample, see\]\[p. 18][#Hengel2016].
FGLS difference ($\beta_{1P}$, column four) divided by the effect in published articles ($\wt\beta_{1W}+\beta_{1P}$, column three).
The discussion in [](#footnote46) also applies to the precise accuracy of the assumption's phrasing used here.</Comments>
        </Document>
        <Document ID="0F42141D-7EF3-42ED-8A99-7BA16F18625B">
            <Title>Double-blind review before the internet</Title>
            <Text>Since gender bias is possible only when authors' identities are known or can be reasonably inferred, estimates in [](#table7) exclude the 279 articles in the sample subjected to double-blind review before the internet.
I now consider their impact. Two journals---*QJE* and *AER*---employed double-blind review at some point during the time period covered by the data. *AER*'s spell began 1 July, 1989 and ended 1 July, 2011. *QJE* used double-blind procedures until 1 June, 2005. *Econometrica* and *JPE* have never blinded referees to authors' identities.
[](#table8a) presents the marginal effect of female ratio by review process based on OLS estimation of [](#equation3a).
&lt;!--\input{$HOME/Dropbox/Readability/draft/equations/equation3a}--&gt;
 where $\text{Blind}_j$ is a dummy variable equal to 1 if an article was subjected to double-blind review before 1998, the year Google incorporated.
Before the internet, double-blind review may have reduced the gender readability gap. [](#table8a) suggests a smaller---possibly negative---gap under blinded peer review; however, difference-in-difference estimates (reported in the final row) are not statistically significant. Although their consistent direction provides some (weak) evidence that masking authors' identities limits peer review's impact on the gender readability gap, this interpretation should be made with caution given the study did not originally intend to assess the policy's effectiveness.
&lt;!--\input{$HOME/Dropbox/Readability/draft/tex/generated/table8a}--&gt;
And in any case, this result does not survive the internet. In [][AppendixDoubleBlind], I analyse the impact of double-blind review *after* 1998. Gender differences are positive in both single- and double-blind samples, suggesting blinding referees to authors' identities may no longer be the anti-bias fix it possibly once was.</Text>
            <Comments>Forty-three articles with at least one female author and 236 articles with no female author.
&lt;!--\label{Footnote66}--&gt;From 1 May 1987 to 31 May 1989, half of the papers submitted to *AER* were evaluated by single-blind review; the remaining half were subjected to double-blind review[for details on the trial, see\]\[][#Blank1991]. Referees correctly identified at least one author in 45.6 percent of double-blind reviewed papers---indicating that only about a quarter of the manuscripts were truly double-blind reviewed. I therefore classify every paper published during the trial as having undergone single-bind review. Excluding these observations from the analysis, however, has very little impact; estimated coefficients and standard errors are similar to those presented in [](#table8a).
$\text{Blind}_j$ is equal to 1 for articles published during an official policy of double-blind review. A final publication date, however, may substantially lag the actual review date[for an illustration and discussion, see\]\[][#Blank1991]. Because results are unchanged when including only *AER* articles published post May 1989 (see [](#Footnote66)) and all *QJE* articles published before June 2005 were evaluated under double-blind review, misclassification errors are unlikely to substantially bias estimates presented in [](#table8a).
In a preliminary version of this paper [#Hengel2015], I estimated the impact of double-blind review on the gender readability gap in the sample of published papers, only. (It did not compare draft vs. published readability as is done here.) I found that double-blind review was correlated with a higher readability gap. As shown in [][AppendixDoubleBlind], however, this conclusion is not robust to including time effects.
This conclusion is obviously specific to economics, although it may also apply to fields with an analogous culture of presenting, disseminating and publicising working paper results. Again, please interpret these results with caution.</Comments>
        </Document>
        <Document ID="D83425EA-5CA9-430A-8213-250CBC4D6C67">
            <Title>[](#table5), male effects</Title>
            <Text>[](#tableC4) displays total male effects---*i.e.*, the total effect for men co-authoring only with other men---from the regressions presented in [](#table5). Effects estimated at a female ratio of zero and observed values for other co-variates. Grade-level effects (Flesch-Kincaid, Gunning Fog, SMOG and Dale-Chall) have been multiplied by negative one (see [][MeasuringReadability]).
&lt;!--\input{$HOME/Dropbox/Readability/draft/tex/generated/tableC4}--&gt;
&lt;!--\clearpage--&gt;</Text>
        </Document>
        <Document ID="5D741475-37DD-4CE3-AE49-EDCF1CD1CA81">
            <Title>Thesis</Title>
        </Document>
        <Document ID="29A31DD2-C169-4A85-822A-D29E31F6BA9F">
            <Title>[](#table11), alternative thresholds for \\(\text{mother}_j\\)</Title>
            <Text>[](#tableC18) repeats the regression pre\\-sented in [](#table11) column (5), using alternative age thresholds to define motherhood: $\text{mother}_j$ equals 1 if paper $j$'s co-authors are all mothers to children younger than three (first column), four (second column), *etc.* Changing this threshold has little effect on female ratio's coefficient. The coefficients on $\text{mother}_j$ and $\text{birth}_j$ are persistently negative and positive (respectively), although magnitudes and standard errors vary. Remaining coefficients are unaffected.
&lt;!--\input{$HOME/Dropbox/Readability/draft/tex/generated/tableC18}
\clearpage--&gt;</Text>
        </Document>
        <Document ID="186E4648-6F29-477E-B553-86C6CCD7A99D">
            <Title>Does peer review affect readability?</Title>
        </Document>
        <Document ID="15388DBA-0041-4429-BB18-6B7FF29D2856">
            <Title>Super-senior financing</Title>
            <Text>Let lower support for $X_1$ be $X_1^L&lt;0$; $X_2$ remains non-negative.  Consider the entrepreneur of a viable firm who obtained a loan at face value $D_1$ and earned $X_1&lt;0$ in time 1.  To continue operating, he must finance first period losses. I assume loans taken out in this manner are legally conferred seniority to all existing debt but obligations in the original debt contract award existing creditors right of first refusal.
Allowing super-senior financing prevents creditors from blocking new loans---should existing debt holders refuse, the borrower can turn to the wider lending market. Thus, creditors evaluate the new loans independent of their previous stake. They extend one if there exists some $D_2$ such that
$$!e[p1equation12]\EE_1\l[\min\{D_2, X_2+K_2\}\r]=-X_1.e!$$
[](#p1equation12) is increasing in $D_2$. $D_2$ exists if and only if $-X_1\le\ol X_2+K_2$, where $\ol X_2$ is the expected value of $X_2$. If $D_2$ does not exist for the original creditor, it does not exist for any creditor. Without necessary financing, the firm is liquidated.
When evaluating the original loan at time 0, the entrepreneur expects $D_1$ when $D_1\le X_1$. If $X_1$ falls below that but above 0, the project is insolvent. In the absence of bankruptcy costs, workout negotiations favour the creditor since reorganisation is equivalent to continuation and liquidation guarantees recovery of his initial investment; without loss of generality I assume both parties agree to continue operating the firm but do not otherwise modify the original loan contract. Creditor returns are
$$\EE_1\l[\min\{D_1,X_1+X_2+K_2\}\r].$$
If $X_1$ is less than 0 but more than $-(\ol X_2+K_2)$, the creditor loans $-X_1$ per [](#p1equation12). His expected returns at time 1 are
$$\EE_1\l[\min\{D_1+D_2,X_2+K_2\}\r]+X_1.$$
When $X_1$ falls below $-(\ol X_2+K_2)$ the project is liquidated. Since the firm is deemed "viable", $K_0&lt;-(\ol X_2+K_2)$. The creditor earns nothing.
In every scenario, creditor returns increase in $D_1$. Their supremum occurs at the limit as $D_1$ tends to infinity, *i.e.*,
$$!e[p1equation13]\ol X_1+\ol X_2+K_2,e!$$
 where $\ol X_1$ is the expected value of $X_1$. Since the firm is viable, [](#p1equation13) is more than $K_0$. Applying similar arguments used to prove [](#prop2), a $D_1$ exists that satisfies $\ol C_0=K_0$. The lending conditions for viable firms are not fundamentally altered when bankruptcy law allows super-senior financing.</Text>
            <Comments>Since neither creditor nor entrepreneur are responsible for time 2 losses, the assumption $0\le X_2$ is made for explanatory ease. If $X_2$ were negative, an equivalent non-negative random variable and corresponding probability mass function exist that assign 0 for all $X_2$ such that $X_2+K_2&lt;0$ and $X_2+K_2$, otherwise.
Non-negative returns at time 1 introduces a nuance to "viability". In fact, the firm was *not* viable---only expected to be. After realisation of time 1 earnings, it is better to liquidate, since project earnings are not high enough to cover the costs required to keep it in operation.
Note that even if the firm were to go bankrupt it would be immediately liquidated. The new loan is required to continue operating (for example, to pay wages), thus even if the judge mandated reorganisation no funding would emerge to make that possible.</Comments>
        </Document>
        <Document ID="001E946F-302B-4710-AA2D-EFF1F23C5F5B">
            <Title>Results</Title>
        </Document>
        <Document ID="C97C25C4-FAE9-4E24-9FF2-DC11A2AEC103">
            <Title>Hypotheses</Title>
            <Text>This paper's goal is to test the validity of [](#prop5) and [](#cor3). Consider first $Y$. The local cost of debt recovery should impact lending only when formal court-sponsored intervention is unavoidable; when creditors control assets, cumbersome proceedings and expensive oversight are probably irrelevant. This leads to the following two testable hypotheses.
HYPOTHESIS
In countries with dual-chapter bankruptcy, an increase in $Y$ increases the probability a firm reports finance as an obstacle.
ehypoth
HYPOTHESIS
In countries with receivership, an increase in $Y$ has no effect on the probability a firm reports finance as an obstacle.
ehypoth
A similar argument suggests $p$ has no effect in receivership countries. Yet, [](#cor3) claims independence if $p$ impacts only judicial accuracy. More realistically, security prices in efficient capital markets, stringent disclosure laws and well-developed credit-rating agencies and financial analysis sectors better reveal "firm fundamentals" to lenders[#Berkovitch1999]---an unexplored component of [][rescue_culture] given both parties possess identical information.
Indeed, theoretical and empirical research has mostly shown that this type of information reduces borrower-lender asymmetry, thereby alleviating credit constraints. It limits information monopolies enjoyed by incumbent lenders[#Padilla1997,Sharpe1990] and encourages better borrower behaviour[#Klein1992]. Greater information is linked to more lending[#Jappelli2002,Djankov2007], lower costs[#Brown2009] and more loans to riskier borrowers[#Berger2005]. The take-away is that receivership should make borrower information irrelevant to resolving bankruptcies but still vital for getting credit.
Testing the impact of $p$ under dual-chapter bankruptcy is complicated by an additional identification issue. $p$ is undeniably important---it crucially impacts the probability a firm is liquidated in bankruptcy. Its sign, however, is ambiguous. According to [](#prop5), a higher $p$ reduces credit rationing for non-viable firms and increases it for viable firms. Because firm viability is not known, however, [](#p2equation5) aggregates $p$'s impact across all firm types.
Although I cannot identify $p$'s impact by firm, I *can* distinguish its impact between bankruptcy regimes. [](#hypoth3) contrasts countries with receivership to those without it---and theorises that information's effect on firms in the latter is less precisely estimated and hews more closely zero than it does for firms in the former.
HYPOTHESIS
In countries with receivership, higher $p$ reduces the probability a firm reports finance as an obstacle by an amount greater than the analogous effect in countries with dual-chapter bankruptcy.
ehypoth
</Text>
            <Comments>When loan contracts can specify either receivership or Chapter 11-style procedures to resolve potential defaults---as was possible in the U.K. between 1986--2003---evidence suggests a large majority  are based on receivership[#Armour2008].
Extrapolating beyond this assumption, [](#cor3) technically makes no prediction on $Y$, either; in contrast to credit information, however, there is no convincing theory that contract enforcement costs matter in the absence of contract disputes.
Several partial equilibrium theoretical models actually predict an ambiguous effect. For example, as discussed in [][p2Introduction], [#Pagano1993;] demonstrate that greater levels of borrower information boost loans to safe borrowers but cut them to risky ones.</Comments>
        </Document>
        <Document ID="5ABF8EA6-409A-467F-A815-FC4437CE8E10">
            <Title>Readability meta-analysis</Title>
        </Document>
        <Document ID="EE478991-6BD5-46F1-8A38-DA02AFB8E93A">
            <Title>[](#table4), alternative program for calculating readability scores</Title>
        </Document>
        <Document ID="462F4C77-ADD6-4646-97AC-3C2648409855">
            <Title>Ideas</Title>
        </Document>
        <Document ID="5B7FA46D-1F81-4F7E-AF23-02266CE0366B">
            <Title>Rescue Culture, part 2</Title>
            <Synopsis>Chapter 1's model implies the following: (i) reducing bankruptcy's impact on future earnings improves financial contracting; but (ii) judicial accuracy has an ambiguous effect. I empirically test both hypotheses using data from a large, cross-country, firm-level survey. I find reorganisation's cost contributes a great deal to perceived financing constraints. Information, on the other hand, appears to have the anticipated ambiguous effect, although data limitations prevent definitively attributing it to the predictions of the model.</Synopsis>
        </Document>
        <Document ID="00DDAE17-DE91-4C67-AB69-8B560BC180D6">
            <Title>Outcome.</Title>
            <Text>High bankruptcy costs weaken creditors' ability to solicit fairer workouts. From [](#lem2), however, workouts are no worse than bankruptcy and liquidation may be better. If the former exists, both parties (weakly) prefer to settle; otherwise, insolvent firms sometimes go bankrupt ([](#prop1)).
PROPOSITION
Insolvent projects choose between voluntary liquidation and: (i) a workout if $\,\ol V_1^B\le\ol V_1^C$; (ii) bankruptcy, otherwise.
eprop
Without reorganisation, viable firms optimally continue---and per [](#prop1)(i), this is precisely what they do. Liquidation cannot beat an accepted workout and creditors never reject proposals by viable firms.
Non-viable firms optimally liquidate---but their owners sometimes insist on workouts that their lenders don't refuse. Workouts enable entrepreneurs to extract deviations from absolute priority. How much depends on creditor earnings in bankruptcy. When those earnings are low, creditors tolerate larger write-offs. Thus, entrepreneurs demand a premium to liquidate: the project's piecemeal value must offset the "haircut" debt holders concede in a workout (see [](#p1figure1)).
But workouts aren't always available. Sometimes creditor gains from a possible liquidation outweigh the potential cost of an expensive reorganisation---making bankruptcy a worthwhile gamble. And sometimes, time 1 earnings are so low that entrepreneurs fight liquidation no matter how inefficient continuation may be. The firm goes bankrupt.</Text>
            <Comments>\input{$PPATH/p1/figures/tex/p1figure1}</Comments>
        </Document>
        <Document ID="386CE913-CCE2-422B-B8C0-02B731120203">
            <Title>Explanations</Title>
            <Text>[](#p3table12) documents a rise in the readability gap as women publish more articles. [](#p3eqSUP5) is an author-level estimation of the impact publication counts have on men and women's readability; the positive differences observed in [](#p3table13) therefore point to individual-specific explanations driving this phenomenon.
One such explanation is "learning-by-doing". If the payoff from lucid exposition is high, people will catch on---either by internalising explicit comments on text readability in referee reports from earlier papers or making the (un)conscious connection that review times are faster when text is clearer. Applying that payoff only to women yields a succinct explanation for the gap's observed growth.
But there are two convincing alternatives---one a complement, the other a substitute to "learning-by-doing". I now investigate both and show that neither is likely.
Consider first the substitute. As discussed in [][p3authorlevel], readability and female ratio may be nonlinearly related---specifically, evidence suggests the latter is increasing and convex in the former (see [](#fn3) and the Online Appendix). Thus, if women are more likely to co-author with other women (or by themselves) when they already have several publications behind them, the observed readability increase would merely reflect that late-career concentration.
This is not the case. [](#p3table16)displays male and female marginal effects on the number of prior papers from a fixed effects regression on female ratio. As publication counts increase, genders diversify: men publish with more women and women publish with more men.
The complementary explanation applies to the evolution of female writers as a group---perhaps the clearest women publish more often. [](#p3equation9) investigates:where $\Phi$ is the standard normal cumulative distribution function, $T_i\ge x$ equals one if author $i$'s total publication count $T_i$ is greater than or equal to the positive integer $x$ and subscript 1 denotes author $i$'s first paper. [](#p3table14)displays $\Phi^\prime\cdot\l(\beta_2+\beta_4\r)$---the marginal effect of female authors' first papers' readability scores on the probability of publishing multiple times.
The readability of a woman's first paper does not predict how often she will publish. Regardless of the threshold chosen, it has no impact on her eventual productivity and is indistinguishable from zero in all estimates. Several alternative dependant variables, independent variables, functional forms and dataset restrictions were tried. None suggest initially good writers, male or female, publish more frequently than bad ones (see Online Appendix).</Text>
            <Comments>Another related explanation is that women are more responsive to referee reports. This and other explanations are addressed in [][Discussion].
\input{/Users/erinhengel/Desktop/tables/tex/p3table16}
\input{/Users/erinhengel/Desktop/equations/p3equation9}
\input{/Users/erinhengel/Desktop/tables/tex/p3table14}</Comments>
        </Document>
        <Document ID="4A61D303-F61C-478F-B670-8429D6A8A57F">
            <Title>Results</Title>
            <Text>[](#p2table6)displays results from estimating [](#p2equation5). The second panel presents effects of the variables from [](#p2table1). As before, youth and manufacturing contribute to steeper financing obstacles. Older, service industry companies complain less. Size and foreign-ownership are newly significant---smaller, domestic firms have the toughest time with credit---but state-ownership no longer is. Export status remains close to zero and insignificant.
The third panel in [](#p2table6) lists marginal effects for the lending environment variables from [](#p2table2). None are remotely significant, save GDP---an extra $1,000 per person reduces perceived financing constraints by eight percent.
[](#p2table7)displays the marginal effects of $Y$, $p$ and $B$. The first column shows each at observation-weighted grand means of the covariates. Only credit information is (weakly) significant---a full unit increase reduces perception that finance is an obstacle by about a third. Implementing receivership likewise cuts financing constraints; large contract enforcement costs increase them. Neither effect is significant.
The next two columns in [](#p2table7) break down $Y$ and $p$ by bankruptcy regime. The latter is signifiant only with receivership: a unit increase corresponds to an almost fivefold decrease in reported financing constraints. Without it, the effect is close to zero and not statistically different from it. The difference is both large and significant---confirming [](#hypoth3).
In countries with receivership, the effect on $Y$ is negative, but erratic---in line with [](#hypoth2)'s prediction. [](#hypoth1) appears likewise confirmed: without receivership, $Y$ is positively correlated with financing constraints---as anticipated---but significant only at relatively extreme values of $p$.
[](#p2figure1)maps this relationship, showing $Y$'s marginal effect (fixed at various percentiles) over $p$. $Y$'s positive correlation with $\text{obstacle}_i$ is persistent, yet significant only when information is abundant. Recall that sparse borrower information makes accurate adjudication of viable firms less likely---possibly mitigating the impact of costly reorganisation. As information increases, however, reorganisations do, too---so their impact may be more acutely felt.
Of course, non-viable firms experience the situation in reverse. Poor information makes reorganisation more probable, so $Y$ should have its clearest impact when $p$ is small. This interpretation would be less significant, however, if viable projects dominate the sample---or at least respondents' frame of mind when asked "Is access to financing [an obstacle] to the current operations of this establishment?".</Text>
            <Comments>\input{$PPATH/p2/tables/tex/p2table6}
\input{$PPATH/p2/tables/tex/p2table7}
\input{$PPATH/p2/figures/tex/p2figure1}
"Accurate" is used here in the legal sense: most laws make weak or no reference to bankruptcy's costs, providing judges with little scope to refer to them in rulings. See [][rescue_culture], [](#p1footnote1) for a discussion.
For example, firm owners may benchmark a lending environment based on their most difficult experience obtaining a loan. In the context of the model, this corresponds to requests to fund long-term, "viable" projects. (I frequently use "firm" and "project" interchangeably; in the context of the WES survey, however, they are conceptually separate things: a firm (or its business owner) may have several projects for which it is seeking funding.) An additional consideration is that "viable" projects may sometimes be converted into "non-viable" projects in order to satisfy [lenders](#cor2). Their business owners may nevertheless continue to perceive constraints according to their original conception of a project (for which it was impossible to obtain credit) as opposed to its actual implementation.</Comments>
        </Document>
        <Document ID="FF57AED3-6EAB-4B34-9983-D1C235D464C7">
            <Title>Identification</Title>
            <Text>The data pre- and post-review make it possible to isolate gender differences in readability pre-existing peer review from those incurred during it---and therefore identify gender's contemporaneous effect on peer review scrutiny. The key equation connects published articles to earlier versions of the same paper: scores depend on draft readability as well as factors that affect writing clarity any time *after* being released as working papers. [](#equation2) is the OLS representation of this relationship.
&lt;!--\input{$HOME/Dropbox/Readability/draft/equations/equation2}--&gt;
 where $R_{jP}$ and $R_{jW}$ are readability scores for working ($W$) and published ($P$) versions of paper $j$, respectively. $\beta_{0P}$ is a constant specific to version $P$; $\beta_{1P}$ is the coefficient of interest and reflects the particular impact $\text{female ratio}_j$ has in peer review. $\vect X_{jP}$ and $\mu_{jP}$ are $P$-specific observable (editor, journal, journal-year interactions and English language dummies and $\text{max. }t_j$) and unobservable components, respectively. $\vep_{jP}$ is $P$'s error term.
$P$-specific variables may be correlated with $R_{jW}$. Even if $\mu_{jP}$ and $\text{female ratio}_j$ remain independent, positive correlation between $R_{jW}$ and [$\text{female ratio}_j$](#table6) still biases OLS estimates of $\beta_{1P}$ in a direction opposite to the bias on $R_{jW}$. [](#equation3) eliminates the distortion by subtracting $R_{jW}$ from both sides of [](#equation2):
&lt;!--\input{$HOME/Dropbox/Readability/draft/equations/equation3}--&gt;
 Assuming zero partial correlation between $\text{female ratio}_j$ and $\mu_{jP}$, OLS generates an unbiased estimate of $\beta_{1P}$.
An alternative strategy based on[#Ashenfelter1994;] separately estimates NBER working paper and published article readability using generalised least squares (GLS); $\beta_{1P}$ is identified post-estimation by differencing coefficients. The set-up combines [](#equation2) with a relationship defining readability scores *before* external evaluators demand [edits](#equation4).
&lt;!--\input{$HOME/Dropbox/Readability/draft/equations/equation4}--&gt;
 where $\beta_{0W}$ is a constant specific to version $W$ and $\beta_{1W}$ reflects $\text{female ratio}_j$'s impact on readability prior to peer review. $\vect X_{jW}$ and $\mu_{jW}$ are version-invariant observable (publication year, citation count, *JEL* effects and $\text{max. }T_j$) and unobservable components, respectively. $\vep_{jW}$ is version $W$'s error term.
OLS estimates of [](#equation4) may be biased by arbitrary correlation between $\mu_{jW}$ and the explanatory variables. [](#equation5) defines a general structure for that correlation[#Ashenfelter1994].
&lt;!--\input{$HOME/Dropbox/Readability/draft/equations/equation5}--&gt;
 where $\omega_j$ is uncorrelated with $\text{female ratio}_j$, $\vect X_{jW}$ and $\vect X_{jP}$. Substituting [](#equation5) into [](#equation4) generates the following reduced form representation of $R_{jW}$:
&lt;!--\input{$HOME/Dropbox/Readability/draft/equations/equation6}--&gt;
 where $\wt\beta_{0W}=\beta_{0W}+\gamma$, $\wt\beta_{1W}=\beta_{1W}+\eta$, $\wt{\bm\uptheta}_W=\bm\uptheta_W+\bm\updelta_W$ and $\wt\vep_{jW}=\vep_{jW}+\omega_j$. Similarly, obtain $R_{jP}$'s reduced form by substituting [](#equation6) into [](#equation2):
&lt;!--\input{$HOME/Dropbox/Readability/draft/equations/equation7}--&gt;
 where $\wt{\bm\uptheta}_P=\bm\uptheta_P+\bm\updelta_P$ and $\wt\vep_{jP}=\wt\vep_{jW}+\vep_{jP}$. [](#equation6) and [](#equation7) are explicitly estimated via feasible GLS (FGLS). $\beta_{1P}$ is identifiable post-estimation by subtracting reduced form coefficients; assuming zero partial correlation between $\mu_{jP}$ and $\text{female ratio}_j$, it is unbiased.
Both OLS estimation of [](#equation3) and FGLS estimation of [](#equation6) and [](#equation7) require zero partial correlation between $\mu_{jP}$ and $\text{female ratio}_j$ to obtain a valid $\beta_{1P}$. Roughly restated, non-peer review factors must be either independent of its timing (and therefore subsumed in version-invariant fixed effects) or unrelated to gender. [][NBERResults] evaluates this assumption; briefly, however, I could think of nothing that simultaneously (and convincingly) influences readability, coincides with peer review's timing and correlates with author gender.</Text>
            <Comments>&lt;!--\label{Footnote121}--&gt;Research may be easier to explain in certain fields. If so, then field affects a published paper's readability because it affects the readability of the underlying working paper and is therefore not due to peer review. By differencing these scores, [](#equation2) therefore implicitly already controls for field.
&lt;!--\label{Footnote46}--&gt;$\text{max. }t_j$ is the number of prior papers published in any of the top four economics journals by article $j$'s most prolific co-author. It and the English language dummy are considered $P$-specific because they may influence the degree to which editors and/or referees scrutinise the paper. Because all papers in both samples share the same highest-ranked institution (NBER), authors' institutions---which presumably have a similar effect---are omitted.
I assume the duration between a paper's NBER release and its publication is too short to influence aggregate time trends; publication year dummies are applied to both working paper and published versions.
$\mu_{jP}$ may be correlated with $\wt\vep_{jW}$ via $\omega_j$ and/or $\vep_{jW}$ without biasing the FGLS estimate of $\beta_{1P}$ because both are uncorrelated with the explanatory variables in [](#equation4) (by assumption) and [](#equation6) (by definition).
Unbiased estimation of $\beta_{1P}$ in [](#equation7) requires zero partial correlation between $\mu_{jP}$ and $\text{female ratio}_j$ after controlling for $\vect X_{jW}$ and $\vect X_{jP}$; [](#equation3) requires zero partial correlation after controlling for $\vect X_{jP}$, only.
&lt;!--\label{footnote46}--&gt;This phrasing is slightly inaccurate but convenient for exposition. Zero correlation between $\text{female ratio}_j$ and $\mu_{jP}$ does not preclude biased estimates of $\beta_{1P}$ when $\mu_{jP}$ is correlated with other explanatory variables that are, in turn, correlated with $\text{female ratio}_j$ by some factor independent of $\mu_{jP}$. Unbiasedness instead requires zero *partial* correlation between $\mu_{jP}$ and $\text{female ratio}_j$.
A possible exception is external feedback solicited outside of peer review---*e.g.*, during conferences and seminars. As the next section points out, however, the population of people who provide such feedback overlaps with the population of journal referees. It seems unlikely that this population is biased only in one setting---especially given both settings emphasise gender neutrality.</Comments>
        </Document>
        <Document ID="B30AEF02-62BB-4CAF-A417-1D257EA9867E">
            <Title>Average first, mean and final paper scores</Title>
            <Text>[](#tableB1) displays authors' average readability scores for their first, mean and final papers. Grade-level scores (Flesch-Kincaid, Gunning Fog, SMOG and Dale-Chall) have been multiplied by negative one (see [][MeasuringReadability]). Sample excludes authors with fewer than three publications.
&lt;!--\input{$HOME/Dropbox/Readability/draft/tex/generated/tableB1}--&gt;
&lt;!--\clearpage--&gt;</Text>
        </Document>
        <Document ID="016D7AE3-8D8F-48FB-93DB-A4FF922ABFD4">
            <Title>[](#table4), alternative clustering and quality/productivity controls</Title>
            <Text>The following tables repeat regressions in [](#table4), clustering errors instead on [volume](#tableD8), [issue](#tableD9) and [article](#tableD10). Standard errors vary little.
[](#tableD11) repeats the regressions in [](#table4) using an alternative measure of a paper's quality---the order an article appears in an issue. (For example, the lead article is assigned one, the next article two, *etc.*) It is meant to capture a paper's contribution and importance---its so-called "$q$-quality"[#Ellison2002b]. As [](#tableD11) illustrates, including fixed effects for order has little impact on coefficients or their standard errors.
&lt;!--\input{$HOME/Dropbox/Readability/draft/tables/tex/tableD8}--&gt;
&lt;!--\input{$HOME/Dropbox/Readability/draft/tables/tex/tableD9}--&gt;
&lt;!--\input{$HOME/Dropbox/Readability/draft/tables/tex/tableD10}--&gt;
&lt;!--\input{$HOME/Dropbox/Readability/draft/tables/tex/tableD11}--&gt;
&lt;!--\clearpage--&gt;</Text>
            <Comments>[#Ellison2002a;] showed that papers published earlier in an issue spend less time in peer review. (This is supported by [](#table9).) He attributes this to a "$q$-$r$ trade-off": reviewers demand fewer $r$-quality revisions (robustness checks, clarity, *etc.*) from papers that represent important contributions ($q$-quality).</Comments>
            <Notes>&lt;!--\clearpage--&gt;</Notes>
        </Document>
        <Document ID="F42EBDA9-AE93-46A6-AAA2-4F651BCC8E71">
            <Title>Results</Title>
            <Text>Does the readability gap change as publication counts increase? Yes, it widens---from women writing more clearly and men possibly less so. As their careers evolve, women improve: their average readability scores are 1-5 percent higher than the readability of their first papers; their latest papers 1--7 percent [higher][AppendixFirstLast]. For a man, however, his average and last paper may be more poorly written than the first. [](#figure4) plots mean Flesch Reading Ease scores grouped by authors' $t\text{th}$ article; as the count increases, men and women diverge.
&lt;!--\input{$PPATH/figures/tex/figure4}--&gt;
[](#table10) tests significance of that divergence by FGLS estimation of [](#equation1) (omitting $\text{score}_{it-1}^s$) on subsamples corresponding to authors' first ($t=1$), second ($t=2$), third ($t=3$), fourth and fifth ($t=4\text{--}5$) and sixth and up ($t\ge6$) articles published in the journals and time periods covered by the data. Only marginal effects on co-authoring with women for female authors are shown ($\beta_1$). Final column is a population-averaged estimate on the pooled sample. Regressions in columns ($t=1$) to ($t\ge6$) are weighted by $1/N_j$ (see [][AuthorLevel]), standard errors adjusted for two-way clustering on editor and author and corrected for cross-model correlation. Final column estimates are unweighted, error correlations are specified by an auto-regressive process of order one and standard errors are clustered on author.
&lt;!--\input{$PPATH/tables/tex/table10}--&gt;
All figures agree---women write better---but the magnitude and significance of that difference increases as $t$ increases despite falling numbers of observations. Between columns ($t=1$) and ($t=2$), the gap marginally widens but is not significant; after that, it triples (at least); the increase is significant ($p&lt;0.05$) for all five scores. At higher publication counts, differences in male-female readability remain roughly constant, although estimates are only weakly significant and suffer from small samples of female authors.
Despite having the largest sample, first-time publications are not driving the observed readability gap. [](#figure4) suggests little or no gender difference when $t=1$; [](#table10) backs this up. Coefficients in column ($t=1$) are imprecise, roughly half the size of those from a pooled regression (last column) and a fraction the size of estimates in columns ($t=3$), ($t=4\text{--}5$) and ($t\ge6$). [Wald tests][AppendixEqualityTests] reject equality of $\beta_1$ in the first and third models at $p&lt;0.01$ for the Flesch Reading Ease, Flesch-Kincaid and SMOG scores and $p&lt;0.05$ for the Gunning Fog and Dale-Chall scores.
To generate a slope coefficient measuring the mean additional contribution each paper makes to readability, I estimate:[](#equation9) includes $t$ and its interaction with an article's female ratio and the author's sex; otherwise, it is identical to [](#equation1) and similarly estimated by first-differencing with endogeneity instrumented with earlier lags[#Arellano1995,Bundell1998].
$t$'s (immediate) marginal effect is presented in [](#table11). Male effects estimated for male authors co-authoring with no females $(\beta_3+\beta_4)$; female effects for female authors co-authoring with no males $(\beta_3+\beta_5)$. The coefficients agree: readability scores remain constant as men publish more papers---all are very close to zero and none significantly different from it. For women, however, every additional paper is more readable than the last; the effect is statistically significant for four out of five scores.
&lt;!--\input{$PPATH/tables/tex/table11}--&gt;
[](#figure5) plots mean male and female effects over $t$ using estimates derived from the Flesch Reading Ease regression in [](#table11). It serves as a more precise illustration of the trends depicted in [](#figure4). As before, there is little or no gender difference in readability for the first two publications but thanks to women's self-improvement, the gap widens substantially after that. While [](#figure4) suggested men were also writing more poorly, that effect is not present here.
&lt;!--\input{$PPATH/figures/tex/figure5}--&gt;</Text>
            <Comments>Coefficient equality test statistics are available in [][AppendixEqualityTests].
Note that figures in columns two and three of [](#table10) are roughly in line with third column estimates in [](#table8)---on average, $t=2.7$ for female-authored articles released first as NBER working papers.
\input{$PPATH/equations/equation9}</Comments>
        </Document>
        <Document ID="5501BE86-C631-47A1-AF86-70D4FE823133">
            <Title>[](#table4), journal effects</Title>
            <Text>[](#tableC4) shows the coefficients on the journal dummies in column (2), [](#table4). They compare *AER*'s readability to the readability of *Econometrica*, *JPE* and *QJE*.
&lt;!--\input{$PPATH/tables/tex/table5}--&gt;</Text>
        </Document>
        <Document ID="8EDD2579-E7F5-48CC-BBBE-C048BCBD97F7">
            <Title>Robustness</Title>
            <Text>[](#table10) documents a rise in the readability gap as women publish more articles. [](#table11) points to an individual-specific explanation driving this phenomenon. One such explanation is "learning-by-doing". If the payoff from lucid exposition is high, people will catch on---either by internalising explicit comments on text readability in referee reports from earlier papers or making the (un)conscious connection that review times are faster when text is clearer. Applying that payoff only to women yields a succinct explanation for the gap's observed growth.
&lt;!--\input{$PPATH/figures/tex/figure6}--&gt;
But there are two possible alternatives. I investigate both and show that neither is likely. As discussed in [][AuthorLevel], readability and female ratio may be nonlinearly related---specifically, evidence suggests the latter is increasing and convex in the former. Thus, if women are more likely to co-author with other women (or by themselves) when they already have several publications behind them, the observed increase would actually reflect that late-career concentration.
This is not the case. [](#table12) displays the marginal effect of $t$ for men and women from a fixed effects regression on female ratio. The effect for men is positive; the effect for women negative. As $t$ increases, genders diversify: men publish with more women and women publish with more men.
&lt;!--\input{$PPATH/tables/tex/table12}--&gt;
The second explanation applies to the evolution of female writers as a group---perhaps the clearest women publish more often? [](#equation10) investigates:where $\Phi$ is the standard normal cumulative distribution function, $T_i\ge x$ equals one if author $i\text{'s}$ total publication count, $T_i$, is at least the positive integer $x$ and subscript 1 refers to his first publication. [](#table13) displays $\Phi^\prime\cdot\l(\beta_2+\beta_4\r)$---the marginal effect of female authors' first paper readability scores on the probability of publishing multiple times.
&lt;!--\input{$PPATH/tables/tex/table13}--&gt;
The readability of a woman's first paper does not predict how often she will publish. Regardless of the threshold chosen, it has no impact on her eventual productivity and is indistinguishable from zero in all estimates.</Text>
            <Comments>A related possibility is that women are more responsive to referee reports. This and other explanations are addressed in [][Alternatives].
\input{$PPATH/equations/equation10}</Comments>
        </Document>
        <Document ID="E8B47CC6-8DB8-4F9F-A5E1-AADFB743A31C">
            <Title>Measuring discrimination</Title>
            <Text>Estimates presented in [](#table11NEW) suggest Conditions 2 and 3 are satisfied *on average*, but [](#Theorem1) technically requires more: *i.e.*, that they are satisfied for the same author, on average. I find Condition 2 and 3 are twice as likely to be simultaneously satisfied for a female author than they are for a male author: in roughly half of all matched pairs, the female author satisfied both Conditions 2 and 3 simultaneously; in another quarter the male author satisfied both conditions simultaneously. In the remaining matched pairs, neither male nor female author simultaneously satisfied both criteria, rendering the test in [](#Theorem1) inconclusive. This suggests female authors are twice as likely to experience discrimination according to [](#Theorem1) than otherwise equivalent male authors.
&lt;!--\input{$PPATH/tables/tex/table11NEWXX}--&gt;
It is also possible to generate a conservative measure of the average effect of discrimination over all matched pairs in which one member satisfies Conditions 2 and 3---and thus, determine whether the average effect leans against men or women. This is the average of the following figure: $$D_i=\bm1_i\big(R_{i3}-\max\left\{R_{i1},R_{k3}\right\}\big),$$ where $\bm1_i$ equals 1 if the author is female and -1 if the author is male.
Consider a matched pair for which Conditions 2 and 3 are satisfied, suggesting discrimination against $i$. The raw estimate of final gender difference in readability is $R_{i3}-R_{k3}$. This reflects actual discrimination only if author $i$ would otherwise voluntarily choose readability $R_{k3}$ if the probability of acceptance at that readability were identical to her probability of acceptance at readability $R_{i3}$. If $R_{k3}&gt;R_{i1}$, then this is true: $R_{i1}$ forms an upper bound on the choice of readability author $i$ would make in the absence of peer review---*i.e.*, left to her own devices, we know author $i$ would choose some $R\le R_{i1}$. If $R_{k3}&lt;R_{i1}$, however, then we do not know if $i$ would otherwise voluntarily choose readability $R_{k3}$ if the probability of acceptance at that readability were identical to her probability of acceptance at readability $R_{i3}$---we only know that if that statement were true, she would choose no higher readability than $R_{i1}$.
The first two columns of [](#table11NEWXX) shows average $D_i$ for male and female authors. Mean $D_f$ is noticeably higher than mean $D_m$ in absolute value (although not always significantly so) in addition to being experienced by twice as many female authors than male authors. The final column of [](#table11NEWXX) averages columns (2) and (3) (weighted by the number of duplicate male observations in the sample). In all five scores it is positive and significant, indicating that, on average, discrimination is more likely to affect female authors; on average, it causes readability scores to rise by about 5--10 percent in female-authored papers.</Text>
        </Document>
        <Document ID="510FE4D4-748C-4E5E-B2EB-3D14433DD0AA">
            <Title>Identification-1</Title>
            <Text>
 =========================
Correlation between $\mu_{jW}$, $\text{female ratio}_j$ and/or $\vect X_{jW}$ biases OLS estimates of [](#p3equationAX1). Yet the 
The coefficient of interest is $\beta_{1P}$. 

 
Realistically, $\vep_{jP}$ has some impact on the unobserved components of the publication process captured by $\mu_{jP}$---for example, referees demand more readability edits when realisations of $\vep_{jW}$ are particularly low. [](#p3equationAX3) defines that correlation.
&lt;!--\begin{equation}\label{p3equationAX3}
\mu_{jP}=\zeta\,\vep_{jW}+\omega_j
\end{equation}--&gt;
 where $\omega_j$ is uncorrelated with $\vep_{jW}$, $\text{female ratio}_j$ and $\vect X_{jP}$. Correlation between $\mu_{jP}$ and $\vep_{jW}$ implies correlation between $\mu_{jP}$ and $\text{score}_{jW}$. OLS estimates of $\beta_{1P}$ are subsequently biased by partial correlation between $\text{female ratio}_j$ and $\text{score}_{jW}^s$.
[](#p3equationAX4) replaces $\mu_{jP}$ with [](#p3equationAX3) and then substitutes $\text{score}_{jP}^s$ and $\vep_{jW}$ with their predicted values (denoted by a carat) from unbiased estimates of [](#p3equationAX1).
&lt;!--\begin{equation}\label{p3equationAX4}
\text{score}_{jP}^s=\widehat{\text{score}_{jW}^s}+\beta_{0P}+\beta_{1P}\,\text{female ratio}_j+\bm\uptheta_P\,\vect X_{jP}+\zeta\,\widehat{\vep_{jW}}+\omega_j+\vep_{jP},
\end{equation}--&gt;
 Assuming $\omega_j$ is uncorrelated with the explanatory variables in [](#p3equation4), estimating [](#p3equationAX4) generates an unbiased estimate of $\beta_{1P}$. More likely, however, correlation between $\mu_{jW}$, $\text{female ratio}_j$ and/or $\vect X_{jW}$ biases OLS estimates of [](#p3equationAX1). [](#p3equationAX3), adopted from [#Ashenfelter1994;], defines the general structure of the underlying correlation.
&lt;!--\begin{equation}\label{p3equationAX5}
\mu_{jW}=\eta\,\text{female ratio}_j+\bm\updelta\,\vect X_{jW}+\bm\upgamma\,\vect X_{jP}+\omega_j,
\end{equation}--&gt;
 where $\omega_j$ is uncorrelated with $\text{female ratio}_j$, $\vect X_{jW}$ and $\vect X_{jP}$. Substituting [](#p3equationAX4) into [](#p3equationAX1) generates the following reduced form representation of working paper readability:
&lt;!--\begin{equation}\label{p3equationAX6}
\text{score}_{jW}^s=\beta_{0W}+\wt\beta_{1W}\,\text{female ratio}_j+\wt{\bm\uptheta}_W\,\vect X_{jW}+\bm\upgamma\,\vect X_{jP}+\wt\vep_{jW},
\end{equation}--&gt;
 where $\wt\beta_{1W}=\beta_{1W}+\eta$, $\wt{\bm\uptheta}_W=\bm\uptheta_W+\bm\updelta$ and $\wt\vep_{jW}=\vep_{jW}+\omega_j$. To obtain a reduced form for published article readability, substitute [](#p3equationAX5) into [](#p3equationAX2):
&lt;!--\begin{equation}\label{p3equationAX7}
\text{score}_{jP}^s=(\beta_{0W}+\beta_{0P})+(\wt\beta_{1W}+\beta_{1P})\,\text{female ratio}_j+\wt{\bm\uptheta}_W\,\vect X_{jW}+(\bm\uptheta_P+\bm\upgamma\vect)\,\vect X_{jP}+\wt\vep_{jP},
\end{equation}--&gt;
 where $\wt\vep_{jP}=\wt\vep_{jW}+\vep_{jP}$. [](#p3equationAX5) and [](#p3equationAX6) are explicitly estimated via feasible GLS (FGLS). $\beta_{1P}$ is identifiable post-estimation by subtracting reduced form coefficients.
GLS estimates of $\beta_{1P}$ may still be biased by arbitrary correlation between $\vect X_{jW}$ and $\mu_{jP}$. This bias is obviated, however, by subtracting $\text{score}_{jW}^s$ from both sides of [](#p3equationAX2):
&lt;!--\begin{equation}\label{p3equationAX8}
\text{score}_{jP}^s-\text{score}_{jW}^s=\,\beta_{0P}+\beta_{1P}\,\text{female ratio}_j+\bm\uptheta_P\,\vect X_{jP}+\mu_{jP}+\vep_{jP}.
\end{equation}--&gt;
 [](#p3equationAX8) is estimated via OLS; the coefficient on $\text{female ratio}_j$ is an unbiased estimate of $\beta_{1P}$ under the weakest identifying assumption of no partial correlation between $\text{female ratio}_j$ after controlling for $\vect X_{jP}$.
 ============================================
 ============================================
 is the weakest identifying assumption necessary to obtain an unbiased estimate of $\beta_{1P}$.
In the absence of $\mu_{jW}$, OLS $\widehat{\text{score}_{jP}^s}$ and $\widehat{\vep_{jW}}$ based on OLS estimation of [](#p3equationAX1) are unbiased; $\beta_{1P}$ from a corresponding two-stage-least-squares (2SLS) estimation of [](#p3equationAX4) are similarly unbiased.

Correlation between $\mu_{jP}$ implies correlation between $\text{score}_{jW}$ and $\mu_{jP}$; partial correlation between $\text{score}_{jW}^s$ and $\text{female ratio}_j$ then biases OLS estimates of $\beta_{1P}$ from [](#p3equationAX2)
Unless partial correlation between $\mu_{jP}$ and $\text{female ratio}_j$ is zero after controlling for $\text{score}_{jW}^s$ and $\vect X_{jP}$, 
Unfortunately, OLS generates unbiased estimates only under an implausible assumption: zero partial correlation between $\mu_{jP}$ and $\text{female ratio}_j$ 
 More realistically, $\text{score}_{jP}^s$ has some impact on the unobserved components of the publication process captured by $\mu_{jP}$. 
As long as $\text{female ratio}_j$ and $\mu_{jP}$ are not partially correlated, it is possible to obtain a consistent estimate of the coefficient of interest, $\beta_{1P}$. The simplest case assumes zero partial correlation between $\mu_{jP}$ and $\text{female ratio}_j$ after controlling for $\text{score}_{jW}^s$ and $\vect X_{jP}$; OLS estimates of [](#p3equationAX2) generate an unbiased $\beta_{1P}$.
More likely, however, $\text{score}_{jP}^s$ has some impact on an unobserved component of the publication process captured by $\mu_{jP}$. 
by eliminating selection bias from version-invariant omitted variables. To isolate gender differences in readability pre-existing peer review from those incurred during it, I use two separate (yet analogous) estimation strategies based on a conceptual framework outlined by[#Ashenfelter1994;]. The first strategy removes selection effects by regressing female ratio on the change in score via OLS. In the second strategy, I regress female ratio on abstract readability of NBER working papers and, separately, published articles using generalised least squares (GLS). Selection effects are eliminated post-estimation by differencing coefficients.
In the absence of $\mu_{jW}$, one might replace $\text{score}_{jW}^s$ in [](#p3equationAX2) with its predicted value and error from an OLS estimation of [](#p3equationAX1). The presence of $\mu_{jW}$, however, generate biased estimates of both. Instead, I define the general structure of the underlying correlation between $\mu_{jW}$ and the observable variables in both [](#p3equationAX1) and [](#p3equationAX2) based on the conceptual framework outlined in 
In the absence of $\mu_{jW}$, OLS of [](#p3equationAX1) produces an unbiased estimate of $\vep_{jW}$. Assuming $\mu_{jP}$ 
compensate for a low realisation of $\vep_{jW}$ by demanding more readability edits. 
In the absence of $mu_{jW}$, OLS estimates of [](#p3equationAX1) are unbiased; replace $\text{score}_{jW}^s$
replace $\text{score}_{jW}^s in [](#p3equationAX2) with its predicted value
It is possible to obtain an unbiased estimate of the coefficient of interest, $\beta_{1P}$, as long as the partial correlation 
A generalised least squares (GLS) regression of female ratio on abstract readability of NBER working papers and, separately, published articles, still produces an unbiased estimate of $\beta_{1P}$.
an unbiased estimate of $\beta_{1P}$ can still be obtained by regressing 
 using generalised least squares (GLS). 
Although correlation between $\mu_{jW}$, $\text{female ratio}_j$ and/or $\vect X_{jW}$ biases estimates of [](#p3equationAX1), selection effects are eliminated post-estimation by differencing coefficients.
, OLS generates unbiased estimates of [](#p3equationAX2).
Correlation between $\mu_{jP}$ and $\vep{jW}$ implies correlation between $\text{score}_{jW}$ and $\mu_{jP}$; OLS estimates of $\beta_{1P}$ are then biased by partial correlation between $\text{score}_{jW}^s$ and $\text{female ratio}_j$. In the absence of $\mu_{jW}$, one might replace $\text{score}_{jW}^s$ in [](#p3equationAX2) with its predicted value and error from an OLS estimation of [](#p3equationAX1). The presence of $\mu_{jW}$, however, generate biased estimates of both. Instead, I define the general structure of the underlying correlation between $\mu_{jW}$ and the observable variables in both [](#p3equationAX1) and [](#p3equationAX2) based on the conceptual framework outlined in 
partial correlation between $\text{female ratio}_j$ and $\text{score}_{jW}^s$ then biases OLS estimates of $\beta_{1P}$. 
Throughout this section, I assume (i) correlation between $\mu_{jW}$, $\text{female ratio}_j$ and/or $\vect X_{jW}$ biases OLS estimates of [](#p3equationAX1); but (ii) partial correlations between $\mu_{jP}$ and $\text{female ratio}_j$ are zero after controlling for $\vect X_{jP}$.
If partial correlations between $\mu_{jP}$ and $\text{score}_{jW}^s$ are similarly zero, OLS estimation of [](#p3equationAX2) generates unbiased estimates of the parameter of interest, $\beta_{1P}$. More likely, however, $\text{score}_{jP}^s$ has some impact on the unobserved components of the publication process captured by $\mu_{jP}$. For example, $\mu_{jP}$ and $\vep_{jW}$ could be correlated as would occur if a low realisation of $\vep_{jW}$ induces more readability edits during peer review. Alternatively, perhaps Dutch writers are both more likely to assiduously address referee comments and more likely to be highly productive economists. Partial correlation between $\text{female ratio}_j$ and $\text{score}_{jW}^s$ then biases the OLS estimate of $\beta_{1P}$.
f partial correlation between $\mu_{jP}$ and $\text{female ratio}_j$ are also zero after controlling for $\vect X_{jW}$, then a second method also generate unbiased estimates of $\beta_{1P}$. As mentioned, correlation between $\mu_j$ and and $\text{score}_{jW}^s$ variables biases the OLS parameter estimates $\beta_{1P}$ in  [](#p3equationAX2). To define that correlation, I adopt the general structure from[#Ashenfelter1994;]:</Text>
            <Comments>$\beta_{0P}$, $\bm\uptheta_P$ and $\bm\upgamma$ are similarly identifiable post-estimation. $\beta_{1W}$, $\eta$, $\bm\uptheta_W$ and $\bm\updelta$ are not.</Comments>
        </Document>
        <Document ID="53EF818D-92E4-43C2-8C5A-A012C5E98E9A">
            <Title>Proofs</Title>
            <Text>The proof of [](#Theorem1) follows directly from [](#Lemma5), at the end of this section. The proof of [](#Lemma5) relies on a series of additional lemmas stated and proved below. Throughout, $\{(r_{0it},R_{it})\}$ represents the sequence of readability choices made by author $i$ for all $t$. $R_i^\star$ is defined as the $R$ that solves $\phi_i'(R)=c_i'(R)$. Review group $s$ is referred to as "state $s$".
&lt;!--\input{$HOME/Dropbox/Readability/draft/theorems/proofs}--&gt;
&lt;!--\clearpage--&gt;</Text>
        </Document>
        <Document ID="577D95F7-966E-46C5-A4EB-C923F194752D">
            <Title>347 (alastairâs macbook's conflicted copy 2016-10-06)</Title>
            <Text>The data pre- and post-peer review make it possible to identify the contemporaneous effect of gender on peer review by eliminating selection bias---*i.e.*, the effect of omitted variables that are independent of immediate peer review, but correlated with gender. To isolate gender differences in readability pre-existing peer review from those incurred during it, I use two separate (yet analogous) estimation strategies based on a conceptual framework outlined by[#Ashenfelter1994;].  The first strategy removes selection effects by regressing female ratio on the change in score via OLS. In the second strategy, selection effects are eliminated by differencing coefficients on female ratio from a feasible, generalised least squares (FGLS) regression on abstract readability of NBER working papers and, separately, their published versions.
[](#p3equationAX1) defines the linear relationship between readability scores and non-peer reviewed articles. Its components shape writing clarity *before* external evaluators demand edits.
&lt;!--\begin{equation}\label{p3equationAX1}
\text{score}_{jW}^s=\beta_0+\beta_1\,\text{female ratio}_j+\bm\uptheta\,\vect X_j+\mu_j+\vep_{jW},
\end{equation}--&gt;
 where $\text{score}_{jW}^s$ is readability score $s$ for the working paper version ($W$) of article $j$, $\vect X_j$ is a vector of version-invariant variables (publication year and productivity effects) and $\mu_j$ are article fixed effects. $\vep_{jW}$ is version $W$'s unobserved error term.
Published articles are obviously revised versions of earlier papers. Their readability depends on $\text{score}_{jW}^s$ as well as factors that affect writing clarity anytime after drafts are released as working papers. This relationship is shown in [](#p3equationAX2).
&lt;!--\begin{equation}\label{p3equationAX2}
\text{score}_{jP}^s=\text{score}_{jW}^s+\beta_0^P+\beta_1^P\,\text{female ratio}_j+\bm\upzeta\,\vect Z_{jP}+\vep_{jP},
\end{equation}--&gt;
 where $\beta_0^P$ is an additive constant specific to article $j$'s published version ($P$), $\beta_1^P$ reflects an additional impact $\text{female ratio}_j$ may have in peer review and the vector $\vect Z_{jP}$ contains factors that only influence version $P$ (editor, journal and journal-year interaction effects). $\vep_{jP}$ is an error term specific to version $P$; $\vep_{jP}$ and $\vep_{jW}$ are uncorrelated.
The parameter of interest is $\beta_1^P$. $\beta_1^P$ is not separately identified in [](#p3equationAX2); furthermore, OLS estimates are biased by article fixed effects. Subtracting [](#p3equationAX1) from [](#p3equationAX2) deals with both issues. Fixed effects are removed; the coefficient on $\text{female ratio}_j$ is specific to peer review:
&lt;!--\begin{equation}\label{p3equationAX3}
\text{score}_{jP}^s-\text{score}_{jW}^s=\,\beta_0^P+\beta_1^P\,\text{female ratio}_j+\bm\upzeta\,\vect Z_{jP}+\vep_{jP}.
\end{equation}--&gt;
 [](#p3equationAX3) is estimated via OLS.
A second method obtains $\beta_1^P$ in stages. As mentioned, correlation between $\mu_j$ and the observable variables biases OLS parameter estimates of [](#p3equationAX1) and [](#p3equationAX2). To define that correlation, I adopt the general structure from[#Ashenfelter1994;]:
&lt;!--\begin{equation}\label{p3equationAX4}
\mu_j=\eta\,\text{female ratio}_j+\bm\updelta\,\vect X_j+\bm\upgamma\,\vect Z_{jP}+\omega_j,
\end{equation}--&gt;
 where $\omega_j$ is uncorrelated with $\text{female ratio}_j$, $\vect Z_{jP}$ and $\vect X_j$. Substituting [](#p3equationAX4) into [](#p3equationAX1) generates the following reduced form representation of [](#p3equationAX1):
&lt;!--\begin{equation}\label{p3equationAX5}
\text{score}_{jW}^s=\beta_0+\beta_1^\prime\,\text{female ratio}_j+\wt{\bm\uptheta}\,\vect X_j+\bm\upgamma\,\vect Z_{jP}+\vep_{jW}^\prime,
\end{equation}--&gt;
 where $\beta_1^\prime=\eta+\beta_1$, $\wt{\bm\uptheta}=\bm\uptheta+\bm\updelta$ and $\vep_{jW}^\prime=\omega_j+\vep_{jW}$. To obtain a reduced form for published article readability, substitute [](#p3equationAX5) into [](#p3equationAX2):
&lt;!--\begin{equation}\label{p3equationAX6}
\text{score}_{jP}^s=(\beta_0+\beta_0^P)+(\beta_1^\prime+\beta_1^P)\,\text{female ratio}_j+\wt{\bm\uptheta}\,\vect X_j+(\bm\upzeta+\bm\upgamma\vect)\,\vect Z_{jP}+\vep_{jP}^\prime
\end{equation}--&gt;
 where $\vep_{jP}^\prime=\vep_{jW}^\prime+\vep_{jP}$. [](#p3equationAX5) and [](#p3equationAX6) are explicitly estimated via FGLS. $\beta_1^P$ is identifiable post-estimation by subtracting reduced form coefficients.</Text>
        </Document>
        <Document ID="D06F6684-AD5E-4D28-8703-9F31A3CDC3CE">
            <Title>[](#table7), accounting for field</Title>
            <Text>[](#tableXD) replicates the first four columns of [](#table7) but also includes fixed effects for primary *JEL* category. As discussed in [][NBER], the OLS estimates in the final column of [](#table7) already control for field, assuming the impact of a field on readability is the same in the draft version of a paper as it is in the final, published version. Given this assumption, the almost identical FGLS and OLS estimates presented in [](#table7) already made it pretty clear that field did not have an effect on the gender readability gap. [](#tableXD) confirms this. Figures are very similar to those presented in [](#table7).
&lt;!--\input{$HOME/Dropbox/Readability/draft/tex/generated/tableXD}
\clearpage--&gt;</Text>
        </Document>
        <Document ID="D61D4703-AFC6-424B-BADE-B0F242136E96">
            <Title>Summary statistics</Title>
            <Text>As discussed in [][Data], drafts were collected from NBER Technical and Working Paper Series. NBER series were used as the exclusive data source for two reasons. First, approximately one-fifth of articles in the data were originally part of an NBER series, making it the largest single source of draft papers. Second, NBER persistently releases its working papers two to three years before publication (mean 2.1 years)---precisely the length of time spent in peer review[#Ellison2002a,Goldberg2015].
Does peer review alter abstracts? Prior comparisons of medical journal articles published in the *British Medical Journal* suggest yes---although not a lot. [#Hopewell2014;] investigated 93 randomised trials assessing healthcare interventions in human participants published in *BMC*-series medical journals in 2012. A comparison between the original and final submitted manuscript suggested that abstracts were altered in peer review in about 16 percent of all papers, with referees generally asking authors to tone down their conclusions. [#Hayden2008;] found no significant change in the Flesch Reading Ease score during peer review itself (submission vs. acceptance), but a significant positive effect from post-acceptance editing by the journal editor and a copy-editor.
Compared to economics journals, however, medical journals ask for fewer revisions[#Ellison2002a,Hayden2008] and enjoy substantially shorter review times[see, *e.g.*,\]\[][#Trauma2015], suggesting pre-acceptance readability edits are less common. Indeed, as is shown in [](#table6), peer review alters manuscripts by quite a lot.
[](#table6) compares textual characteristics between versions. Means in the first three columns are of majority male-authored papers (female ratio strictly below 50 percent); the final three columns are majority female-authored papers (female ratio at or above 50 percent).
&lt;!--\input{$HOME/Dropbox/Readability/draft/tex/generated/table6}--&gt;
Abstracts are considerably altered during peer review. [](#table6)'s first panel displays raw counts. Draft abstracts are longer---more characters, words and sentences---and denser---more syllables, polysyllabic words and difficult words. The biggest changes are made to female-authored papers: figures in column six are 20--30 percent higher (in absolute value) than those in column three.
Peer review's impact on readability, however, is unclear. Readability scores are weighted averages of the ratios of (i) total word or "hard" word to sentence count and (ii) hard word to word count. Between working paper and published versions, (i) decreases and (ii) increases ([](#table6), second panel). (i) Peer review shortens sentences and reduces hard words per sentence: in male-authored papers, sentences are 5 percent shorter and contain 26 percent fewer polysyllabic words; in female-authored papers, they are 7 percent shorter and contain 30 percent fewer polysyllabic words. (ii) As a fraction of total word count, however, syllables, polysyllabic words and difficult words rise. To wit, hard word counts and total word count decline, but the latter by proportionately more; their ratios increase: between 1--3 percent for men and 1--2 percent for women.
According to the majority of scores, peer review improves readability ([](#table6), third panel), a finding consistent with similar investigations at medical journals[#Biddle1996,Hayden2008,Roberts1994]. Thanks to fewer hard words per sentence, SMOG scores are higher in published articles regardless of gender (see [](#table2)). In female-authored papers, the net effect for remaining scores is similarly positive. In male-authored papers, however, only the Gunning Fog and Flesch-Kincaid scores indicate a positive net effect; for the Flesch Reading Ease and Dale-Chall scores, it's negative. In any case, women's papers endure comparatively greater cuts in hard words relative to total words and larger falls in words per sentence; their abstracts always become more readable during peer review than do those by men.
&lt;!--\input{$HOME/Dropbox/Readability/draft/tex/fixed/figure2}--&gt;
[](#figure2) reiterates women's readability gains. It plots draft Dale-Chall scores ($x$-axis) against abstracts' published scores ($y$ axis) for men (blue) and women (pink). The grey, dashed line is a 45 degree line through the origin. As might be expected, poorly written draft abstracts emerge more readable in the published version (above the 45 degree line); abstracts that were already well written come out slightly less so (below the 45 degree line). Regardless, female-authored published papers are again more readable than they were as working papers relative to male-authored papers---further evidence that women's papers are more heavily scrutinised during peer review.</Text>
            <Comments>A greater decline in total word count relative to hard word count may be specific to abstracts, which are edited for length as well as readability. In an analysis of abstracts, introductions and discussions, abstract sentences were shorter but contained more hard words; overall, they had the lowest Flesch Reading Ease scores[#Hartley2003a].
[#Hayden2008;] found no significant change in the Flesch Reading Ease score during peer review itself (submission vs. acceptance), but a significant positive effect from post-acceptance editing by the journal editor and a copy-editor. Compared to economics journals, however, medical journals ask for fewer revisions[#Ellison2002a,Hayden2008] and enjoy substantially shorter review times[see, *e.g.*,\]\[][#Trauma2015], suggesting pre-acceptance readability edits are less common.
An alternative hypothesis consistent with [](#figure2) is that male-authored papers are scrutinised more, but edits made as a result reduce readability. The more substantial changes made to female-authored papers documented in [](#table6), however, contradicts this theory.</Comments>
        </Document>
        <Document ID="F0A739EF-20B6-42C6-86F4-2312EA990C73">
            <Title>Majority female-authored</Title>
            <Text>In the following tables, article gender is represented by a binary variable equal to one if at least half of all authors are female; articles below that threshold with at least one female author are excluded.
&lt;!--\input{$HOME/Dropbox/Readability/draft/tex/generated/tableC4c}--&gt;
&lt;!--\input{$HOME/Dropbox/Readability/draft/tex/generated/tableC5b}--&gt;
&lt;!--\input{$HOME/Dropbox/Readability/draft/tex/generated/table8XB}--&gt;
&lt;!--\input{$HOME/Dropbox/Readability/draft/tex/generated/table11XC}--&gt;
&lt;!--\input{$HOME/Dropbox/Readability/draft/tex/generated/tableXC}--&gt;
&lt;!--\clearpage--&gt;</Text>
        </Document>
        <Document ID="92C41DA9-4B7F-4A75-9ED9-5FE82D5E7B71">
            <Title>Readability scores</Title>
            <Text>Advanced vocabulary and complicated sentences are two strong predictors of text difficulty[#Chall1995]. Hundreds of formulas exploit this relationship to measure so-called "readability". I concentrate on the most widely used, tested and reliable formulas for adult reading material: Flesch Reading Ease, Flesch-Kincaid, Gunning Fog, SMOG (Simple Measure of Gobbledegook) and Dale-Chall[#DuBay2004]. Each are listed in [](#figure2X).
&lt;!--\input{$HOME/Dropbox/Readability/draft/tex/fixed/table2}--&gt;
The Flesch Reading Ease formula ranks passages of text in ascending order---*i.e.*, more readable passages earn higher scores. The other four formulas generate grade levels estimating the minimum years of schooling necessary to confidently understand an evaluated text---and so more readable passages earn lower scores. To minimise confusion, I multiply the four grade-level scores by negative one. Thus, higher numbers universally correspond to clearer writing throughout this paper.
The constants in each formula vary widely as do the components used to rank vocabulary. Because of these differences, grade-level scores rarely generate identical figures; nevertheless, all five scores produce similar rankings. The yellow box plot in [](#figure0X) summarises 165 inter-score correlations found in 25 separate published studies. The median is 0.84.
Readability scores also correlate with alternative measures of text difficulty, including (i) oral reading fluency, (ii) human judgement, (iii) reading comprehension tests and (iv) the cloze procedure. The dark blue box plots in [](#figure0X) summarise 166 correlations in 43 published cross-validation studies. Readability scores are highly correlated with reading comprehension and cloze tests; they are also strongly associated with rankings based on human judgement. Correlations with oral reading fluency are more moderate.
&lt;!--\input{$HOME/Dropbox/Readability/draft/tex/fixed/figure0X}--&gt;
Other studies have validated readability scores against surrogate measures of reading comprehension. More readable high school and college-level correspondence courses have higher completion rates[#Klare1973]. More readable academic journals enjoy larger readership[#Richardson1977,Swanson1948]; their most readable articles win more awards[#Sawyer2008], are downloaded more often[#Guerini2012] and cited more frequently (see [](#figure0X)).
Thanks to high predictive validity and ease of use, readability formulas are widely employed in education, business and government. The U.S. Securities and Exchange Commission encourages clearer financial disclosure forms benchmarked against the Gunning Fog, Flesch-Kincaid and Flesch Reading Ease scores[#Cox2007]. The formulas have also guided readability assessments of, *inter alia*, standardised test questions[#Chall1977,Chall1983], medical inserts[*e.g.*,\]\[][#Wallace2008], technical manuals[*e.g.*,\]\[][#Hussin2012,Klare1973], health pamphlets[*e.g.*,\]\[][#Foster2002,Meade1989] and data security policies[#Alkhurayyif2017].
In research, readability scores are considered objective proxies for "complexity". [#Enke2018;] controls for language sophistication using the Flesch Reading Ease formula in a study of moral values in U.S. presidential elections. [#Spirling2016;] employs the same score to show that British parliamentarians simplified speeches to appeal to less educated voters in the in the wake of the Great Reform Act. Legal research has found that judges are more reliant on legislative history when interpreting complex legal statutes, as measured by the Flesch-Kincaid formula[#Law2010]. And in finance, various scores have linked the clarity of financial communication materials to better firm and market financial health[#Li2008,Biddle2009,Jansen2011], larger investment and trading volume [#Miller2010,ThÃ¶rnqvist2015,DeFranco2015,Lawrence2013] and lower demand for---albeit higher reliability of---outside research by sell-side analysts[#Lehavy2011].</Text>
            <Comments>Included in this sample are between-score correlations found in two non-published studies---the present paper (correlations range from 0.53 to 0.97) and [#Benoit2017;].
Generally measured as the number of words read aloud correctly per minute.
The cloze procedure deletes random words in passages of text and ranks their difficulty according to average readers' ability to correctly enter the missing words.
See [][AppendixMetaAnalysis] for a brief overview of the studies included in [](#figure0X) and a description of the criteria used to select them.
In a [blog post](http://lukaspuettmann.com/2017/12/09/voxeu-gobbledygook/), Lukas PÃ¼ttmann evaluates the readability of the abstract text of columns published on the [VoxEU.org](http://www.voxeu.org) website and compares them to the number of times that article was viewed. He finds more readable columns are read about 3 percent more often[#Puttmann2017].
Evidence linking readability scores and citations in other studies is weaker. [#Lei2016;] find a positive yet non-significant relationship between readability in information journals and citations. [#Berninger2017;] find a positive correlation between abstract text and citations yet a negative correlation with the text of the actual paper and citations. [#Guerini2012;] find that the most readable academic articles are downloaded the most, although their connection to citations is positive, but less important.
[#Bischof2018;] similarly show a positive link between plain language in political speeches and a desire to appeal to voters using German political manifestos and the BjÃ¶rnssonâs readability index. Their study also finds evidence that voters better understand more readable political messages.came to a similar conclusion using German political manifestos and the BjÃ¶rnssonâs readability index. Their study also finds evidence that voters better understand more readable political messages.
[#Long2011;] investigate whether a legal brief's readability score correlates with its success on appeal, but find that they do not.
See [#Loughran2016;] for a thorough review of the use of readability measures in finance and accounting research.</Comments>
        </Document>
        <Document ID="E8C2F9DA-7160-4BE1-8B89-4DA8E012BE0D">
            <Title>Front Matter</Title>
        </Document>
        <Document ID="6E035480-E9AF-4C01-8570-5ED522C3159C">
            <Title>Appendices</Title>
        </Document>
        <Document ID="2849EBC4-661D-43EE-8C6A-9DA357D9AD13">
            <Title>Condition 1</Title>
            <Text>A readability gap could emerge from underlying gender differences in $u_i$: ambitious authors or those strongly averse to being rejected probably work harder to secure publications. Nevertheless, they eventually learn readability's true impact on rejection, so $i$'s unconditional probability of acceptance must be higher than $k$'s for $u_k&lt;u_i$ to produce $R_{kt}&lt;R_{it}$ indefinitely.
I cannot use the data analysed in this paper to test for Condition 1. Luckily, gender's impact on acceptance rates has been extensively studied elsewhere. To the best of my knowledge, publication outcomes expose no female advantage anywhere, ever. For example, [][#Blank1991;] found that acceptance rates for male- and female-authored papers were 12.7 and 10.6 percent, respectively, at the *American Economic Review*. At *JAMA*, 44.8 percent of referees accept male-authored papers as is or if suitably revised; 29.6 summarily reject them. Corresponding figures for female-authored papers are [38.3 and 33.3][#Gilbert1994]. [][#Ceci2014;] comprehensively review other research on the subject. Their conclusion: "When it comes to actual manuscripts submitted to actual journals, the evidence for gender fairness is unequivocal: There are no sex differences in acceptance rates" [\]\[p. 111][#Ceci2014].</Text>
            <Comments>[][#Blank1991;] found that the double-blind acceptance rate for female-authored papers was 10 percent (versus 11 percent for men); the single-blind acceptance rate was 11.2 percent (versus 15 percent for men).
[][#Gilbert1994;] broke down reviewer recommendations by content reviewer and statistical reviewer. The figures presented here aggregate their responses. Note also that with respect to the actual decision, manuscripts with male corresponding authors were more likely to be summarily rejected (41.7 percent as opposed to 37.4 percent); however, there was no gender difference in overall acceptance rates.</Comments>
        </Document>
        <Document ID="E915A4FA-023A-4914-82FC-7AE0809A6829">
            <Title>[](#table6), equal weighting</Title>
            <Text>In order to create author time series, article-level data were duplicated by article $j$\'s co-author count, $N_j$. Each duplicate observation was assigned a single author. [](#table6) weights authors' observations by $1/N_j$---papers with fewer co-authors are weighted more heavily because they've been duplicated fewer times. [](#tableD13) weights all author-level observations equally. Doing so does not meaningfully affect the analysis.
&lt;!--\input{$HOME/Dropbox/Readability/draft/tables/tex/tableD13}--&gt;</Text>
            <Notes>&lt;!--\clearpage--&gt;</Notes>
        </Document>
        <Document ID="175B78D7-2A1B-4F7D-8780-739D8F2651D6">
            <Title>At least one female author</Title>
            <Text>In the following tables, papers authored entirely by women are compared to papers authored entirely by men.
&lt;!--\input{$HOME/Dropbox/Readability/draft/tex/generated/tableC4b}--&gt;
&lt;!--\input{$HOME/Dropbox/Readability/draft/tex/generated/table8XA}--&gt;
&lt;!--\input{$HOME/Dropbox/Readability/draft/tex/generated/tableC5a}--&gt;
&lt;!--\input{$HOME/Dropbox/Readability/draft/tex/generated/table11XB}--&gt;
&lt;!--\input{$HOME/Dropbox/Readability/draft/tex/generated/tableXB}--&gt;
&lt;!--\clearpage--&gt;</Text>
        </Document>
        <Document ID="F3385050-0BC3-4C1D-BD5F-ADBA3AA4E758">
            <Title>English as a native language</Title>
            <Text>In this section, I conduct a primitive surname analysis to determine whether female economists are more (or less) likely to speak English natively. To proxy for native language, I construct a binary variable equal to one if an author's last name is shared with at least 100, 1,000, *etc.* people in the U.S., according to the 2000 Census. Given historical immigration to the U.S., I supplement the analysis with an analogous indicator based on popular Scottish surnames during 1975--2015 (shared by 10 or more people); data are from the National Records of Scotland.
[](#tableA1) displays correlations between the various surname popularity variables. Note the substantial overlap between Scottish and U.S. surnames shared by 1,000--100,000 people.
&lt;!--\input{$HOME/Dropbox/Readability/draft/tables/tex/tableA1}--&gt;
[](#tableA2) displays coefficients on $\text{male}_i$ from a probit regression with the surname indicators as dependant variables. In the first column, male authors are slightly more likely to have popular U.S. and Scottish last names, although figures are statistically significant for very popular American surnames, only. [](#tableA2)\'s second column includes fixed effects for the first year in which an author published in the data. Year effects are meant to control for changes in surname popularity over time---*e.g.*, due to immigration---that might be correlated with authors' gender. Indeed, their inclusion eliminates gender's impact; column two suggests men and women are equally likely to be native-English speakers.
&lt;!--\input{$HOME/Dropbox/Readability/draft/tables/tex/tableA2}--&gt;
&lt;!--\clearpage--&gt;</Text>
            <Comments>It is not clear how---or even if---native English speakers write more clearly than non-native speakers. In fact, [#Hayden2008;] found peer reviewed articles by the latter actually *more* readable, on average.
I use Scottish in lieu of U.K. data because only popular 1911 surnames are available from the latter. (British Census data are first publicly released 100 years after being collected.)</Comments>
        </Document>
        <Document ID="13535585-A497-4FFA-B94B-30B8D25B5E5E">
            <Title>Data</Title>
        </Document>
        <Document ID="BC5A83FE-B0B1-43B8-B751-8B8DD5CA66A5">
            <Title>[](#table10X), including Condition 1</Title>
        </Document>
        <Document ID="252AE959-2A15-4A79-992A-A83F2FDC3EEB">
            <Title>Untitled</Title>
        </Document>
        <Document ID="51B6F600-C7F0-401E-A9C3-5CFABD9D24FC">
            <Title>Firm-level data</Title>
            <Text>To account for credit-rationing and control for firm-specific characteristics, I use data from the World Enterprise Surveys (WES). The WES are a series of company surveys conducted by the World Bank and the European Bank for Reconstruction and Development (EBRD). They cover business constraints in emerging and developing markets. The surveys start in 2005 and are generally repeated every three years. They contain 117,105 observations from 135 countries.
One question asks the degree to which access to finance---including interest rates, fees and collateral---is an obstacle to business operations. Answers range from 0 (no obstacle) to 4 (very severe obstacle). I define $\text{obstacle}_i$ as a binary variable equal to 1 if firm $i$'s response was moderate (2) or above---roughly half the sample.
Additionally, I control for several firm-specific characteristics. $\text{Age}_i$ is the difference between the year firm $i$ was established and the year in which the survey took place. $\text{Size}_i$ distinguishes between small (fewer than 20 employees), medium and large (more than 100 employees) companies in ascending order from 0 to 2. $\text{Exporter}_i$ is a binary variable equal to 1 if the firm exports all or part of its output; $\text{foreign-owned}_i$ and $\text{state-owned}_i$ equal 1 if the firm is partly or wholly foreign- or state-owned, respectively. $\text{Manufacturing}_i$ equals 1 if firm $i$ operates in the manufacturing industry.
[](#p2table1)summarises these variables. Surveyed firms tend to be small, privately owned operations that produce for the domestic market. Export status, foreign ownership and size are the same for firms reporting finance as an obstacle compared to those that don't. State-owned firms are less likely to find financing an obstacle, but the difference is slight. Younger firms and those in manufacturing, however, have a significantly tougher time accessing finance than their older, service-oriented peers.</Text>
            <Comments>The WES were conducted prior to 2005 but earlier surveys did not report firm size or employ a consistent sampling methodology.
Fully state-owned enterprises are not surveyed.
\input{$PPATH/p2/tables/tex/p2table1}</Comments>
        </Document>
        <Document ID="99EA8132-17DD-487B-AD1E-32721974321C">
            <Title>269 (erinâs imac's conflicted copy 2016-05-01)</Title>
            <Text>[](#p3table3)displays each gender's average per sentence number of characters, words, syllables, polysyllabic words and difficult words. By all measures, women write shorter, simpler sentences---they contain fewer characters, fewer syllables, fewer words and fewer "hard" words. Differences are highly significant.
[](#p3table4)presents coefficients from an ordinary least squares regression of ratio of female co-authors on the five readability scores. To account for error correlation by editorial policy, observations are grouped by journal editor/editorial board and standard errors are adjusted accordingly.
Column (1) controls for journal: abstracts written only by women score about one point higher on the Flesch Reading Ease scale; according to the four grade-level measures, they take 1--6 fewer months of schooling to understand. Percentage-wise, women write between 1--3 percent better than men.
Column (2) includes 63 year dummies; column (3) adds another 182 journal and year interaction dummies; column (4) introduces the 98 institution dummies. Controlling for time and institution has little effect. Coefficients and standard errors are very similar to those in the first column.
The coefficients on the journal dummies in (2) are presented in [](#p3table5).They contrast the readability of the *American Economic Review* with *Econometrica*, *JPE* and *QJE*, providing a useful check on the reliability of readability formulas in the context of economic writing. As intuitively anticipated, all five scores agree that *Econometrica* is harder to read; the Flesch Reading Ease, Flesch-Kincaid, Gunning Fog and SMOG scores suggest the *JPE* is too; according to the Flesch Reading Ease, Gunning Fog, SMOG and Dale-Chall scores, *QJE* is easier.
Column (5) controls for primary *JEL* classification. Since only post-1990 *JEL* classifications are used, estimates in (5) exclude roughly 40 percent of the data. Nevertheless, coefficients are roughly equivalent---with the exception of the Flesch Reading Ease score which halves and is no longer significant at the 10-percent level.
[](#p3figure4)displays the results from an ordinary least squares regression on the Dale-Chall score; regressors are: (i) ratio of female co-authors; (ii) dummies for each primary *JEL* code, (iii) interactions from (i) and (ii) and (iv) controls for journal, year, institution and editorial board. Due to small sample sizes---particularly of female authors---[](#p3figure4) includes 549 articles from *AER Papers &amp; Proceedings*.
The pink vertical line in the left-hand graph is the mean marginal effect of the ratio of female co-authors: in line with estimates from [](#p3table4), female-authored papers require about six fewer weeks of schooling to understand and the estimate is highly significant (standard error 0.04). Points on the graph are marginal effects of the ratio of female co-authors across *JEL* classification; bars represent 90 percent confidence intervals (standard errors clustered by editorial board). Women ear higher marks for readability in 12 out of 15 categories; but only three are significant on their own: Q (Agricultural and Natural Resource Economics; Environmental and Ecological Economics), N (Economic History), and J (Labour Economics). In no field do men write significantly better than women.
[](#p3table4)'s right-hand graph displays coefficients on the interactions between ratio of female co-authors and each *JEL* code. Q and N are significantly above the mean; O (Economic Development, Innovation, Technological Change, and Growth) and H (Public Economics) are significantly below it. The effect of ratio of female co-authors in the other categories does not differ from the mean effect.
In general, sample sizes are small and estimates are are highly imprecise---only Labour Economics and Microeconomics contain more than 100 papers written only by women; the others average 35. Nevertheless, [](#p3figure4) suggests the gender gap in readability is not being driven by an outlier field.
[](#p3figure4) also suggests that more women in a field is not necessarily an antidote to gender differences in readability. Economic history has one of the highest concentrations of female-authored papers; Agriculture/Environment one of the lowest; Labour Economics falls between the two. Admittedly, Economic History papers are still overwhelmingly---as in 74 percent---penned just by men. But given the readability gap is present in subfields with both above- and below-average rates of sole female authorship, women may need to be better writers even where more of them publish.
To investigate the gender readability gap at the author-level, I disaggregated the data by duplicating each article $n_j$ times, where $n_j$ is article $j$'s number of co-authors, assigned observation $j_k\in\{1,\ldots,n_j\}$ article $j$'s $k\text{th}$ author and estimated [](#p3eqSUP6):where $\text{score}_{j_{it}}^s$ is readability score $s$ for article $j$---the article corresponding to author $i$'s $t$th publication. Gender enters twice via the binary variable $\text{male author}_i$ and $\text{ratio of female co-authors}_{j_{it}}$ to account for author $i$'s sex (zero for women, one for men) as well as the sex of his co-authors, respectively. $\text{No. co-authors}_{j_{it}}$ is article $j$'s total number of co-authors and controls for author $i$'s proportional contribution to his $t$th paper. $\alpha_i$ are author-specific effects, $\vect X_{j_{it}}$ is a vector of editor, journal, year and institution dummies and $\varepsilon_{j_{it}}$ is an i
[](#p3table6)displays results. The first panel estimates [](#p3eqSUP6) using a Thanks to short panels---again, particularly for women---the sample includes 549 articles from *AER Papers &amp; Proceedings* (see [](#fn1)). To account for duplicated observations, the regression is weighted by $1/n_j$. Standard errors adjusted for two-way clustering at the editorial board and author levels.</Text>
        </Document>
        <Document ID="CA03D0ED-0104-442B-9711-F2B3AEB42E3D">
            <Title>Article-level analysis</Title>
            <Text>[](#table3) displays each gender's average per sentence number of characters, words, syllables, polysyllabic words and difficult words. Women write shorter, simpler sentences---they contain fewer characters, fewer syllables, fewer words and fewer "hard" words. Differences are highly statistically significant.
[](#table4) presents coefficients from an ordinary least squares (OLS) regression of the ratio of female co-authors on the five readability scores. To account for error correlation by editorial policy, observations are grouped by journal editor/editorial board and standard errors are adjusted accordingly.
&lt;!--\input{$HOME/Dropbox/Readability/draft/tex/generated/table3}--&gt;
Column (1) controls for journal and editor: abstracts written only by women score about one point higher on the Flesch Reading Ease scale; according to the four grade-level measures, they take 1--6 fewer months of schooling to understand. Percentage-wise, women write 1--2 percent better than men.
Column (2) includes 63 year dummies; column (3) adds another 182 journal and year interaction dummies; columns (4) and (5) introduce 64 institution effects, quality controls---citation count and 30 $\text{max. }T_j$ effects (maximum co-author lifetime publication count for paper $j$)---and a dummy variable capturing English fluency. Coefficients and standard errors in columns (2)--(5) are very similar to those in column (1).
The coefficients on the journal dummies in (2) are presented in [][AppendixArticleMale]. They compare *AER*'s readability to the readability of *Econometrica*, *JPE* and *QJE*, providing a useful check on the reliability of readability formulas in the context of economic writing. As intuitively expected, all five scores agree that *Econometrica* is harder to read; four out of five scores suggest *JPE* is, too, while *QJE* is easier.
&lt;!--\input{$HOME/Dropbox/Readability/draft/tex/generated/table4}--&gt;
Columns (6) and (7) control for primary *JEL* classification. (6) includes 20 fixed effects for primary *JEL* categories; (7) includes 718 effects for tertiary categories. Due to small sample sizes, (7) includes 561 articles from *AER Papers &amp; Proceedings*. Since only post-1990 *JEL* classifications are used, estimates in both columns exclude over 40 percent of the data. Nevertheless, coefficients and standard errors are roughly equivalent, suggesting the readability gap is largely independent of sub-field. [][AppendixJEL] explores this hypothesis in more detail. Conditional on the other explanatory variables, however, there is very little evidence that the gender readability gap is related to sub-fields.</Text>
            <Comments>Standard errors are very similar when clustering at the volume-, issue- or paper-level[see\]\[p. 39--41][#Hengel2016].
Coefficients from regressions on Flesch-Kincaid, Gunning Fog, SMOG and Dale-Chall scores represent the marginal effect in years of schooling. Monthly figures found by multiplying each coefficient by 12.
Quotient of the coefficient on female ratio divided by the effect for men (ratio of zero) estimated at other co-variates' observed values (see [][AppendixArticleMale]).
In [\]\[p. 44 and p. 46][#Hengel2016;], I include controls for the order an article appears in an issue---another measure of a paper's quality. Results are similar to those in [](#table4). In addition to the control from English fluency presented here, see[\]\[pp. 35--36][#Hengel2016;] for further evicence that the female authors in my data are no more or less likely to be native English speakers.
&lt;!--\label{footnote34}--&gt;*AER Papers &amp; Proceedings* is coded as a separate journal and edited by the American Economic Association's president-elect. *AER Papers &amp; Proceedings* does not publish abstracts in its print version; only select years and papers are available online (2003 and 2011--2015), all of which are included. Excluding these articles does not impact results or conclusions---coefficients are almost identical to those in column (6), but standard errors are somewhat higher. (Analysis not shown, but is available on request: &lt;!--\href{mailto:erin.hengel@gmail.com}{\texttt{erin.hengel@gmail.com}}--&gt;.)</Comments>
        </Document>
        <Document ID="489D7173-3DB1-4F9E-9617-148B45E330DC">
            <Title>Recovered Files (3 Dec 2017 at 16:59)</Title>
        </Document>
        <Document ID="80CBCD83-081D-4233-ACA7-DA39005D272C">
            <Title>Tables</Title>
            <Text>The following tables display full output used to generate marginal effects in [][p3Results] (for space considerations, [](#p3table8)'s full output is not provided).</Text>
            <Comments>\input{$P3/tables/tex/table4a_1.tex}
\input{$P3/tables/tex/table4a_2.tex}
\input{$P3/tables/tex/table4a_3.tex}
\input{$P3/tables/tex/table4a_4.tex}
\input{$P3/tables/tex/table4a_5.tex}
\input{$P3/tables/tex/table5a.tex}
\input{$P3/tables/tex/table6a.tex}
\input{$P3/tables/tex/table9a_1.tex}
\input{$P3/tables/tex/table9a_2.tex}
\input{$P3/tables/tex/table10a_1.tex}
\input{$P3/tables/tex/table10a_2.tex}</Comments>
        </Document>
        <Document ID="1CA292A5-32B3-4C5A-969D-EC3DBF1AE137">
            <Title>Notes</Title>
        </Document>
        <Document ID="3DA20168-ED59-4A60-BCE5-FF0135174308">
            <Title>Supplemental summary statistics</Title>
        </Document>
        <Document ID="DAEFBE26-2900-4C78-AD90-93AAA4B9DB95">
            <Title>Determining gender</Title>
        </Document>
        <Document ID="2608C4D4-3A6F-4D5D-80BD-764A4848ACBA">
            <Title>Gender</Title>
            <Text>Authors were assigned a gender using [GenderChecker.com](http://genderchecker.com)'s database of male and female names. Authors with unisex first names, first names not in the database or those identified only by initial(s) were assigned gender either by me, a research assistant or at least three separate [Mechanical Turk](http://www.mturk.com) workers based on a visual inspection of photos on faculty websites, Wikipedia articles, *etc.* or personal pronouns used in text written about the individual. In situations where the author could not be found but several people with the same first and last name were and all shared the same gender, the author was also assigned that gender. For the remaining cases, I emailed or telephoned colleagues and institutions associated with the author.
Determining the "gender" of a paper is not nearly as straightforward. For solo-authored papers---of which there are 3,747 in the sample---gender corresponds to the sex of the author. Unfortunately, top economics journals have collectively published just 269 by women. Only a slightly larger number were written entirely---*or even mostly*---by [women](#figure0). Proportions are similar when the sample is restricted to later years: *QJE* did not publish a single exclusively female-authored paper between 2015--2017 (inclusive); in four of the last fifteen years covered by the data (2001--2015), *Econometrica* and *JPE* didn't either.
&lt;!--\input{$HOME/Dropbox/Readability/draft/tex/fixed/figure0}--&gt;
A greater number of papers (1,176) are authored by at least one woman. To take advantage of the information contained in this larger sample, [#Blank1991;] classified all such papers as "female". I opt instead for a less inclusive and continuous measure of gender: the proportion of female authors on a paper.
This approach assumes a linear relationship between a paper's readability and its gender composition. (As shown in [][Authorlevel], however, it may be increasing and convex.) As a robustness check, I have repeated most of the analysis (a) on the subset of papers co-authored entirely by a single gender; (b) using a binary variable equal to one if at least one author is female; and (c) using a binary variable equal to one if at least half of all authors are female. Standard errors from (a) tend to be larger; those from (b) and (c) are usually smaller. Result and conclusions are otherwise [unchanged][AppendixAlternativeMeasure].</Text>
            <Comments>315 papers in the sample were authored entirely by women. Women made up more than 50 percent of all authors in another 47. An additional 35 observations have a female lead authors---*i.e.*, the first author was female in a paper with authors listed non-alphabetically or in which contributions were explicitly noted.
This decision was made on the assumption that a gender readability gap---if it exists---is function of (i) the probability a particular portion of analysed text was written and/or revised by a female co-author; and (ii) referees' beliefs about female authors' contributions to the writing of a paper. I assume the intersection of (i) and (ii) is positively related to the ratio of female authors on a paper. This assumption is supported by prior research suggesting that co-authors---regardless of seniority---tend to share responsibility for writing and revising collaborative work[#Hart2000,Kumar2016]. [CONFIRM REFERENCE]</Comments>
        </Document>
        <Document ID="B236ED0E-EF88-4EC3-91AC-E4554A34EC5C">
            <Title>[](#table11XX), Hausman test statistics</Title>
            <Text>[](#table11AXX) displays $\chi^2$ statistics from a Hausman test between fixed and random effects. (Random effects model shown in [](#table11XX).) In all specifications, the null-hypothesis (no systemic differences between models) is not rejected.
&lt;!--\input{$PPATH/tables/tex/table11AXX}--&gt;</Text>
        </Document>
        <Document ID="3595D8FE-749A-4AED-BBAF-7FDFA16280A3">
            <Title>[][SEUMatching], list of matched pairs</Title>
            <Text>[](#tableC14) displays the names of the economists in each matched pair.
&lt;!--\input{$HOME/Dropbox/Readability/draft/tex/generated/tableC14}--&gt;
&lt;!--\clearpage--&gt;</Text>
        </Document>
        <Document ID="3BF9D3D3-7E94-45F7-9978-1132940BDF45">
            <Title>Introduction-1</Title>
            <Text>Prior research indicates journal acceptance rates are genuinely bias-free [see, *e.g.*,\]\[][#Blank1991,Borsuk2009,Gilbert1994]. To the best of my knowledge, however, gender neutrality is established in only a narrow context (publication outcomes) using this single indicator. I ask a different question. Men's and women's papers may be published at comparable *rates*, but are they reviewed with comparable *scrutiny*? For, if women are stereotypically assumed less capable at math, logic and reasoning than men and generally need more evidence to rate as equally competent, some well-intentioned referees might (unknowingly) inspect their papers more closely, demand a larger number of revisions and, in general, be less tolerant of complicated, dense writing.
Complicated, dense writing is my focus. In the English language, more clearly written prose is better prose, all things equal. Thoughtful word choice and simple sentence structure make text easier to understand, more interesting to read and expose inconsistencies long-winded writing often hides. Journal editors tend to agree---*Econometrica* asks authors to write "crisply but clearly" and to take "the extra effort involved in revising and reworking the manuscript until it will be clear to most if not all of our readers" ([*Econometrica* submission guidelines](http://www.econometricsociety.org/publications/econometrica/information-authors/instructions-submitting-articles), June 2016).
If referees hold female- and male-authored papers to identical standards, both should be equally well written. To test this, I rely on a relationship familiar to linguists and educators: simple vocabulary and short sentences are easier to understand and straightforward to quantify. Using the five most widely used, studied and reliable formulas to exploit this, I analyse 9,123 article abstracts published in the *American Economic Review* (*AER*), *Econometrica* (*ECA*), *Journal of Political Economy* (*JPE*) and *Quarterly Journal of Economics* (*QJE*).
Female-authored abstracts are 1--6 percent more readable than those by men. Women write better despite controls for editor, journal, year and primary and tertiary *JEL* classification; that remains unchanged when proxying for article and author quality or accounting for English fluency. This means the readability gap probably wasn't (i) a response to specific policies in earlier eras; (ii) caused by women writing on topics that are easier to explain; (iii) due to a lopsided concentration of (non-)native English speakers; nor (iv) generated by factors correlated with gender but really related to knowledge, intelligence and creativity.
Additionally, the gender readability gap widens *during* peer review. I compare National Bureau of Economic Research (NBER) working papers to their final, published versions; the gap is almost twice as large for the latter. While both papers are exposed to many factors that impact readability, only published articles are subject to peer review. By comparing the two, influences unrelated to immediate peer review are isolated from those that are; assuming the former are not correlated with the latter's timing, a widening gap suggests a causal link. Consistent with this hypothesis, I also find no evidence of a gender readability gap in the (small) number of articles subjected to double-blind peer review before the internet.
Revising, redrafting and selecting just the right word is hard work; making sentences even marginally more readable takes time. Consistent with this hypothesis, I find female-authored papers spend ***six months longer*** in peer review. This estimate is based on submit-accept times from *Econometrica*, persists across a range of specifications and, in addition to other factors, controls for motherhood,  childbirth, citations and field.
Two explanations could account for these findings: either women voluntarily write better papers---*e.g.*, because they're more sensitive to referee criticism or overestimate the importance of writing well---or better written papers are women's response to external thresholds they do not control. Both imply women spend too much time rewriting old papers and not enough time writing new papers---but my evidence suggests the latter is primarily to blame.
In a dynamic model of an author's decision making process, I show that if women improve their writing over time and are not commensurately rewarded with higher acceptance rates (relative to men), then a persistent readability gap between equivalent peers is caused by discrimination. Authors improve readability only if they believe better writing leads to higher acceptance rates. And while oversensitivity and/or poor information may distort their beliefs---and affect readability---the impact declines with experience. Holding acceptance rates constant, this implies that a widening readability gap between equivalent authors is caused by discrimination---*i.e.*, asymmetric editorial standards and/or biased referee assignment beyond their [control](#Theorem1).
[](#Theorem1) establishes sufficient conditions to demonstrate double standards are present in academic publishing: (i) experienced women write better than experienced men; (ii) women improve their writing over time; (iii) female-authored papers are accepted no more often than male-authored papers. Estimates from pooled subsamples at fixed publication counts suggest (i) and (ii) hold. On average, women's writing gradually gets better but men's does not. Between authors' first and third published articles, the readability gap increases by up to 12 percent. Although my data do not identify probability of acceptance, conclusions from extensive study elsewhere are clear: "there are no sex differences in acceptance rates."[\]\[p. 111; see also [][SEUMatching] for references to other research supporting this claim][#Ceci2014].
I also match prolific female authors to similarly productive male authors on characteristics that predict the topic, novelty and quality of research. In addition to explicitly accounting for author equivalence---the (principle) conditional independence assumption behind [](#Theorem1)---matched pair comparisons: (i) identify the gender most likely to satisfy [](#Theorem1)'s conditions simultaneously; and (ii) generate a (conservative) estimate of the effect of higher standards on authors' [readability](#Corollary1).
[](#Theorem1)'s conditions were satisfied in 65 percent of matched pairs. In three quarters of those, the member discriminated against was female. Moreover, instances of obvious discrimination were predominately against women: the estimated effect of higher standards was almost twice as large in pairs suggesting discrimination against women; it clustered near zero for the small minority of pairs indicating discrimination against men. On average, higher standards cause senior female economists to write at least nine percent more clearly than they otherwise would.
Asymmetric editorial standards and/or biased referee assignment affect women directly---as already discussed, women write more readably during and spend longer in peer review. They probably affect women's behaviour indirectly, too. As a final exercise, I compare papers pre- and post-review over increasing publication counts. In authors' earliest papers, the readability gap exclusively emerges during peer review; there is no gender difference in the draft readability of authors' first top publication. In later papers, women write well upfront; the gap chiefly materialises before peer review. Thus, female economists appear to adapt to higher standards *in* peer review by writing more clearly *before* peer review.</Text>
            <Comments>A possible exception is *Behavioral Ecology*, which increased its number of female first-authored papers after switching to double-blind review in 2001[#Budden2008a]. Whether that increase was due to bias or the universal upward trend in female authorship, however, has been somewhat controversial[#Budden2008b,Budden2008c,Webb2008,Whittaker2008].
The *American Economic Review* rejected Robert Lucas's paper "Expectations and the Neutrality of Money" for insufficient readability; one referee wrote "If it has a clear result, it is hidden by the exposition"[\]\[p. 172][#Gans1994]. In a random selection of 100 posts on [Shit My Reviewers Say](http://shitmyreviewerssay.tumblr.com/submit), a quarter deal with writing quality, document structure or word choice/tone.
Readability scores are highly correlated across an article's abstract, introduction and discussion sections[#Hartley2003a]. See [][Data] for further discussion.
For a discussion on the reliability of readability formulas, see [#DuBay2004;] and [][MeasuringReadability]. A sixth commonly used measure is the Lexile Framework. Because its formula and software are proprietary, I do not include it in the analysis.
It is not clear how---or even if---native English speakers write more clearly than non-native speakers. In fact,[#Hayden2008;] found that peer reviewed articles by the latter are more readable, on average.
Many thanks to Kevin Schnepel for suggesting this idea.
Predictably, giving birth slows down peer review. The coefficient on motherhood, however, is consistently negative (indicating a productivity boost) and almost always highly significant when subjected to several robustness checks. This result is provocative and discussed in [][Duration]. I encourage interpreting it with caution, however, given (i) counterintuitive results; (ii) the analysis did not intend to measure motherhoodâs impact on review times; and especially (iii) only a small number of mothers with young children are published in *Econometrica*.
Each of [](#Theorem1)'s conditions must technically hold for the same author in two different situations---before and after gaining experience and when compared to an equivalent, experienced author of the opposite gender.
While nine percent seems small, it is based on a single paragraph. Assuming a similar standard applies to every paragraph in a paper and improving each one takes slightly more time, the accumulated impact may be substantial. See also[#Berk2017;] for a general discussion on how current culture may encourage extraneous (and time-consuming) demands in otherwise publishable papers.</Comments>
        </Document>
        <Document ID="0D157A8A-4F7E-4FAC-BA03-572162E1537B">
            <Title>[](#table7) (first column), full output</Title>
            <Text>[](#tableC6) estimates [](#equation2) via OLS. The first row displays coefficients on the working paper score, $R_{jW}$. The second row is the coefficient on female ratio ($\beta_{1P}$), also shown in the first column of [](#table7). Remaining rows present estimated coefficients from the other (non-fixed effects) control variables: $\text{Max. }t_j$ and $\text{Max. }T_j$---contemporaneous and lifetime publication counts for article $j$'s most prolific co-author, respectively---number of citations and a dummy variable equal to one if article $j$ is authored by at least one native (or almost native) English speaker.
&lt;!--\input{$HOME/Dropbox/Readability/draft/tex/generated/tableC5}--&gt;
&lt;!--\clearpage--&gt;</Text>
        </Document>
        <Document ID="6EA6F09A-3D8E-4793-9BF9-CC0684F77E53">
            <Title>[](#table8), equality test statistics and male effects</Title>
            <Text>[](#tableC6) displays $\chi^2$ test statistics from Wald tests of [$\beta_1$](#equation1) equality across estimation results in [](#table8). 
&lt;!--\input{$HOME/Dropbox/Readability/draft/tex/generated/tableC6}--&gt;
&lt;!--\clearpage--&gt;</Text>
        </Document>
        <Document ID="B7F0DE17-4271-4D50-93F7-078BAD727EC4">
            <Title>Summary statistics</Title>
        </Document>
        <Document ID="69D1654F-12AF-4843-8260-D48E333BB8F8">
            <Title>\\(Y\\) and \\(p\\)</Title>
            <Text>To measure $Y$ and $p$, I use World Bank Doing Business data. Doing Business indicators cover the cost and complexity of complying with business-related administrative procedures and regulations. They are estimated based on responses to a set of standardised, hypothetical scenarios posed to accountants, lawyers and other relevant local experts. 
The Doing Business's "cost of enforcing contracts" proxies for $Y$. It aggregates attorney, court and enforcement fees needed to resolve a commercial dispute. The final figure reflects those costs as a percentage of the claim in question.
For $p$, I construct an index of the depth, coverage and quality of financial information available on individuals and firms in a country. It is calculated per [](#p2equation1):$\text{Public registry}_{jt}$ is the Doing Business's "credit registry coverage"---the percentage of individuals covered by a public data registry for country $j$ in year $t$. $\text{Private bureau}_{jt}$ is its "credit bureau coverage"---the corresponding coverage of a private data bureau. $\text{Depth of credit info. index}_{jt}$ is the Doing Business's "depth of credit information index". It measures the comprehensiveness of credit-related data in the economy; $S_t$ divides by 6 for years prior to 2013 and 8 thereafter to account for methodological changes.
[](#p2table3)summarises these variables. On average, resolving a financial dispute costs 36.7 percent of the underlying claim in the countries surveyed by the WES, slightly above the 2014 global average of 33.5 percent. Values range between 0 and 1.63. The latter distinction goes to East Timor in 2009; the former to Bhutan that same year.
The credit information index ranges between 0 and 1. In a given year, roughly half of the WES countries have the minimum value. Argentina scored 1 in 2006 and 2010; Honduras and Uruguay in 2010. The mean is 20.6 percent---almost seven points lower than the 2014 global average (27.7 percent).  This difference mainly reflects limited private bureau coverage in WES countries (global average 27.3 percent) and, to a lesser extent, a lower rating on the depth of credit information index (global average 49.0). WES countries surpass the global average in public registry coverage (10.1 percent).</Text>
            <Comments>Doing Business also publishes a measure of the average cost of bankruptcy proceedings. Unfortunately, the data is based on a highly stylised case study of a viable firm and is calculated conditional on  specific bankruptcy procedures without explicitly mentioning what those procedures are. The cost of enforcing contracts, on the other hand, records many of the same fees associated with reorganisation---court costs, attorney fees and eventual sale of the disputed asset---thus proxying for its price while remaining independent of its use. Unlike the average cost of bankruptcy measure, it is also available for all WES countries.
\input{$PPATH/p2/equations/p2equation1}
$t$ corresponds to the year in which data are collected---generally the year prior to their publication.
&lt;!--\label{p2footnote2}--&gt;The depth of credit information index was revised in 2014 but backwardly updated only to a single year. The previous methodology scored countries based on six criteria; the new methodology appends two more features.
\input{$PPATH/p2/tables/tex/p2table3}
Simple average of 189 economies covered in the Doing Business database.
Values above 1 indicate resolving a commercial dispute costs more than the value of the underlying claim.</Comments>
        </Document>
        <Document ID="F3136EA2-10B7-4EC2-8AA1-43B66FA0BA1E">
            <Title>Convex female ratio</Title>
            <Text>As discussed in [][p3authorlevel], [](#p3table6) suggests an increasing, convex relationship between female ratio and readability. The following tables restrict samples to male authors and apply several increasing, convex transformations to female ratio based on [](#p3equationAX). The plot of [](#p3equationAX) at different values of $x$ is shown in X; as it illustrates, [](#p3equationAX) assumes little or no relationship between readability and female ratio when few or no co-authors are women; past some threshold, however, the relationship increases exponentially.
</Text>
            <Comments>\input{$PPATH/p3/equations/p3equationAX}</Comments>
        </Document>
        <Document ID="3D65E866-13F8-4094-BE2C-3E8254E8226D">
            <Title>Co-variate balance</Title>
        </Document>
        <Document ID="A9CB7415-AD23-4D32-BC4E-8F3D95A31C57">
            <Title>[](#figure1), excluding *AER Papers &amp; Proceedings*</Title>
            <Text>Due to small samples of female authors, [](#figure1) includes 561 articles from *AER Papers &amp; Proceedings*. [](#figureD1) replicates its analysis excluding these observations. In addition to dropping *JEL* codes A (General Economics, Handbooks and Teaching), B (History or Economic Thought, Methodology and Heterodox Approaches), M (Business Administration and Business Economics; Marketing; Accounting; Personnel Economics) and P (Economic Systems) (see [](#footnote33)), [](#figureD1) excludes K (Law and Economics), N (Economic History), Q (Agricultural and Natural Resource Economics; Environmental and Ecological Economics), R (Urban, Rural, Regional, Real Estate, and Transport Economics) and Z (Other Special Topics): each has five or fewer observations that are both 100 percent female authored and assigned no more than two distinct *JEL* codes. Results are consistent with those in [](#figure1).
&lt;!--\input{$HOME/Dropbox/Readability/draft/figures/tex/figureD1}--&gt;</Text>
            <Comments>Articles are assigned, on average, two distinct primary *JEL* codes. A quarter of all articles are assigned three or more primary *JEL* codes; eight percent are assigned four or more; two percent are assigned five or more (up to a maximum of eight). Observations assigned five or more *JEL* codes are disproportionately clustered in the excluded codes.</Comments>
            <Notes>&lt;!--\clearpage--&gt;</Notes>
        </Document>
        <Document ID="F97F3B14-C5E0-4BC4-8E55-A73A9617216A">
            <Title>Investigating readability over authors' lifetimes</Title>
            <Text>The wider gap post-peer review confirms a causal link with peer review. It does not assure causality with referee scrutiny. In this section, I evaluate the alternatives: women write more clearly because of gender differences in (i) biology/behaviour---*e.g.*, they're more sensitive to referee criticism---or (ii) knowledge about referee expectations---*e.g.*, by overestimating the importance of writing well.
In a dynamic model of authors' decision-making processes, I show that any gap caused exclusively by (i) or (ii) declines with experience. Yet the gap does not decline. It widens. Estimates from pooled subsamples and matching indicate women write more clearly as their publication count increases; men, possibly less so. This pattern of behaviour suggests discrimination---either directly in the form of biased referee scrutiny or indirectly from biased referee [assignment](#Theorem1).</Text>
        </Document>
        <Document ID="2898D675-9504-4E38-8828-5B4B5EED218D">
            <Title>Discussion</Title>
            <Text>A gender readability gap exists. It's still there after including editor, journal and year effects---meaning we cannot blame specific policies or attitudes in the 50s, long since overcome. The gap is unaffected by field controls, so it's not that women research topics that are easier to explain. Perhaps it's caused by factors correlated with gender but actually linked to authors' (or co-authors') competence as economists or fluency in English? If so, institution and native speaker dummies would reduce it. They do not.
The gap grows between first draft and final publication and over the course of women's careers. This precludes systemic bias by article- or author-specific fixed effects---*e.g.*, inborn advantages and one-off improvements in response to external circumstances unrelated to peer review.
It likewise rules out gender differences in (i) biology/behaviour---*e.g.*, sensitivity to referee criticism---or (ii) knowledge about referee expectations. If diligently addressing every referee concern has no apparent upside---acceptance rates are unaffected---and a very clear downside---constant redrafting takes time---shouldn't even oversensitive, ill-informed women *eventually* re-examine beliefs... and start acting more like [men](#Theorem1)? Yet this is not what we observe. The largest investments in writing well are made by female economists with greatest exposure to peer review---*i.e.*, those with the best opportunity to update their priors.
Women's papers are more likely assigned female referees[#Abrevaya2012,Gilbert1994]. If female referees are more demanding critics, clearer writing could reflect their tougher reviews. Women concentrate in particular fields, so it's natural their papers are more often assigned female referees. However, for the readability gap to exist only because of specialisation, controlling for *JEL* classification should explain it. It does not. In fact, even including 718 tertiary *JEL* category dummies has virtually no effect. So if referee assignment is causing the gap, it's only because journals disproportionately refer female-authored papers to the toughest critics. Meaning it isn't referees who are biased---it's editors.
A final alternative is rather uncomfortable. Perhaps female-authored manuscripts deserve more criticism because they aren't as good? As mentioned earlier, factors correlated with gender but actually related to competency should decline when appropriate proxies are included. The sample itself is one such proxy---these are, after all, only articles published in the top four economics journals. Adding other controls---author institution, total article count, citation counts and published order in an issue---has no effect. The gap is widest for the most productive economists and even exists among articles originally released as NBER working papers---both presumably very clear signals of merit.
Yet I cannot rule out the possibility that women's work is systematically worse than men's---or that the female and male authors in [][SEUMatching] are not really equivalent. (To decide for yourself, see [][AppendixMatchingNames].) And if this is true, referees *should* peruse our papers more carefully---a byproduct of which could be better written papers after-the-fact or more attractive prose compensating for structural weaknesses before it.
"Quality" is subjective; measurement, not easy. Nevertheless, attempts using citation counts and journal acceptance rates do not indicate that men's research is any better: as discussed in [][SEUModel], gender has virtually zero impact on the latter; a review of past studies on male vs. female citations find four in which women's papers received fewer, six where they were cited more and eight with no significant difference[#Ceci2014].
More complicated, multi-factor explanations could resolve inconsistencies present when each is analysed in isolation. Perhaps female economists are perfectionists, and it gets stronger with age? Maybe women actually enjoy being poorly informed, overconfident and sensitive to criticism---or (more likely) I may have otherwise misspecified the author's objective function in [][SEUModel]. It is also possible that the statistically significant relationships this paper documents are unfortunate (particularly for me!) flukes.
Still, no explanation matches the simplicity and believability of biased referees and/or editors. Coherence and economy do not establish fact, but they are useful guides. This single explanation neatly accounts for all observed patterns. If reviewers apply higher standards to female-authored papers, those papers undergo more thorough review. Added scrutiny should improve women's exposition but lengthen review times---as seen in [][Duration]. The rewards from clearer writing are presumably internalised, meaning women gradually improve---which they do, as illustrated in [][Experience].
Moreover, several studies document a gender difference in critical feedback of similar form---employee performance reviews and student evaluations. Ongoing research suggests female workers are held to higher standards in job assessments. They are acknowledged less for creativity and technical expertise, their contributions are infrequently connected to business outcomes; guidance or praise supervisors do offer is vague[#Correll2016].
Students display a similar bias. [Data](http://benschmidt.org/profGender/) from [Rate My Professors](http://www.ratemyprofessors.com/) suggest female lecturers should be "helpful", "clear", "organised" and "friendly". Men, instead, are praised (and criticised) for being "smart", "humble" or "cool"[#Schmidt2015]. A study of teaching evaluations similarly finds students value preparation, organisation and clarity in female instructors; their male counterparts are considered more knowledgable, praised for their "animation" and "leadership" and given more credit for contributing to students' intellectual development[#Boring2017].
TO ADD:
Readability scores may be capturing something not related to readability.
Readability scores may be capturing the fact that referees are less interested in female work. This suggests a systematic bias against women in peer review, but is not necessarily the *fault* of individual referees. Instead, it just reflects another---albeit no less insiduous---bias taht results from systematic underrepresentation of women. However, the readability gap remains after controlling for detailed *JEL* codes. Thus, this must mean that referees are systematically less interested in women's work even after controlling for highly specific sub-fields. That is, male-authored papers are more appealing to editors and referees even compared to female-authored papers in the exact same field.
The readability of text may be more important when interest is low than when it is high[#Klare1976,Fass1978]. It has also been shown that prior knowledge and beliefs about a topic improved reading comprehension for a particular text[#Woern1977,Spilich1979,Chiesi1979]. Easier readability of a text has more benefits for those of less knowledge and interest than those of more. Advanced knowledge of a subject can "drown out" the effects of an otherwise difficult text. This study also suggested that when reader interest is high, comprehension is not improved by writing the material below, rather than at, the grade level of the readers. When interest is low, however, comprehension is improved by writing the materials below, rather than at, the reading level of the readers.
A remaining concern is whether peer review actually affects manuscript readability. In a review of prestigious biomedical journals, [#Boutron2016;] find that 47 percent of editors clearly request that referees evaluate the language of a manuscript they are reviewing. Additionally, a review of the comments posted on ShitMyReviewer.com, quarter deal with writing quality, document structure or word choice/tone (FIGURE XXXXX).
Thus underlying all of the conclusions presented in this paper is the implicit assumption that if some perfect measure of readability existed that incorporated these factors, it would conclude the same thing. Thus assumption implies that the effects presented in this paper suffer from attenuation bias. given the relatively large textual samples used in the paper, the effects presented in fact suffer from attenuation bias.
Because readability scores omit these factors using them to infer causality between gender and any outcome they attempt to proxy for implicitly assumes that gender impacts these other facets in the same way that gender impacts readability. Or, in other words, if we *did* have a more comprehensive measure of readability, it would show the same thing.
And in any case, readability scores are perfect (or almost perfect) predictors of sentence and word length. Thus, the figures presented in this paper will always capture differences in the weighted averages of these two measures.
I do not measure "quality" with readability. Quality is multidimensional and in any way highly subjective---I challenge anyone to find more than two people to agree on the components most important to quality. Nevertheless, if readability is an unbiased proxy for it, then these results suggest women's papers are in fact higher quality.
*Discuss classicial measurement error vs. non-classical measurement error*.
Classical measurement error is of most concern in small sample sizes with a high degree of variability between individuals because higher weights are placed on individual results. While I cannot rule out classical measurement error entirely as a factor driving my results, the fact that sample sizes are both relatively large and results do not appear to be driven by any individual results, suggests it is probably not, on its own, a particularly important factor.
Thus, if non-classical measurement error prevents inferring gender differences in *readability* it is still the case that sentence-length and vocabulary complexity in female-authored papers is negatively affected by peer review. 
Thus, should non-classical measurement error prevent inferring readability between genders, one must nevertheless confront the observation that sentence-length and vocabulary complexity in female-authored papers is negatively affected by peer review. 
The weighted average of these two variables is informative in much the same way that the general "readability" inference may be.
</Text>
            <Comments>I also conducted a primitive surname analysis [see\]\[pp. 35--36][#Hengel2016]. It suggests that the female authors in my data are no more or less likely to be native English speakers.
While women do appear more *internally* responsive to feedback---criticism has a bigger impact on their self-esteem---available evidence suggests they aren't any more *externally* responsive to it, *i.e.*, women and men are equally likely to change behaviour and alter performance after receiving feedback[#Johnson2002,Roberts1989].
Note that women are only a fraction of all referees---8 percent in 1986[#Blank1991], 10 percent in 1994[#Hamermesh1994] and 14 percent in 2013[#Torgler2013]. [#Abrevaya2012;] report female-authored papers were only slightly more likely to be assigned a female referee between 1986--1994, although matching does increase in 2000--2008.
It's not so clear whether their reports are any more critical. A study specific to post-graduate biologists suggests yes[#Borsuk2009]; another analysing past reviews in an economics field journal does not[#Abrevaya2012].
Specifically, men and women publishing in the same field face the same pool of referees. Controlling for that pool would account for gender differences in readability.
This is a form of biased referee [assignment](#Theorem1). A similar argument contends that female research is more provocative, and more provocative work warrants more scrutiny. If this were true, controlling for *JEL* classification would also reduce (or eliminate) the gap---unless women's work is systematically more provocative even among researchers in very narrow fields. Yet provocative work is (presumably) highly cited work, and there is no discernible gender difference in citation counts[#Ceci2014]. Alternatively, perhaps the wider public excessively scrutinises female work, and referees respond similarly to minimise blowback. This explanation assumes a wider public capable of discrediting our work---a view many economists would (privately) disagree with. In any case, economics employs advanced mathematics and technical language, making it especially inaccessible to a layperson.
Published order in an issue refers to the order an article appears in a particular issue (*i.e.*, one for the lead article, two for the second article, *etc.*). This control was introduced as a a set of indicator variables. See [\]\[p. 42 and p. 44][#Hengel2016;] for regression output.
It does seem contradictory, however, that women would be capable of writing better than men---even before referee [input](#table7)---but incapable of producing similar quality research. One is inclined to believe clarity of thought and quality of research to go hand-in-hand, although I am not aware of any study on the topic.
Journals may have a policy of publishing female-authored research over equal (or even better) male work. If so, acceptance rates are not an unbiased indicator of quality.
While women score higher on maintaining order[#Feingold1994]---a trait including organisation and perfectionism---significant differences are not universally present in all cultures[#Costa2001]. Moreover, differences that are present decline---or even reverse---as people age[#Weisberg2011].
No one (to my knowledge) has tested whether men and women receive different critical feedback in peer review reports, 
A similar phenomenon exists in online fora. The *Guardian* commissioned researchers to study 70 million comments on its website. It found female and black writers attract disproportionately abusive threads[#Gardiner2016].
These conclusions are based on an observational account of the data.
An alternative bias is instead the "file drawer bias"---*i.e.*, all the other studies showing no effect just don't get published. I try to counteract this effect by collecting a relatively large sample. Nevertheless, the required sample size necessarily depends on the number of female authors, of which there are not very many. Thus, the possibility that the effects I observe in *three separate samples* are just statistical flukes can not be ruled out.</Comments>
        </Document>
        <Document ID="3A0100A1-0A5D-47DB-B485-BA27DDE45FDC">
            <Title>Male effects</Title>
        </Document>
        <Document ID="446183A8-F5D9-4C49-8821-3FF368B70629">
            <Title>[](#table11), submission year fixed effects</Title>
            <Text>[](#table11XXA) repeats the regressions pre\\-sented in [](#table11) but using submission year fixed effects in lieu of publication year fixed effects. As discussed in [](#Footnote88), this change narrows the gap in publication times between male- and female-authored papers by about two months.
&lt;!--\input{$HOME/Dropbox/Readability/draft/tex/generated/table11XXa}
\clearpage--&gt;</Text>
        </Document>
        <Document ID="40EB9DC5-9E7B-41F5-9A6D-3BB99C603C47">
            <Title>Condition 2</Title>
            <Text>The purpose of Condition 2 is subtle. It rules out the possibility that women just intrinsically prefer writing more clearly than men or enjoy a lower cost of writing. Yet any such intrinsic preference or cost explanation defines the minimum readability at which a women is willing to write. Thus, *all* of her papers, including the most poorly written one, is at least this readability level. Because women will always choose a readability level that satisfies this minimum threshold dictated by their preference, any *increase* in readability must be driven by something else---*i.e.*, either their own mis-information or sensitivity or by factors outside their control.</Text>
        </Document>
        <Document ID="3D4820AE-D0B6-4745-94A3-34AFDB24DBA2">
            <Title>Robustness</Title>
            <Text>Conclusions drawn from [](#table10) are principally predicated on two assumptions: (i) $i$ and $k$ are equivalent; (ii) $t$ is sufficiently large---*i.e.*, $t&gt;t'$ ($e_{nit}^s$ is on the convergence path to zero for $n=0,1$) and any errors in $i$'s beliefs about $\widetilde r_{0i}$ and $\widetilde R_i$ are sufficiently small. If either is violated, discrimination against women cannot be inferred from an overrepresentation of matched pairs with $\underline D_{ik}&gt;0$.
The first assumption depends on match accuracy. Post-match co-variates are well [balanced][AppendixMatchingBalance]. They remain well balanced---and similar to the matched population---when restricted to pairs satisfying $\underline D_{ik}&gt;0$ and/or [$\underline D_{ik}&lt;0$][AppendixMatchingBalance]. To facilitate further scrutiny, [][AppendixMatchingNames] lists the names of economists in each pair.
Matches are sensitive to the choice and construction of variables and the model and method used to estimate propensity scores. Outcomes, however, are not. After controlling for $T_i$, decade, journal and *JEL* code, matches using alternative variables (*e.g.*, minimum citation counts and mean institutional rank) and specifications (*e.g.*, logit and no replacement) generate similar figures and conclusions. Results using a Mahalanobis matching procedure are shown in [][Appendix Mahalanobis]. Alternative specifications are available on request (&lt;!--\href{mailto:erin.hengel@gmail.com}{\texttt{erin.hengel@gmail.com}}--&gt;).
The second assumption demands a "sufficiently large" $t$. For diagnosing discrimination, "sufficiently large" means $t'&lt;3$ and the difference in $i$ and $k$'s error in beliefs at $t=3$ is smaller than $D_{ik}$. Forty-eight percent of all women with three or more top publications satisfy Conditions 1 and 2 when compared to equivalent men. Among them, $\underline D_{ik}$ is far from zero ([](#table10), first column): these women write, on average, 29 percent more clearly than equivalent men with identical experience. It is unlikely that half of all female economists with three top publications---plus many more second-tier publications and substantial experience refereeing and editing themselves---make mistakes of this magnitude.
Interpreting $\underline D_{ik}$ as a causal, conservative estimate of discrimination's impact on readability requires the stronger assumption that $e_{ni3}^s=e_{nk3}^s$. When violated, I can no longer conclude that $\underline D_{it}$ conservatively estimates $D_{ik}$. Nevertheless, $e_{nit}^s-e_{nkt}^s$ is converging to zero and likely very small at $t=3$. Any upward bias from $e_{nkt}^s&lt;e_{nit}^s$---*i.e.*, from senior female economists *still* making more mistakes about reviewers' thresholds than equivalent men even after previously publishing two top papers---is probably small and arguably offset by the downward bias already baked into $\underline D_{ik}$\.
Finally, causal interpretation technically requires that three additional criteria are also met. Assuming discrimination against $i$: (i) $i$'s acceptance rate is no more than $k$'s; (ii) $r_{0k3}\le r_{0i3}$---*i.e.*, $i$'s draft readability is at least as high as $k$'s; and (iii) $r_{0i1}\le r_{0i3}$---*i.e.*, $i$'s draft readability at $t=3$ is at least as high as his draft readability at $t=1$. As already discussed, (i) rules out the possibility that $i$ is appropriately rewarded (relative to $k$) for writing more clearly. (ii) and (iii) eliminate situations in which women write more clearly during peer review to compensate for poorer writing---and consequently higher desk rejection rates---before peer review.
Unfortunately, my data do not perfectly identify acceptance rates nor do I have $t=1$ and $t=3$ draft readability scores for every matched pair. Nevertheless, the data I do have and prior research strongly suggest (i)--(iii) not only hold on average, but do not exert upward bias on my estimate of $D_{ik}$, more generally. First, the previous section reviews the literature on gender neutrality in journals' acceptance rates. Women are not accepted more often than men. In [][AppendixConservative], I attempt to control for them explicitly by adding the requirement $T_i\le T_k$ or $T_k\le T_i$ to categorise matched pairs as discrimination against $i$ or $k$, respectively. Results are similar; conclusions unchanged. As shown in [][NBER], women's draft papers are indeed more readable than men's. [][IndirectEffect] provides further confirmation. It plots the readability of women's and men's draft and published papers over increasing $t$. Women's drafts are more readable than men's drafts at $t=3$ *and* more readable than their own earlier drafts at $t=1$.</Text>
            <Comments>&lt;!--\label{Footnote84}--&gt;I use "error" and "mistake" to refer to anything that would cause authors to write more (or less) clearly than they would if $\widetilde r_{0i}^s$ and $\widetilde R_i^s$ were known. This includes actual mistakes in judgement as well as character components---*e.g.*, conscientiousness or risk aversion---that impact beliefs and/or the optimal choice set under uncertainty.
Women are the better writers in 73 percent of matched pairs. In 34 percent of those, however, the woman did not improve her writing between $t=1$ and $t=3$ (Condition 2), thus rendering [](#Theorem1)'s test for discrimination inconclusive.
[](#table10), first column divided by the mean male $\widehat R_{k3}$ ([][AppendixConservative]).
$\underline D_{ik}$ actually remains a causal, conservative estimate of the impact of discrimination against women under the weaker assumption $e_{ni3}^s\le e_{nk3}^s$, $n=0,1$ ($i$ female, $k$ male). See the proof of [](#Corollary1) in [][AppendixProofs].
Specifically, this assumption is violated if at $t=3$ the women listed in [][AppendixMatchingNames] make more (positive) mistakes about $\widetilde r_{0i}^s$ and/or $\widetilde R_i^s$ than the men they are matched to. For $\underline D_{ik}$ to remain a *conservative* estimate of $D_{ik}$, women's mistakes must be no greater than men's mistakes at $t=3$.
For a description of this downward bias, see the discussion on [](#Corollary1) in [][SEUModel] and the proof of [](#Corollary1) in [][AppendixProofs].</Comments>
        </Document>
        <Document ID="C17ED5AA-791B-418B-A8AF-022A6F9AF7B0">
            <Title>NBER working paper release dates vis-Ã -vis Journal submission dates</Title>
            <Text>As shown in [][NBERResults], female-authored papers are more likely to be released as NBER Working Papers *after* the underlying paper was already submitted to *Econometrica*. Here, I show that this conclusion is robust to controlling for year fixed effects. Specifically, I regress the ratio of female authors on the time difference (in months) between a paper's NBER Working Paper release and submission to *Econometrica*. The coefficient on female ratio from this regression is 6.81 months (robust standard error 4.31), indicating that the female ratio of an article is correlated with later NBER release.
[](#figure8) plots gender differences over year. The horizontal grey line denotes that the paper was submitted to *Econometrica* and released as an NBER Working Paper at the same time. Values below this line indicate that papers in a year were, on average, released as an NBER Working Paper *before* they were submitted to *Econometrica*; values above this line indicate that papers in a year were, on average, submitted first to *Econometrica* and then released as an NBER Working Paper. Points in blue are effects estimated for 100 percent male-authored papers; points in pink are effects estimated for 100 percent female-authored papers.
&lt;!--\input{$HOME/Dropbox/Readability/draft/tex/fixed/figure8}--&gt;
As can be seen in [](#figure8), men and women both generally release their papers as NBER working papers only *after* they've already submitted the paper to *Econometrica*. For women, however, this delay is more pronounced.
&lt;!--\clearpage--&gt;</Text>
        </Document>
        <Document ID="B4274C52-4B25-452A-92C4-11AB5031CDD2">
            <Title>Introduction</Title>
            <Text>Female economists are less likely to make tenure, take longer when they do and earn much less than their male peers[#Bandiera2016,Ceci2014,Ginther2004,Weisshaar2017]. Only a third, fifth and tenth of assistant, associate and full professors, respectively, are women[#Romero2013]. We make up just 14 percent of authors published in top economics journals since 2000.
These statistics are uncomfortable, but their causes are myriad: lower publishing rates, career choices, motherhood and, probably, bias. In lab experiments women are subject to tougher standards. Their qualifications and ability are underestimated[#Foschi1996,Grunspan2016,Moss-Racusin2012,Reuben2014]. Female-authored manu\\-scripts are evaluated more critically[#Goldberg1968,Krawczyk2016,Paludi1983]; when collaborating with men, women are given less credit[#Heilman2005,Sarsons2015].
This paper uses five reliable measures of writing clarity to show that women are likewise held to higher standards in peer review. (i) Female-authored articles published in top economics journals are better written than similar papers by men. The difference cannot be explained by year, journal, editor, topic, institution, English language ability or with various proxies for article quality and author productivity. (ii) The gap widens precisely while papers are being reviewed. I compare published articles to their pre-reviewed drafts. Forty percent of the gap originates *during* peer review. Moreover, there is no evidence of a gap in papers subjected to double-blind review before the internet. (iii) Female economists improve their writing; male economists don't. A dynamic model of an author's decision-making process shows that tougher editorial standards and/or biased referee assignment are the only explanations consistent with women's choices. Using a conservative measure derived from the model, I estimate that this type of discrimination causes senior female economists to write at least 9 percent more clearly than they otherwise would.
I also document evidence that higher standards confound productivity measurement and their own identification. First, higher standards presumably delay review---and as anticipated, female-authored papers spend six months longer in peer review. This estimate is based on submit-accept times at *Econometrica*, and controls for, *inter alia*, motherhood, childbirth, citations and field. Second, women adjust to biased treatment in ways that partially---or even totally---confuse it with voluntary choice. Although the readability gap rises with experience, the portion formed in peer review falls. Studies that analyse only this trajectory may underestimate discrimination, misallocate responsibility or even conclude bias against men.
Higher standards impose a quantity/quality tradeoff that likely contributes to academia's "Publishing Paradox" and "Leaky Pipeline". Spending more time revising old research means there's less time for new research. Fewer papers results in fewer promotions, possibly driving women into fairer fields. Moreover, evidence of this tradeoff is present in a variety of occupations---*e.g.*, doctors, real estate agents and airline pilots---suggesting higher standards distort women's productivity, more generally.</Text>
            <Comments>[#Weisshaar2017;] evaluates the probability of making tenure in Sociology, Computer Science and English departments.
"Publishing Paradox" and "Leaky Pipeline" refer to phenomena in academia whereby women publish fewer papers and disproportionately leave the profession, respectively.</Comments>
        </Document>
        <Document ID="35E34F9E-C912-4C4C-AF35-EEB8D22B5720">
            <Title>`Textatistic`</Title>
            <Text>To determine sentence count, the program replaces common abbreviations with their full text, decimals with a zero and deletes question and exclamation marks used in an obvious, mid-sentence rhetorical manner. The remaining full stops, exclamation and question marks are assumed to end a sentence and counted.
Next, hyphens are deleted from commonly hyphenated single words such as "co-author" and the rest are replaced with spaces, remaining punctuation is removed and words are split into an array based on whitespace. Word count is the length of that array.
An attempt is made to match each word to one on an expanded Dale-Chall list. The count of difficult words is the number that are not found. This expanded list, available on [GitHub](https://github.com/erinhengel/Textatistic), consists of 8,490 words. It is based on the original 3,000 words, but also includes verb tenses, comparative and superlative adjective forms, plural nouns, *etc.* It was created by first adding to the Dale-Chall list every conceivable alternate form of each word using Python's Pattern library. To eliminate nonsense words, the text of 94 English novels published online with Project Gutenberg were matched with words on the expanded list. Words not found in any of the novels were deleted.
Syllable counts are based on the C library `libhyphen`, an implementation of the hyphenation algorithm from[#Liang1983;]. [#Liang1983;]'s algorithm is used by &lt;!--\TeX--&gt;'s typesetting system. `libhyphen` is employed by most open source text processing software, including OpenOffice.</Text>
            <Comments>Abbreviations which do not include full-stops are not altered. I manually replaced common abbreviations, such as "*i.e.*" and "U.S." with their abbreviated versions, sans full stops.
For example, "?)." is replaced with ").".
Per[#Chall1995;], hyphenated words count as two (or more) words.</Comments>
        </Document>
        <Document ID="69BF07BE-F69B-411E-AA52-CEA9BE2D00E9">
            <Title>[](#table8), male effects</Title>
            <Text>[](#tableC7) shows male effects from the regressions described and presented in [](#table8). Effects estimated at a female ratio of zero and observed values for other co-variates. Grade-level effects (Flesch-Kincaid, Gunning Fog, SMOG and Dale-Chall) have been multiplied by [negative one][MeasuringReadability].
&lt;!--\input{$HOME/Dropbox/Readability/draft/tex/generated/tableC7}--&gt;</Text>
        </Document>
        <Document ID="B8F4B6D9-7AB3-427A-8FF5-9DB01D5CD64B">
            <Title>Indirect effect of higher standards</Title>
            <Text>As a final exercise, I investigate how women react to higher standards as they update beliefs about referees' expectations. [](#figure6) compares papers pre- and post-review at increasing publication counts. Solid circles denote NBER draft readability; arrow tips reflect readability in the final, published versions of those same papers; dashed lines trace readability as papers undergo peer review. Figures are based on FGLS estimation of [](#equation14) (see [][NBER]):
&lt;!--\input{$HOME/Dropbox/Readability/draft/equations/equation14}--&gt;
 where $m=W,P$ for working papers and published articles, respectively, and $\vect X_j$ is a vector of observable controls: editor, journal, year, journal and year interactions, English fluency dummies and quality controls---citation count and $\text{max. }T_j$. Since $t_i$ is author-specific, I disaggregate the data by duplicating each article $N_j$ times; to account for duplicate articles, regressions are weighted by $1/N_j$ (see [][AuthorLevel]).
All things equal, economists who anticipate referees' demands are rejected less often; economists who donât enjoy more free time. [](#figure6) implies little---if any---gender difference in this tradeoff: senior economists of both sexes sacrifice time upfront to increase acceptance rates.
&lt;!--\input{$HOME/Dropbox/Readability/draft/tex/fixed/figure6}--&gt;
Moreover, [](#figure6) emphasises that only *inexperienced women* improve readability during peer review. Assuming choices by senior economists express optimal tradeoffs with full information, this implies that women initially underestimate refereesâ expectations. Men, however, do not. Their draft and final readability choices remain relatively stable over increasing $t$.
Are men just better informed? Yes and no. Male and female draft readability scores for first-time publications are exactly the same, suggesting both start out with identical beliefs. Yet identical beliefs do not promise identical information. Men are indeed better informed because the standards they believe apply to them actually do. Female authors make the mistake in assuming those same standards apply to women, too.
The first panel of [](#table12) displays the magnitude and standard errors of the contemporaneous marginal effect of peer review ($R_{jP}-R_{jW}$) for men and women over increasing $t$. Estimates correspond to the lengths of the dotted lines in [](#figure6). Gender differences are shown in the final row.
&lt;!--\input{$HOME/Dropbox/Readability/draft/tex/generated/table12}--&gt;
For publications $t=1$ and $t=2$, differences are large, positive and significant; for publications three and up, they're fairly small. Nevertheless, the readability gap in the published article remains large, statistically significant and relatively stable at every $t$ ([](#table12), second panel). Increasingly, however, it forms before submission. Draft readability contributes nothing to the gap at $t=1$. That rises to 43 percent at $t=2$ and 73 percent at $t=3$. By $t=4\text{--}5$ and $t=6+$, men and women mostly choose to address referee concerns prior to peer review, corroborating analysis based on [](#figure6). 
[](#figure6) and [](#table12) document evidence that female economists are held to relatively constant---albeit higher---standards throughout their careers. Over time, women adjust to those standards by writing more clearly before peer review. Assuming---as other evidence suggests---that women's papers are accepted no more often than men's papers, this implies that female economists at every level of seniority must work harder than their male peers to achieve a similar outcome.</Text>
            <Comments>Results and conclusions based on unweighted regressions---or by replacing $t_i$ with $\text{max. }t_j$ and *not* duplicating articles---are very similar or identical to those presented here. Regression output from alternative specifications available on request (&lt;!--\href{mailto:erin.hengel@gmail.com}{\texttt{erin.hengel@gmail.com}}--&gt;).
&lt;!--\label{Footnote132}--&gt;Alternatively, if desk rejection rates are gender neutral, authors subjected to higher standards will undergo more arduous peer review. Greater scrutiny would therefore replace higher desk rejection rates when editors (or even referees) monitor and implement a policy of gender neutral acceptance rates.
Assuming no gender difference in acceptance rates at $t=3$ and given evidence that women are held to higher standards documented earlier, [](#figure6) suggests---but does not prove---that manuscripts by junior female economists are disproportionately rejected. See also [](#Footnote132) for an alternative interpretation in which acceptance rates are identical but scrutiny is not.</Comments>
        </Document>
        <Document ID="EC768D57-346B-4795-AFAA-FD86683B8174">
            <Title>Model</Title>
            <Text>An entrepreneur has an idea for a business project. The project lasts two periods and requires a machine. Operating profits each period are $X_1$ and $X_2$, where $X_1$ and $X_2$ are non-negative and independently distributed according to the joint cumulative distribution  function $\Pi$.
The machine costs $K_0$. Up to time 1, it may be resold for its initial value. After time 1, it depreciates; its value at time 2 is $K_2$, where $0&lt;K_2&lt;K_0$. $K_0$ and $K_2$ are non-random and known at time 0.
The entrepreneur has no money to buy the machine. He pitches a creditor the take-it-or-leave-it offer to borrow $K_0$ at time 0 and owe $D$ at time 1. The entrepreneur and creditor are risk neutral, have symmetric information and the risk-free interest rate is zero; credit markets are perfectly competitive.
If the lender accepts his offer, the entrepreneur buys the machine. The project begins and $X_1$ is realised.</Text>
            <Comments>The analysis is applicable to any tangible or intangible depreciable productive asset.
For an extensive form representation of the bankruptcy game, see [][p1appendixextensiveform].
Borrowing more than $K_0$ is a risk-free transfer of wealth from time 1 to time 0. Since the entrepreneur is risk-neutral and the risk-free interest rate is zero, the transfer itself confers no benefit. The fact that loans mature after one period while project returns last for two is fundamental to the model. If loans could last the entire duration of the project, bankruptcy is irrelevant.</Comments>
        </Document>
        <Document ID="C1E5A10A-F292-4EE1-A8AA-E29E36962741">
            <Title>Identification</Title>
            <Text>
 $\alpha_i$ represents author-specific time-independent preferences and costs ($u_i$, $\phi_i$, $c_i$); $\mu_{it}$ are the time-dependent confounding factors picked up by $\Pi_{0it}^s$ and $\Pi_{1it}^s$. $\vect X_{it}$ is a vector of observable variables that factor into referees' readability thresholds $\widetilde r_{0i}^s$ and $\widetilde R_i^s$. $\beta_1$ is the impact of taste-based discrimination. $\varepsilon_{it}$ is an ideosyncratic error that reflects the draw of referee group.
If author $i$ knew $(\beta_1,\bm\uptheta)$, then he knows $\widetilde r_{0i}^s$ and $\widetilde R_i^s$, so $\mu_{it}=0$. A fixed effects regression of [](#EquationXX1) would consistently estimate $\beta_1$. Unfortunately, he does not know this parameter vector. However, past some point, he *will*---or at least he's on enough of the right track so that with each subsequent paper the impact of $\mu_{it}$ gets smaller and smaller.
Assume that at time $t'$ $\mu_{it}&lt;\mu_{it'}$ for all $t&gt;t'$. Then the change in readability between times $t''&lt;t'$ is exactly proportional to the initial impact on readability of the time varying confounding factors,
&lt;!--\input{$PPATH/equations/equationXX1.tex}--&gt;
Because $\mu_{it'}&lt;\mu_{it''}$, a positive [](#EquationXX1) implies $\mu_{it''}&lt;0$. Thus, confounding factors at time $t''$ caused author $i$ to underestimate the parameter vector $(\beta_1,\bm\uptheta)$. At time $t'$ she has corrected her mistake so her readability is higher. This also implies that for $i$, acceptance rates are higher at $R_{it'}$ than at $R_{it''}$---if they were the same, then the fact that $i$ chose $R_{it''}$ earlier means she prefers that readability to $R_{it'}$, all thing equal. A higher acceptance rate is the only reason she's motivated to choose the higher readability at time $t''$.
The difference in readability between matched pairs, is the effect of taste-based discrimination ($\beta_1$) plus the degree to which their preferences differ:
&lt;!--\input{$PPATH/equations/equationXX2.tex}--&gt;
 Although it is is impossible to disentangle $\beta_1$ from $\Delta\alpha_{ik}$, as long as [](#EquationXXX1) and [](#EquationXXX2) are both positive then either of the following must be true: $$R_{it'}&gt;R_{kt'}\ge R_{it''}$$ or $$R_{it'}&gt;R_{it''}\ge R_{kt'}.$$ If it's the first case, then $i$ would chose $R_{kt'}$ if she could get the same acceptance probability as she does at $R_{it'}$ because she'd choose $R_{it''}&lt;R_{kt'}$ if she could get the same probability of acceptance at that level as she does at $R_{it'}$. Thus her probability of acceptance must be higher at $R_{it'}$ than it is at $R_{kt'}$. If the probability of acceptance for $k$ at $R_{kt'}$ is at least as high as it is for $i$ at $R_{it'}$ (Condition 1), then $\beta_1$ must be positive.
But not only must $\beta_1$ be positive, $R_{it'}-R_{kt'}$ *underestimates* it. If $R_{kt'}&gt;$R_{it''}$ then $\alpha_i&lt;\alpha_k$. Since $\vect X_i=\vect X_k$, $$R_{kt'}-R_{it''}=-\beta_1+\left(\alpha_k-\alpha_i\right)+\left(\mu_{kt'}-\mu_{it''}\right)+\left(\varepsilon_{kt'}-\varepsilon_{it''}\right).$$ As we've already established, $\mu_{it''}&lt;0$. Assuming $k$ is at least as well informed at time $t'$ than $i$ was at time $t''&lt;t'$ then $\mu_{kt'}$ is smaller in absolute value than $\mu_{it''}$ and their difference must be negative. Because we've already established that $\beta_1$ is positive, $R_{kt}&gt;R_{it''}$ only if $\alpha_i\le\alpha_k$. Thus, $R_{it'}-R_{kt'}$ is a lower bound on $\beta_1$.
On the other hand, if $R_{it'}&gt;R_{it''}\ge R_{kt'}$ then $i$ would choose $R_{it''}$ if she could get the same probability of acceptance at that level as she does at $R_{it'}$. Thus her probability of acceptance must be higher at $R_{it'}$ than it is at $R_{it''}$. If the probability of acceptance for $k$ at $R_{kt'}$, then again, $\beta_1$ must be positive.
=======================================
[](#Theorem1)'s three conditions are sufficient to determine its sign---and therefore if peer review is biased against $i$.
To see this, note that the change in readability between times $t''$ and $t'$ for fixed $i$ is exactly proportional to the initial impact on readability of the time varying confounding factors,
&lt;!--\input{$PPATH/equations/equationXX1.tex}--&gt;
When [](#EquationXX2) is positive (Condition 2), then $\mu_{it''}&lt;0$. At time $t''$ she did not know $\beta_0$, $\beta_1$ or $\theta$ as well as she does at time $t'$
This means that $i$'s increase in readability between times $t''$ and $t'$ was driven by a correction of their earlier mis-information. She writes better at time $t''$ only because earlier she though she could achieve a certain acceptance rate with a lower $R_{it}$; now she knows she needs a higher $R_{it}$ to get that same rate, on average.
$i$ and $k$ are accepted at the same rate at time $t''$ (Condition 1) and 
in [](#EquationXX2). Nevertheless, $i$ preferred the lower $R_{i1}$ when less experienced; she picks $R_{i3}&gt;R_{i1}$ not because she prefers to write more readability, but because experience has taught her this is exactly how readable her papers need to be to achieve the acceptance rate she desires. Assuming $i$ and $k$ are accepted at the same rate with $R_{k3}$ and $R_{i1}$, then if $R_{k3}&gt;R_{i1}$, $\alpha_i&lt;\alpha_k$, so $R_{i3}-R_{k3}$ underestimates $\beta_1$. 
===================================
the change in readability over time for a fixed $i$ is exactly proportional to the initial impact on readability of the time varying confounding factors
&lt;!--\input{$PPATH/equations/equationXX1.tex}--&gt;
 
Although it is impossible to generate an unbiased estimate of $\beta_1$, [](#Theorem1)'s three conditions are sufficient to ensure that $\beta_1$ is strictly positive---and therefore biased against $i$. Assuming $i$ and $k$ are accepted at the same rate, then peer review is biased against $i$ when both [](#EquationXX1) and [](#EquationXX2) are positive.
If [](#EquationXX1) is positive, then $\mu_{i1}&lt;0$---meaning the time-varying confounding factors initially put downward pressure on readability. As the author gained experienced, she realised her readability needed to be better, so that with sufficient experience (*i.e.*, $\mu_{i3]=0$) she writes more clearly than previously.

To control for factors that vary over $t$ in a way that may be correlated with gender---*e.g.*, journal, year, female ratio, *etc.*---I generated author-specific readability scores for fixed values of these variables. Specifically, I estimated [](#EquationXX) on female authors at $t=1$, female authors at $t=3$, male authors at $t=1$ and male authors at $t=3$.
Based the results, I predicted $\varepsilon_{it}$ and with it reconstructed $R_{it}$ at median $t=3$ values of $\text{year}_{it}$, $N_{it}$, $\text{inst. rank}_{it}$ and $\text{max. inst. rank}_{it}$ and $\text{female ratio}_{it}$ equals 0 for men and 1 for women.
</Text>
            <Comments>If her paper was published at $t''$, it was because she got a low draw of $\varepsilon_{it}$---*i.e.*, an $s\in\Sigma$ wish low readability thresholds.</Comments>
        </Document>
        <Document ID="A05C7411-D19E-40DA-9752-0ACD086F6D56">
            <Title>Receivership</Title>
            <Text>Practically, receivership translates to (i) the lack of an automatic stay, (ii) the freedom to pledge an entire business as collateral and (iii) the ability to enforce a claim out-of-court. To identify the presence of each, I again turn to Doing Business data.
Although the definition of a floating charge is relatively straightforward, diagnosing its effective existence is not so. For example, many countries superficially possess legislation that grants this right but simultaneously prevent those securities from automatically including proceeds and inventories. Without that guarantee, other parties may control significant company assets upon default, making it harder to bypass formal bankruptcy. Thus, I assume floating charges are permitted only if the general description of an entire business---including its moveable and future/after-acquired assets---is a permissible form of collateral.
Data were obtained from questionnaire responses used to generate Doing Business's "strength of legal rights index". Columns one and two of [](#p2table4) count countries that do and don't permit the three components of receivership. The fourth column discusses within country variation.</Text>
            <Comments>[](#p2tableB9) ([][p2AppendixB]) lists the specific questions asked. 2012 and 2015 responses were scrapped directly from the Doing Business website using custom perl and python scripts. Because disaggregated data are made available only in the current year, 2010, 2011, 2013 and 2014 responses were comparably scrapped from dated snapshots of the Doing Business website taken by [Internet Archive](https://archive.org). Data between 2006--2009 were based on 2010 data and updated to incorporate reforms mentioned in those years' published reports. Due to methodological changes, only questionnaires pre-2013 asked specifically about the existence of an automatic stay for secured creditors. Those responses were applied to subsequent years and similarly updated for reforms.
\input{$PPATH/p2/tables/tex/p2table4}</Comments>
        </Document>
        <Document ID="7CCF514E-E6AE-4E0B-A543-CB9AD657BFD3">
            <Title>Duration of peer review</Title>
            <Text>"Writing simply and directly only looks easy"[\]\[p. 53][#Kimble1994]. An essay's rhetorical competency is highly correlated with the length of time one is given to compose it[#Hartvigsen1981,Kroll1990]. Skilled writers spend more time contemplating a writing assignment, brainstorming and editing. They also write fewer words per minute and produce more drafts[#Faigley1981,Stallard1974].
Since writing simply and directly takes time, one observable repercussion will be prolonged peer review for female authors. To investigate, I turn to *Econometrica*, the only journal to make disaggregated data on the revision process publicly available.
[](#figure5) is a histogram of time (in months) between dates papers are first submitted to and their final revisions received by *Econometrica*'s editorial office. Blue bars represent articles written only by men, pink bars are those just by women. The 180 papers co-authored by men and women are not included.
&lt;!--\input{$HOME/Dropbox/Readability/draft/tex/fixed/figure5}--&gt;
Since 1950, *Econometrica* published 53 papers authored entirely by women. As [](#figure5) illustrates, their review times disproportionately cluster in the distribution's right tail: articles by women are six times more likely to experience delays above the 75th percentile than they are to enjoy speedy revisions below the 25th.
For a more precise appraisal, I build on a model by [\]\[Table 6, p. 963][#Ellison2002a;] and estimate [](#equation13):
&lt;!--\input{$HOME/Dropbox/Readability/draft/equations/equation13}--&gt;
 where $\text{mother}_j$ and $\text{birth}_j$ are binary variables equal to 1 if article $j$'s authors were all mothers to children younger than five and gave birth, respectively, at some point during peer review, $\max t_j$ is the number of prior papers published in any of the top four economics journals by article $j$'s most prolific co-author, $\text{no. pages}_j$ refers to the page length of the published article, $\text{order}_j$ is the order in which article $j$ appeared in an issue and $\text{no. citations}_j$ are the number of subsequent papers citing $j$.
&lt;!--\input{$HOME/Dropbox/Readability/draft/tex/generated/table11}--&gt;
[](#table11) displays results across a range of specifications. Column (1) does not control for motherhood or childbirth; (2) drops papers authored by women who had children younger than five and/or gave birth during peer review; (3) controls for motherhood but not childbirth; (4) controls for childbirth but not motherhood; (5) controls for both childbirth and motherhood; (6) includes fixed effects for primary *JEL* categories.
Every paper published in *Econometrica* undergoes extensive review, but the consistently large and highly significant coefficient on female ratio suggests women bear the worst of it. The average male-authored paper takes 18.5 months to complete all revisions; papers by women need more than half a year longer.
Why? Well, it's not motherhood. Yes, giving birth slows down review---responding to referees is apparently put on hold for the first year of a newborn's life---but having a young child has the opposite effect. A pause for childbirth is expected; a productivity boost from pre-schoolers is not. Perhaps wanting to spend time with the kids motivates women to get organised? Or, maybe the most organised women are the only ones having children? The former suggests motherhood is not the productivity killer it's rumoured to be---at least among highly educated women. The latter implies only superstar women feel academic careers and motherhood are simultaneously manageable. Both interpretations are provocative, but should be made with caution given (i) counter-intuitive results, (ii) obtaining an unbiased estimate of $\beta_2$ was *not* this study's objective and (iii) $\text{mother}_j$ equals one for only 16 articles in the sample.
As for [](#table11)'s remaining coefficients, all are significant or highly significant and correspond to earlier estimates by [#Ellison2002a;]. Longer papers take more time to review, as do papers with more co-authors and those that appear earlier in an issue. Authors with an established publication history and highly cited papers (possibly) enjoy marginally faster reviews.</Text>
            <Comments>Submit-accept times were not available for four of these articles (see [][Data]).
Three out of four of the papers with the longest peer review are authored by at least one woman. Almost a quarter of the papers that took longest in peer review post 2000 had at least one female author.
If one co-author goes on maternity leave or has young children, I assume another co-author manages the revision process unless she, too, faces similar family commitments.
I control for all significant factors identified by [#Ellison2002a;]. His work evaluates whether author compositional effects contributed to higher mean-accept times at *AER*, *Econometrica*, *JPE*, *QJE* and the *Review of Economic Studies*. 
*JEL* classifications are only available for papers published after 1990 (see [][Data]); [](#table11)'s column (6) estimates [](#equation13) on only half of the data.
This conclusion is robust to altering the age-threshold on $\text{mother}_j$ (see [][AppendixMotherhood]).
&lt;!--\label{Footnote88}--&gt;Based on results in (5). Male effect estimated with zero female co-authors (standard error 0.102). When publication year fixed effects are replaced with submission year fixed effects, female-authored papers spend about four months longer in peer review (see [][AppendixSubmissionYear]).
A third hypothesis is that referees (possibly responding to editors) demand fewer revisions when women have young children. Because reviewers are unlikely to have this information---based on my own experience, it is remarkably difficult to find out---I (perhaps unfairly) give this interpretation less weight.
The count increases to 17 and 19 articles when $\text{mother}_j$'s threshold is defined as children younger than ten and 18, respectively (see [][AppendixMotherhood]).
[#Ellison2002a;]'s analysis includes a dummy variable for female authorship; it is positive post-1990 but not significant (it is negative and insignificant before that). His paper does not discuss the finding.</Comments>
        </Document>
        <Document ID="A7F250A5-8D54-4506-A83A-8AEAA205E166">
            <Title>dissertation</Title>
            <Text>LaTeX input: aer-style-article-header
Title: &lt;$title&gt;
My Short Title: &lt;$custom:ShortTitle&gt;
Author: &lt;$author&gt;
Affiliation: &lt;$custom:Affiliation&gt;
Email: &lt;$custom:E-mail&gt;
My Abstract: &lt;$synopsis&gt;
Thanks Bitches: &lt;$custom:Thanks&gt;
Base Header Level: 2
LaTeX input: aer-style-article-begin
LaTeX footer: aer-style-article-footer</Text>
        </Document>
        <Document ID="6242A060-EDB5-404F-9160-EACCE6C41F6E">
            <Title>Empirical consistency</Title>
            <Text>If topic, novelty and quality are appropriately controlled for, then discrimination is present when [](#Theorem1)'s three conditions hold at large enough $t$. In this section, I evaluate whether each condition holds, on average, using the entire sample of authors. In [][SEUMatching], I use a matching procedure to identify [](#Theorem1) and generate a conservative estimate of discrimination's impact on [readability](#Corollary1).
Consider first Condition 3---female-authored papers are accepted no more often than male-authored papers. The articles I evaluate have already been accepted, precluding gender analysis of acceptance rates. [][SEUMatching] and [][AppendixConservative] use lifetime publication counts to partially overcome this. The measure, unfortunately, embodies obvious imperfections.
Luckily, gender's impact on acceptance rates has been extensively studied elsewhere. To the best of my knowledge, publication outcomes expose no female advantage anywhere, ever. [][#Blank1991;] found that 12.7 and 10.6 percent of male- and female-authored papers were accepted at the *American Economic Review* , respectively. A study of *JAMA*'s editorial process indicated that 44.8 percent of referees accept male-authored papers as is or if suitably revised; 29.6 percent summarily reject them. Corresponding figures for female-authored papers were 38.3 and 33.3 percent, respectively[][#Gilbert1994]. There are also no gender differences in acceptance rates to NBER's Summer Institute programme[#Chari2017]. [][#Ceci2014;] provide a much more comprehensive research review on the subject. Their conclusion: "When it comes to actual manuscripts submitted to actual journals, the evidence for gender fairness is unequivocal: there are no sex differences in acceptance rates." [\]\[p. 111][#Ceci2014].
The data more cleanly identify Conditions 1 and 2. As their careers advance, women do write more clearly: their average readability scores are 1-5 percent higher than the readability of their first papers; their latest papers 1--7 percent [higher][AppendixFirstLast]. For a man, however, his average and last paper may be more poorly written than the first.
&lt;!--\input{$HOME/Dropbox/Readability/draft/tex/fixed/figure3}--&gt;
[](#figure3) plots mean Flesch Reading Ease scores grouped by authors' $t\text{th}$ article; as the count increases, men and women diverge.[](#table8) tests significance of that divergence by FGLS estimation of [](#equation1) (omitting $R_{it-1}$) on subsamples corresponding to authors' first ($t=1$), second ($t=2$), third ($t=3$), fourth and fifth ($t=4\text{--}5$) and sixth and up ($t\ge6$) articles published in the journals and time periods covered by the data. Only marginal effects on co-authoring with women for female authors are shown ($\beta_1$). Final column is a population-averaged estimate on the pooled sample. Regressions in columns ($t=1$) to ($t\ge6$) are weighted by $1/N_j$ (see [][AuthorLevel]), standard errors adjusted for two-way clustering on editor and author and corrected for cross-model correlation. Final column estimates are unweighted, error correlations are specified by an auto-regressive process of order one and standard errors are clustered on author.
All figures agree---women write better---but the magnitude and significance of that difference increases as $t$ increases. Between columns ($t=1$) and ($t=2$), the gap marginally widens but is not significant; after that, it triples (at least); the increase is significant ($p&lt;0.05$) for all five scores. At higher publication counts, estimates are somewhat smaller than column ($t=3$)---but still larger than columns ($t=1$) and ($t=2$)---although figures are only weakly significant and suffer from very small samples of female authors.
&lt;!--\input{$HOME/Dropbox/Readability/draft/tex/generated/table8}--&gt;
First-time publications are not driving the observed readability gap. [](#figure3) suggests little or no gender difference when $t=1$; [](#table8) backs this up. Coefficients in column ($t=1$) are imprecise, roughly half the size of those from a pooled regression (last column) and a fraction the size of estimates in columns ($t=3$), ($t=4\text{--}5$) and ($t\ge6$). [Wald tests][AppendixEqualityTests] reject equality of $\beta_1$ in the first and third models at $p&lt;0.01$ for the Flesch Reading Ease, Flesch-Kincaid and SMOG scores and $p&lt;0.05$ for the Gunning Fog and Dale-Chall scores.
[](#tableC6) displays $\chi^2$ test statistics from Wald tests of [$\beta_1$](#equation1) equality across estimation results in [](#table8). 
&lt;!--\input{$HOME/Dropbox/Readability/draft/tex/generated/tableC6}--&gt;
&lt;!--\clearpage--&gt;</Text>
            <Comments>&lt;!--\label{Footnote67}--&gt;Comparing lifetime publication counts between equivalent authors accounts for most confounding factors except individual productivity---especially factors related to household responsibilities. Greater responsibility at home presumably does not affect readability (other than, perhaps, to push women's scores downward), but it may impact the number of papers women can write. As shown in [][Duration], however, motherhood responsibilities after childbirth *do not*, in fact, slow women down during the revision process---at least at *Econometrica*.
Women's double-blind acceptance rate was 10 percent (11 percent for men); their single-blind acceptance rate was 11.2 percent (versus 15 percent for men).
The figures presented here aggregate responses in Tables 3 and 4 from [\]\[p. 141][#Gilbert1994;]. They average all individual referee recommendations, of which papers usually received several.  The authors found no gender difference in final manuscript acceptance rates---although they did find that manuscripts with male corresponding authors were summarily rejected more often (41.7 percent as opposed to 37.4 percent for women).
No gender difference was found in the pooled sample, but male-authored papers submitted to finance workshops were two percent more likely to be accepted; the effect is weakly significant. NBER's annual Summer Institute Programme is a selective three week economics conference.
In an earlier version of this paper, I estimated the mean additional contribution each paper makes to an author's readability [\]\[pp. 23--24][#Hengel2016]. This analysis included the full set of controls used in [][AuthorLevel]. The results and conclusions were similar to those presented here.
See [][AppendixEqualityTests] for coefficient equality test statistics.
Figures in columns ($t=2$) and ($t=3$) of [](#table8) are roughly in line with third column estimates in [](#table7)---on average, $t=2.7$ for female-authored articles released first as NBER working papers.
Only 40 female authors have 4--5 publications in the data; 28 have six or more. (512 men have 4--5 publications; 545 have more than that.)</Comments>
        </Document>
        <Document ID="8DE86765-F692-4860-93CB-438B7EB12258">
            <Title>Publishing while Female</Title>
            <Synopsis>Using five well-known ``readability'' tests, I analyse every article abstract published in the top four economics journals since 1950. I) Abstracts written by women are 1--6 percent more readable than those by men. II) The gap is up to three times higher in published articles than in earlier, draft versions of the same papers. III) Women's writing gradually improves but men's does not---meaning the readability gap grows over authors' careers. I explore many interpretations; the simplest and most persuasive is that referees apply higher standards to women's writing, subjecting them to an added time tax. This last hypothesis is confirmed by submit-accept times at *Econometrica*: female-authored papers take six months longer to complete peer review.</Synopsis>
        </Document>
        <Document ID="2B88AA6F-A936-48C0-AA27-E8C6CF055C86">
            <Title>Credit rationing.</Title>
            <Text>Unfortunately, borrowing isn't guaranteed. A higher $D$ affects risk twofold: (i) it increases entrepreneurs' chances of default; and (ii) reduces their desire to liquidate. Both effects result in more workouts and bankruptcy. Making either more probable reduces $\ol C_0$. It is entirely possible $\ol C_0$ does not cover the creditor's initial outlay no matter what $D$ the entrepreneur is willing to pay. Credit is rationed.
PROPOSITION
(i) If $\,\ol V_1^B\le\ol V_1^C$, some firms are credit rationed; the creditor is willing to lend if and only if $K_0\le\,\sup_D\ol C_0$. (ii) If $\,\ol V_1^C&lt;\ol V_1^B$, creditors are willing to lend at any $K_0\le D$ on the extended real number line such that $E_2^R=0$ for all $X_1$, $X_2$.
eprop
Credit rationing per [](#prop2)(i) happens to both viable and non-viable firms. For the former, criteria are straightforward. Because $\ol C_0$ is increasing in $D$, its supremum is its limit as $D$ tends to infinity; viable firms are credit-rationed whenever
$$!e[p1equation4]\lim_{D\ra\iy}\ol C_0=\int\!\lim_{D\ra\iy}\ol C_1^B\,\dd\hat\Pi=\ol V_0^B&lt;K_0.e!$$
Viable projects are funded only when their time 0 expected returns in bankruptcy are enough to cover the loan's principal. When it can't, the firm is credit-rationed.
Credit rationing is actually less of a problem for non-viable projects. For a start, more are liquidated in bankruptcy. As shown in the proof of [](#prop2),
$$!e[p1equation5]\ol C_0=\ol C_0^B+\int_{\mc{L^\star}}\!\l(D-\ol C_1^B\r)\,\dd\hat\Pi,e!$$
 where $\mc{L^\star}$ is the set of outcomes in which the firm is voluntarily liquidated. When $D$ tends to infinity, every firm either engages in a workout or goes bankrupt, making [](#p1equation4) also the limit of [](#p1equation5). Because $q=1-p$, however, non-viable projects more easily attain $K_0\le\ol V_0^B$ than do viable ones for project parameters within a sufficiently small neighbourhood of $\ol V_1^C=V_1^L$. And although those that satisfy [](#prop2)(i) do not go bankrupt, their higher probability of being liquidated if they did increases the value of creditors' outside option during a workout. Entrepreneurs have less scope to extract concessions *ex interim*. They are rewarded with better lending opportunities *ex ante*.
Lending to non-viable firms is safer for another reason: they often forgo workouts to voluntarily liquidate. When loaning money to viable projects, creditors expect $\ol C_0^B$; when lending to non-viable projects they earn $\ol C_0^B$ plus a "bonus"---the integral in [](#p1equation5)---pushing the supremum of [](#p1equation5) beyond [](#p1equation4). Even if judges reorganised non-viable and viable projects at the same rate, the latter would still have a tighter credit market. From the creditor's perspective, liquidation is safer so lending is safer when entrepreneurs want it more often.
Arguably, one group of non-viable firm has no problem ever getting credit---those that sometimes file for bankruptcy ([](#prop2)(ii)). Any firm guaranteed to liquidate at time 1 is guaranteed a loan. With workouts on the table, however, this promise is not credible *ex ante* because lenders capitulate *ex interim*. For those that go bankrupt, however, creditors resist. Couple that resistance with an excessively high $D$ and entrepreneur returns in bankruptcy are exactly what they would be in liquidation: nothing. Assuming firms choose the efficient option when their earnings from doing so are no worse than the alternatives, liquidation at time 1 is guaranteed.
Entrepreneurs take out high interest rate loans per [](#prop2)(ii) only as a last resort. Creditors, on the other hand, love them ([](#cor1)). They are risk-free, yet generate a long-run rate of return above the risk-free rate. Not bad for a zero-profit industry.
COROLLARY
Loans made per \autoref{prop2}(ii) satisfy $\ol E_0=0$ and $K_0&lt;\ol C_0$.
ecor
Creditor profits are a byproduct of guaranteed liquidation. Lending is predicated on avoiding bankruptcy and exorbitant rates do this---but only by conferring the creditor absolute rights to $X_1$. Since creditor profits are a side effect rather than the source of lending, no amount of competition will drive them back to zero.</Text>
            <Comments>$C_2^B\le D$ for all $X_1$, $X_2$ making the integral in [](#p1equation5) non-negative.
The real world relevance of [](#prop2)(ii) depends crucially on whether firms actually do liquidate under such conditions. Evidence in the home mortgage market suggests they do. Lenders are more frequently and rapidly transferred ownership of houses from delinquent borrowers with little to no equity in their homes[#Ambrose1998,Pennington-Cross2010]. Borrowers' quick surrender may be especially likely when bankruptcy incurs a personal cost, such as lost time. See also [](#p1footnote3) for a discussion on the likelihood of voluntary liquidation and the entrepreneur's original intent to liquidate.
Note that $\ol V_1^C&lt;\ol V_1^B$ is not sufficient to guarantee loans are made as in [](#prop2)(ii)---only that such loans exist. Cheaper loans, if obtainable, are preferred ([](#cor1)).</Comments>
        </Document>
        <Document ID="10F9894E-55C7-4312-92A8-7A7F80C9058D">
            <Title>Alternative program for calculating readability scores</Title>
            <Text>The following tables repeat the principal analyses from the paper---*i.e.*, those from [](#table4), [](#table5), [](#table7) and [](#table11)---using the R package "readability", an alternative program to calculate the Flesch-Kincaid, Gunning Fog and SMOG readability scores. (The program does not calculate the Flesch Reading Ease or Dale-Chall scores.) As can be seen from the tables, the coefficients are very similar or identical to those presented in the main paper; standard errors are slightly smaller.
&lt;!--\input{$HOME/Dropbox/Readability/draft/tex/generated/tableApp4}--&gt;
&lt;!--\input{$HOME/Dropbox/Readability/draft/tex/generated/table5APPENDIX1}--&gt;
&lt;!--\input{$HOME/Dropbox/Readability/draft/tex/generated/table7ALTERNATIVE}--&gt;
&lt;!--\clearpage--&gt;</Text>
        </Document>
        <Document ID="21787956-28FB-4804-A99E-9DE66DFC8066">
            <Title>Subfield</Title>
            <Text>[](#p3table5b) displays $\beta_c$ from estimating [](#eq:p3eq2)where $\text{score}_{j_k}^s$ is the $k\text{th}$ copy of readability score $s$ for article $j$, and subscript $c_{j_k}$ denotes article $j\text{'s}$ $k\text{th}$ *JEL* code.
According to at least one readability score, gender differences are less pronounced in subfields B (history of economic thought, methodology and heterodox approaches), F (international economics), H (public economics), M (business administration and business economics; marketing; accounting), O (economic development, technological change and growth) and R (regional, real estate and transportation economics).
Abstracts from subfield O are still better written when authored by women according to three of the five scores; marginal effects for B, F, H and R are not significantly different from zero. Weak evidence points to men as the better writers in subfield M: its Flesch-Kincaid score suggests female-authored abstracts take !!FleschKincaidM more years of schooling to understand (standard error !!FleschKincaidse); effects for the other scores are insignificant.
Papers classified as D (microeconomics), N (economic history) or Q (agricultural and natural resource economics; environmental and ecological economics) have more pronounced gender differences in readability---female-authored abstracts read !!DBetter, !!NBetter and !!QBetter times better than the average, respectively. All readability scores agree, are significant on their own and most are significantly higher than the mean effect.
In contrast to my expectation, more women in a field may not necessarily be an antidote to gender differences in readability. While D, N and Q are about average in terms of total female co-authors, they vary widely in how frequently their papers name only female co-authors. Q is in the !!QFemOnly, D the !!DFemOnly, N the !!NFemOnly. Admittedly, !!NWomen percent is itself pretty low; economic history papers are still overwhelmingly---as in !!NMen percent---penned just by men. But given the readability gap is present in subfields with both above- and below-average rates of sole female authorship, women may need to be better writers even where more of them publish.</Text>
            <Comments>\input{$P3/equations/eqn2.tex}
\input{$P3/tables/tex/table5b.tex}
Due to small sample sizes (particularly of female authors) few *JEL*-specific marginal effects are significant on their own. See the [Appendix](#p3table5a) for full regression output).
Women make up roughly !!DFemRatio of all authors in articles classified as D, !!QFemRatio in articles classified as Q and !!NFemRatio in articles classified as N.</Comments>
        </Document>
        <Document ID="C091CDEE-11CC-4EAF-92AA-166B5C1CB38D">
            <Title>abstract</Title>
            <Text>&lt;!--begin{abstract}

*Chapter 1*. &lt;$synopsis&gt;

*Chapter 2*. &lt;$synopsis&gt;

*Chapter 3*. &lt;$synopsis&gt;

\end{abstract}--&gt;</Text>
        </Document>
        <Document ID="EAA52082-F700-45E5-B5C7-E000FBF4FA4A">
            <Title>Data</Title>
            <Text>&lt;!--\input{$HOME/Dropbox/Readability/draft/tex/generated/table1}--&gt;
The data include every English article published in *AER*, *Econometrica*, *JPE* and *QJE* between January 1950 and December 2015 (inclusive). The largest sample comes from *Econometrica* which consistently published abstracts with its articles prior to 1950. *JPE* added them in the 1960s and *QJE* in 1980. *AER* came last in 1986. [](#table1) displays data coverage by journal and decade.
The analysis in [][NBER] matches published articles with NBER working papers. Matches were first attempted using citation data from RePEc and then by searching NBER's database directly for unmatched papers authored by NBER family members. 1,986 published articles were eventually matched to 1,988 NBER working papers---approximately one-fifth of the data. Bibliographic information and abstract text were scraped from [www.nber.org](http://www.nber.org). See [][NBERStats] for a further description and summary statistics of the data.
The analysis in [][Duration] compiles submit-accept times at *Econometrica*---the only journal that makes any kind of disaggregated data on the revision process publicly available. I extracted this information from digitised articles using the open source command utility `pdftotext`. See [][Duration] for summary statistics and a further description.
Controls used in the analysis include, *inter alia* editor fixed effects, institution fixed effects, author productivity fixed effects, English fluency controls, citation counts, and controls for motherhood and childbirth in [][Duration]. See [][AppendixControls] for a detailed description of how these controls were generated.
&lt;!--\input{$HOME/Dropbox/Readability/draft/tex/fixed/year_jel}--&gt;</Text>
            <Comments>Unless otherwise mentioned, observations exclude the May issue of *AER* (*Papers &amp; Proceedings*).
Because a small number of NBER working papers were eventually published as multiple articles or combined into a single paper, the mapping is not one-for-one.
Printed at the end of every *Econometrica* article published on or after March 1970 that was not originally presented as an Econometric Society lecture is the date it was first submitted and the date final revisions were received. Before 1970, only "A Capital Intensive Approach to the Small Sample Properties of Various Simultaneous Equation Estimators" (January, 1965) included this information. "Separable Preferences, Strategyproofness, and Decomposability" (May, 1999) only printed the year of submission; I assume the month is January. </Comments>
        </Document>
        <Document ID="8B3CC743-4F0F-4EE0-B801-66683C9DF7E7">
            <Title>Contracts and credit</Title>
            <Text>Whatever the interim outcome, entrepreneurs' returns are aggregate earnings, $\ol V_0$, less the amount paid to creditors, $\ol C_0$, making their unconditional expected value
$$!e[p1equation2]\ol E_0=\ol V_0-\ol C_0.e!$$
The optimal contract maximises $\ol E_0$, subject to the constraint that creditors' anticipate at least the principal amount of their loan, *i.e.*, $K_0\le\ol C_0$. Given $\ol V_0$ and $\ol C_0$ are positive, it is obvious from [](#p1equation2) that $\ol E_0$ is increasing in the former and decreasing in the latter; it is less obvious, however, how each reacts to $D$.
Start with viable firms. Viable projects operate both periods whether solvent or not---expected aggregate earnings are constant, making $\p\ol V_0/\p D=0$. Solvent firms satisfy $D\le X_1$ so in *any* outcome creditors receive $D$, meaning $\ol C_1^B=D$. Consequently, $\ol C_0=\ol C_0^B$, where $\ol C_0^B$ is the unconditional expected value of creditor returns in bankruptcy. Since $\ol C_0^B$ is increasing in $D$, $\ol E_0$ must be decreasing in it. Entrepreneurs optimally set $D$ as low as possible: $\ol C_0=K_0$.
For non-viable firms it is no longer true that $\ol V_0$ is unaffected by $D$ or that $\ol C_0$ is always increasing in it. As shown in the proof of [](#lem3), the change in $\ol C_0$ with respect to $D$ is
$$!e[p1equation3]\frac{\p\ol C_0}{\p D} = \int_{\mc L}\!\frac{\p C_1^L}{\p D}\,\dd\hat\Pi+\int_{\mc R}\!\frac{\p\ol C_1^R}{\p D}\dd\hat\Pi + \frac{\p\ol V_0}{\p D},e!$$
 where $\mc L$ and $\mc R$ are the set of outcomes in which creditors accept payoffs consistent with liquidation and reorganisation, respectively, and $\hat\Pi$ their joint cumulative distribution function.
$\p\ol V_0/\p D$ is negative but $\p\ol C_1^R/\p D$ and $\p C_1^L/\p D$ are positive so $\p\ol C_0/\p D$ may be positive or negative. A larger $D$ increases creditors' claims to eventual earnings. This direct effect is the sum of the first two terms in [](#p1equation3). A higher $D$, however, also makes workouts and bankruptcy more attractive, depressing $\ol V_0$ and thereby indirectly reducing $\ol C_0$: the third term of [](#p1equation3).
Combine [](#p1equation3) with the partial derivative of [](#p1equation2) with respect to $D$, and $\p\ol E_0/\p D$ is simply the negative direct effect $D$ has on creditors' returns. Although $D$ affects entrepreneurs' decisions at time 1, and that decision affects expected project value at time 0, its cost is borne only by lenders so entrepreneurs ignore it and opt for the smallest $D$ they can; again, $\ol C_0=K_0$ ([](#lem3)).
LEMMA
The entrepreneur prefers the smallest $D$ such that $\ol C_0=K_0$.
elem
The main feature of the optimal contract is that every entrepreneur wants the cheapest loan he can get his hands on. He may have no choice---for viable projects $\p\ol C_0/\p D$ is positive meaning at most one $D$ exists such that $\ol C_0=K_0$. Possibilities are broader for non-viable projects. $\p\ol C_0/\p D$ is neither definitively positive or negative; it isn't necessarily even monotone. Consequently, the $D$ at which $\ol C_0=K_0$ may be on the upward or downward slope of $\ol C_0$ or even occur at several places ([](#p1figure2)).</Text>
            <Comments>Both $C_1^L$ and $\ol C_1^R$ are non-decreasing in $D$, making $\p \ol C_1^B/\p D$ non-negative.
There is one technical (and rather pedantic) exception: when $\ol C_0=K_0$ on a non-trivial closed interval.
\input{$PPATH/p1/figures/tex/p1figure2}</Comments>
        </Document>
        <Document ID="ECC1FB8B-17EA-40D7-B78A-7433F847347C">
            <Title>[][Duration], supplemental output</Title>
        </Document>
        <Document ID="8173FF92-6FD0-4429-A7DA-457ABF1D6EE8">
            <Title>Condition 2</Title>
        </Document>
        <Document ID="905F92B9-1587-4170-AA3E-AE56BFE8DE0F">
            <Title>Untitled</Title>
        </Document>
        <Document ID="0AA73007-2541-415F-A159-0C285887C000">
            <Title>Introduction-2</Title>
            <Text>In economics, theoretical and empirical research on discrimination tends to focus on stereotype formation and belief structures motivating discriminatory actions[*e.g.*,\]\[][#Becker1957,Phelps1972,Arrow1973,Coate1993,Bordalo2016]. This paper exclusively explores, in a non-laboratory environment, discrimination's impact on the behaviour and choices of people discriminated against.
This perspective has two advantages. First, it offers an alternative framework for studying the phenomenon. Discrimination is typically identified from the actions[*e.g.*,\]\[][#Neumark1970,Bertrand2004] and/or learning processes[*e.g.*,\]\[][#Altonji2001,Fryer2013] of those who discriminate. Within a subjective expected utility framework, I show that authors' choices also reveal discrimination by editors and/or referees. Although context-specific, the model's basic logic---and its method of identifying discrimination---apply equally well to situations where people are repeatedly judged on and respond to feedback about some quantifiable component of their output.
Second, analysing discrimination from the perspective of people discriminated against forces us to think more deeply about its impact on, *inter alia*, occupational choice, worker motivation, human capital investment and, especially, productivity measurement. This paper joins a small, emerging literature examining these effects[*e.g.*,\]\[][#Parsons2011,Craig2017,Glover2017,Lavy2015].
Higher standards cause collateral damage to women's productivity. Unequal time spent making revisions leads to unequal time conducting new research; as a result, women write fewer papers. Fewer papers justifies lower promotion rates. If women seek fairer employment elsewhere---or quit the labour force entirely---it feeds a "Leaky Pipeline".
Moreover, I also find evidence that female authors internalise tougher standards with strategies that disguise the underlying discrimination as voluntary choice. Women increasingly submit better written papers *ex ante* to offset biased evaluation *ex post*, meaning the readability gap between senior economists largely forms prior to---therefore appearing independent of---peer review. This pattern of behaviour obscures the line between personal preferences and external constraints and hints that academia overlooks other biases within its ranks.
Although analysed in a specific context---academia---higher standards impose a quantity vs. quality tradeoff that characterises many instances of female output. According to raw numerical counts, women produce less than men. Female reporters write fewer front-page bylines[#Klos2014]; female real estate agents list fewer homes[#Seagraves2013]; female physicians see fewer patients[#Bloor2008] and submit fewer grant proposals[#Gordon2009]; female pharmacists and lawyers work and bill fewer hours, respectively[#Azmat2017,Goldin2016].
When ranked by narrowly defined outcome measures, however, women often outperform. Female students earn better grades[#Voyer2014]; female auditors are more accurate and efficient[#Chung2001,ODonnell2001,Ittonen2013,Niskanen2011]; congresswomen secure more federal funding for their districts, sponsor more legislation and score higher on a composite measure of legislative effectiveness[#Anzia2011,Volden2013]; houses listed by female real estate agents sell for higher prices[#Seagraves2013,Salter2012]; patients treated by female physicians are less likely to die or be readmitted to hospital[#Tsugawa2016]; female pilots are involved in fewer fatal accidents[#Vail1986,Bazargan2011]; female economists write more clearly.
Additionally, if---like senior female economists---women internalise higher standards in somewhat roundabout ways, they could contribute to other labour market phenomena: sectoral and occupational concentration[#Blau2016,Cortes2016,Pertold-Gebicka2016]; women's tendency to under negotiate pay[#Babcock2003] and apply only to jobs they feel fully qualified for[#Mohr2014]. They may likewise reinforce work habits---*e.g.*, conscientiousness, tenacity and diligence---that correlate with quality and connote "femininity": female physicians consult longer with patients[#Roter2004]; female politicians fundraise more intensely[#Jenkins2007]; female faculty commit fewer instances of academic misconduct[#Fang2013]; female lawyers make fewer ethical violations[#Hatamyar2004]; female pharmacists are less likely to face performance-related disciplinary action[#Schafheutle2011].
Higher standards therefore offer another perspective to the gender gap in labour market outcomes. Traditional hypotheses focus on obvious discrimination[#Goldin2000], motherhood[#Bertrand2010] and differences in behaviour[*e.g.*,\]\[][#Niederle2010]. Contemporary theories stress inflexible working conditions[#Goldin2014,Goldin2016], preferences[for a review, see, *e.g.*,\]\[][#Blau2016] and policy design[#Antecol2016]. Still other research---which this paper joins---target more subtle forms of discrimination[*e.g.*,\]\[][#Sarsons2017,Wu2017]. The gap probably emerges from all of these factors---and possibly many that are not yet identified. Equality means levelling the playing field for every one.
Furthermore, my results advocate using caution when employing performance indicators in equations relating earnings (or other labour market outcomes) to gender. Higher standards raise quality at the expense of quantity. Performance indicators that weight the latter's fall more heavily than the former's rise will appear artificially low. If used to interpret gender wage gaps, they will undervalue women's work and confound estimates of labour market discrimination. A similar argument was recently made in a study of racial preferences in Major League Baseball.[#Parsons2011;] find that race affects umpire calls, umpire calls influence players' behaviour and players' behaviour impacts performance metrics. As a result, common baseball statistics underestimate the talent of disadvantaged (usually minority) pitchers and overestimate the talent of advantaged (usually white) pitchers. An important contribution of my paper is to confirm this general point both in the context of gender discrimination and within a highly educated, professional working environment.
This paper makes two final contributions. First, it adds to extensive (ongoing) research into peer review. Although mine, to the best of my knowledge, is the first to suggest and document evidence of gender bias in the peer review process (as opposed to its outcome), it joins contemporary or parallel research studying patterns in the editorial process[#Ellison2002a,Card2013,Clain2017] and bias in editorial decisions[#Abrevaya2012,Card2017,Bransch2017].
Second, my findings emphasise the importance of transparency and monitoring. The least intrusive antidote to implicit bias is simple awareness and constant supervision. Unlike referee reports, journal acceptance rates are easy to measure and frequently audited; both factors foster accountability and encourage neutrality[#Foschi1996]. Monitoring referee reports is difficult, but it isnât impossible---especially if peer review were open. As discussed in [][Discussion], several science and medical journals not only reveal referees' identities, they also post reports online. Quality does not decline (it may actually increase), referees still referee (even those who initially refuse) and, given what's at stake, an extra 25--50 minutes spent reviewing seems tolerable[#vanRooyen1999,vanRooyen2010,Walsh2000].
The remainder of the paper proceeds in the following order. [][Data] describes the data and readability measures. Analyses and results are presented in [][Results]. They are succeeded by a detailed [discussion][Discussion] and [conclusions][Conclusion].</Text>
            <Comments>A parallel research thread examines the broader impact of external signals (discriminatory or not) on women's behaviour[#Kugler2017].
A similar idea was also recently proposed in the philosophy literature[see\]\[][#Bright2017,Lee2016].
Evidence on whether female academics are hired and promoted at lower rates is mixed. One study suggests so-called STEM (science, technology, engineering, mathematics) fields actually prefer hiring women---although male economists continue to show a slight (but not significant) preference for men[#Williams2015]. Other studies find male candidates are preferred in postdoctoral research and laboratory management positions[#Moss-Racusin2012,Sheltzer2014]. Men are also more likely granted tenure when compared to women with an identical publication history[#Weisshaar2017] or for co-authored work[#Sarsons2017]. A study specific to the London School of Economics found female academics earn 12% less than men with identical experience and research productivity[#Bandiera2016].
[#Bloor2008;]'s analysis considers only full-time (or maximum part-time), salaried physicians in the U.K. Similar results are found in Canada and the U.S., where physicians are paid on a per-service basis[#CICH2005,Benedetti2004].
[#Seagraves2013;] find that normal houses (*i.e.* homes not sold under special sales conditions, such as foreclosures, fixer-uppers, corporate-owned properties, transfers and estate sales) sell at a significantly higher price when listed by a female real estate agent. The authors also find buyers pay less if they are represented by a male agent---although the effect is only present for homes sold under special sales conditions. An earlier study did not find any significant gender difference in selling performance for listing and selling agents[#Turnbull2007].
The evidence on general accident rates (including non-fatal accidents) is mixed. [#McFadden1996;] found no difference in female vs. male accident rates after adjusting for pilot experience and age. [#Walton2016;] found female accident rates were higher than male accident rates among inexperienced pilots but lower among experienced pilots.
A more recent study suggests women do ask for higher pay---they just don't get it[#Artz2016].
Female politicians target a larger variety of potential donors using a wider array of methods (direct mail, television advertisements, *etc.*)[#Jenkins2007].
Evidence in several countries suggests female pharmacists are less likely to commit criminal offenses (prescription fraud, drug trafficking, *etc.*) and minor professional misdemeanours (inadequate written records, stock, *etc.*)[#Tullett2003,Payne1997]. Self-reported survey evidence does not suggest female pharmacists make fewer dispensing errors[#Szeinbach2007]; evidence from a laboratory experiment indicates the opposite[#Family2013]. Similar gender trends have been found for physicians, dentists and other medical professionals[for a review of studies and discussion, see\]\[][#Firth-Cozens2008].
Another recent study might also illustrate this point.[#Glover2017;] find that obvious productivity measures decline when minority grocery store workers are overseen by biased managers. If due to demotivation or inattention by managers---as the authors propose---their behaviour reinforces statistical discrimination. On the other hand, slower checkout times, less overtime work and seeing fewer customers could result from biased managers being more critical of minorities' work (*e.g.*, minority workers are more likely to be punished for an incorrect amount of money in the till, not immediately clocking out at the end of a shift or accidentally scanning a single item multiple times).</Comments>
        </Document>
        <Document ID="9CCC98A2-CA39-4B40-BFE1-A8A3E0BD9FC5">
            <Title>Supplemental analysis</Title>
        </Document>
        <Document ID="2C2059F4-D331-439A-93B1-D9A8578F887F">
            <Title>Appendices</Title>
        </Document>
        <Document ID="AF603859-1549-4308-8D91-C534680FADC1">
            <Title>[](#table5), alternative measures of an article's "gender"</Title>
            <Text>[](#tableC5a), [](#tableC5b) and [](#tableC5c) repeat the analysis shown in [](#table5) using three alternative measures of an article's "gender". In [](#tableC5a), papers authored entirely by women are compared to papers authored entirely by men. In [](#tableC5b), papers are considered "female" if at least one author is female. In [](#tableC5c), article gender is represented by a binary variable equal to one if at least half of all authors are female; articles below that threshold with at least one female author are excluded.


&lt;!--\clearpage--&gt;</Text>
        </Document>
        <Document ID="28B4E2C9-F726-4822-A245-EF9F1C2B7CDE">
            <Title>Matching</Title>
            <Text>In light of [](#Theorem1), the preceding evidence forcefully hints that academic publishing is biased against female economists: on average, female-authored papers are accepted no more often than male-authored papers (Condition 3), yet women improve their writing over time (Condition 2) and write better than men at all $t$ (Condition 1).
Nevertheless, the set of women to satisfy one condition is conceivably orthogonal to sets that satisfy others; for [](#Theorem1) to apply, they must overlap. To address this concern, I match female to male authors on characteristics that predict the topic, novelty and quality of research. In addition to explicitly accounting for author equivalence---the primary conditional independence assumption behind [](#Theorem1)---matched pair comparisons: (i) identify the gender most likely to satisfy all conditions simultaneously; and (ii) generate (conservative) estimates of the effect of higher standards on authors' [readability](#Corollary1).</Text>
        </Document>
        <Document ID="E0305587-9547-4338-BCF1-EC4994D166D6">
            <Title>Theoretical framework</Title>
            <Text>To organise the analysis, I develop a simple dynamic model of readability's marginal impact on an author's decision making process. It follows an author---denoted by $i$---who publishes several articles in prestigious academic journals over the course of his career. Each article is roughly equivalent in terms of topic, novelty and quality, but varies on readability.
At stage 0, author $i$ drafts his $t$th paper and submits it for peer review. Upon receipt, the journal's editorial office assigns the manuscript to a group of referees. The (finite) set of all potential review groups is represented by $\Sigma$; $\mu_i$ is the set of strictly positive probability measures on $\Sigma$. $\Sigma$ and $\mu_i$ are known to $i$.
Let $r_{0it}$ and $\widetilde r_{0i}^s$ denote manuscript $t$'s non-negative draft readability and the initial rejection threshold review group $s\in\Sigma$ applies to all papers by author $i$, respectively. $s$ rejects the paper at stage 0 if $$r_{0it}&lt;\widetilde r_{0i}^s.$$ $i$ is otherwise granted a "revise and resubmit" (R&amp;R), yet could still be rejected at stage 1 if the readability of his revised manuscript, $R_{it}=r_{0it}+r_{1it}$, does not meet a second threshold, $$R_{it}&lt;\widetilde R_i^s,$$ where $\widetilde R_i^s=\widetilde r_{0i}^s+\widetilde r_{1i}^s$. All rejections and acceptances are final. $\widetilde R_i^s\ne\widetilde r_{0i}^s$ to account for different standards at different stages of peer review. $r_{1it}$, $\widetilde r_{0i}^s$ and $\widetilde r_{1i}^s$ are non-negative; the latter two are independent.
To aid the revision process, $s$ writes a referee report from which $i$ forms expectations about $\widetilde R_i^s$ by assigning subjective probabilities $\pi_{1it}^s(R)$ to all $R$. Unfortunately, the concept of readability is complex, some referees write insufficiently detailed reports and inattentive or hypersensitive authors misconstrue even perfectly clear advice. This renders $i$'s interpretation of the report imprecise and his subsequent expectations about $\widetilde R_i^s$ inexact and possibly specious.
Conditional on $r_{0it}$, I assume referee reports by $s$ for $i$ are the same for all $t$ and that each is distinctive enough for $i$ to distinguish $s$ in $\Sigma$. Consequently, author $i$'s stage 1 choice of $R_{it}$ maximises his (immediate) subjective expected utility given $s$,
&lt;!--\input{$HOME/Dropbox/Readability/draft/equations/equation8}--&gt;
 $\Pi_{1it}^s(R_{it})$ is the cumulative sum of $\pi_{1it}^s(R)$ for all $R\le R_{it}$; $u_i$ is the utility of having a paper accepted in a prestigious journal; $\phi_{i|r_{0it}}(r_{1it})=\phi_i(R_{it})-\phi_i(r_{0it})$ and $c_{i|r_{0it}}(r_{1it})=c_{i}(R_{it})-c_{i}(r_{0it})$ are the satisfaction and cost, respectively, from making changes $r_{1it}$ given the paper's initial readability $r_{0it}$. $\phi_i$ is increasing and concave in its arguments, $c_i$ increasing and convex---marginally higher $R_{it}$ generates proportionally less satisfaction but needs more effort when the paper is already well written. $c_i(0)$ and $\phi_i(0)$ are 0.
Authors' decisions at stage 0 are myopic; $i$'s choice of $r_{0it}$ maximises his initial subjective expected utility for the current paper,
&lt;!--\input{$HOME/Dropbox/Readability/draft/equations/equation9}--&gt;
 where $\Pi_{0it}^s(r_{0it})$ is the cumulative sum for all $r\le r_{0it}$ of author $i$'s subjective probabilities $\pi_{0it}^s(r)$ about $\widetilde r_{0i}^s$; $v_{1it}^s$ is [](#equation8) evaluated at the optimal $r_{1it}$.
Authors update subjective probabilities (i) using relevant information from their own experience in peer review; and (ii) by observing others' readability choices and publication outcomes. When evidence from (i) contradicts evidence from (ii), (i) takes precedence. These assumptions imply, at a minimum, that $i$ updates $\Pi_{0it}^s$ and $\Pi_{1it}^s$ based on conclusive evidence derived from the choices and outcomes of [equivalent peers](#Definition1) and knowledge acquired during his own prior experience in peer review.
&lt;!--\input{$HOME/Dropbox/Readability/draft/theorems/definition}--&gt;
[](#equation8) and [](#equation9) incorporate a variety of factors that potentially affect authors' readability choices---editorial standards ($\widetilde r_{0i}^s$ and $\widetilde R_i^s$); ambition ($u_i$); the cost of drafting and revising manuscripts ($c_i$); an otherwise unexplained intrinsic satisfaction from writing readable papers ($\phi_i$). Poor information, overconfidence and sensitivity to criticism are not explicitly included, on the assumption that people do not *want* to be poorly informed, overconfident or excessively sensitive. These factors nevertheless enter [](#equation8) and [](#equation9)---and hence influence choices---via the subjective expectations authors form about $\widetilde r_{0i}^s$ and $\widetilde R_i^s$.
A single $R_{it}$ cannot, therefore, establish if and to what extent $i$'s choices are motivated by (a) preferences and costs specific to him ($u_i$, $\phi_i$, $c_i$), (b) editorial standards and/or referee assignment outside his control ($\widetilde r_{0i}^s$, $\widetilde R_i^s$, $\mu_i$) or (c) miscellaneous confounding factors mopped by $\Pi_{0it}^s$ and $\Pi_{1it}^s$. Since preferences and costs are time independent, however, an observed increase in $i$'s choice of readability at two separate $t$ distinguishes (a) from the combined impact of (b) and (c). $i$ may be more sensitive to criticism and he might prefer writing more clearly; nevertheless, he improves readability today relative to yesterday only when he believes it boosts his chances of publishing.
Moreover, because (c) does not reflect activities or states the author enjoys, its impact on choices declines with experience. Authors may miscalculate referee expectations and misconstrue their reports, but with experience they correct their mistakes. Having ruled out (a) and holding acceptance rates constant, this implies that a persistent readability gap between equivalent peers is caused by (b)---*i.e.*, editorial standards and/or referee assignment beyond authors' control.
I capture this idea in [](#Theorem1), where $\bm1_{0i}^s(r)$ and $\bm1_{1i}^s(R)$ are indicator functions equal to 1 if $r\ge\widetilde r_{0i}^s$ and $R\ge\widetilde R_i^s$, respectively, and $\Sigma_{A_{it}}$ is the collection of $s\in\Sigma$ for which $\bm1_{0i}^s(r_{0it})\bm1_{1i}^s(R_{it})=1$. [](#Theorem1) is proved in [](#appendixproofs).
&lt;!--\input{$HOME/Dropbox/Readability/draft/theorems/theorem}--&gt;
[](#Theorem1)'s three conditions are sufficient to verify discrimination in academic publishing: when female authors' unconditional probability of acceptance is no higher than men's (Condition 3), their current papers are more readable than their past papers (Condition 2) and also *persistently* more readable than men's papers (Condition 1) then either editors assign women "tougher" referees---*i.e.*, those with higher $\widetilde r_{0i}^s$ and/or $\widetilde R_i^s$---or referees apply higher standards to women's writing---*i.e.*, $\widetilde r_{0k}^s&lt;\widetilde r_{0i}^s$ and/or $\widetilde R_k^s&lt;\widetilde R_i^s$ for at least one $s\in\Sigma$.</Text>
            <Comments>Should $s$ review a future paper by $i$, $i$ would recognise it as the same (anonymous) group that reviewed his earlier paper. This does not imply that the report reveals individual referees' identities.
Authors probably care about getting their papers accepted and they may care about writing well, but their marginal utility from the intersection of the two events---*i.e.*, higher utility from writing well *only* because the paper is published in a top-four journal (as opposed to a top field journal or second-tier general interest journal)---is assumed to be negligible.
&lt;!--\label{Footnote64}--&gt;Specifically, if $i$ observes with probability 1 that in state $s$ an equivalent author $k$ receives an R&amp;R at $r_{0k}$, then $\Pi_{0it}^s(r)=1$ for all $r\ge r_{0k}$. Similarly, if $i$ observes with probability 1 that in state $s$, $k$ is accepted at $R_k$, then $\Pi_{1it}^s(R)=1$ for all $R\ge R_k$.
&lt;!--\label{Footnote70}--&gt;If $i$ is accepted at stage 1 in time $t'$ for review group $s$, then $\Pi_{1it}^s(R)=1$ for all $t&gt;t'$ and $R\ge R_{it'}$; otherwise, $\Pi_{1it}^s(R)=0$ for all $t&gt;t'$ and $R\le R_{it'}$. Similarly, if $i$ receives an R&amp;R at stage 0 in time $t'$ for review group $s$, then $\Pi_{0it}^s(r)=1$ for all $t&gt;t'$ and $r\ge r_{0it'}$; otherwise, $\Pi_{it}^s(r)\,\le\Pi_{it'}^s(r)$ for all $t&gt;t'$, $r\le r_{0it'}$ and $s\in\Sigma$.
The analysis in [][NBER] similarly establishes that (b) and/or (c) are significant factors driving the choice of $R_{it}$. It cannot, however, distinguish *between* (b) and (c).</Comments>
        </Document>
        <Document ID="1A7A1607-1167-44C7-ABAB-AC899C53BF8D">
            <Title>Robustness</Title>
            <Text>I attribute the difference between NBER abstracts and published abstracts to the peer review process. However, NBER has no word limit while two journals in my sample do: *Econometrica* limits abstracts to 150 words; *AER* limits them to 100 words. It is therefore possible that the effects we observe are due to authors compressing the content of their abstracts rather than due to peer review.
To consider this possibility, I repeat the analysis in [](#table7) but this time only on articles whose NBER abstracts fell below the official minimum word limit of the respective journal in which it was published.  Results are presented in [][AppendixWordLimits]. Standard errors are somewhat larger---the subsequent analysis effectively dropped about 40 percent of the observations---but coefficient magnitudes are very similar to those presented in [](#table7), suggesting results in [](#table7) are not driven by gender differences in how authors adjust their abstracts to adhere to journals' word limits.
A more serious threat to my identification strategy is violation of timing independence, the principle independence assumption required to causally link the readability gap with peer review. This assumption is violated if male and female authors differ in the changes they make to their manuscripts after releasing them as NBER Working Papers.
Nevertheless, manuscript changes post-submission are probably only made in the context of peer review---either because referees actually request them or authors believe (possibly mistakenly) they will be requested in a future revision. Thus, timing independence is likely only violated during a narrow window: after a manuscript is released as an NBER Working Paper but before it is submitted to a top-four journal.
[](#figure7) suggests that only a small fraction of papers are exposed to this window. It displays a histogram of the length of time between a manuscript's release as an NBER Working Paper and its submission to *Econometrica*. Negative values indicate a manuscript was released as an NBER Working Paper before submission to *Econometrica*---during which timing independence may have been violated. Positive values indicate the manuscript was released as an NBER working paper after submission---suggesting non-peer review factors are indeed independent. Pink represents papers with at least one female author; blue are papers with no female authors.
&lt;!--\input{$HOME/Dropbox/Readability/draft/tex/fixed/figure7}--&gt;
Most manuscripts published in *Econometrica* are submitted to peer review at the same time or before they are released as NBER Working Papers. This is particularly true for female-authored papers---only 15 percent are released as working papers first. Assuming similar submission-release patterns in *AER*, *JPE* and *QJE*, [](#figure7) suggests that violation of timing independence is only an issue in a small number of predominately male authored papers.
Moreover, a textual analysis of the acknowledgements sections suggests that most NBER Working Papers have been widely circulated among colleagues and presented at seminars and conferences prior to their release. The average length of an acknowledgement section in an NBER Working Paper is 133 words. Most thank at least one person for helpful comments (and the vast majority thank several) and almost all were presented in at least one conference prior to release.
This evidence combined with the evidence presented in [](#figure7) suggests that even if male and female authors differ in how likely they are to send working papers out for comments, who they send those papers to and how they respond to those comments, gender differences in this respect occur *before* a paper is released as a working paper---and any changes in readability that occur between the NBER Working Paper and the published paper are indeed due to the refereeing process.
The only remaining external factor I am aware of that might confound this assumption is the feedback women receive in conferences and seminars. Perhaps women tighten prose (before or after submission) in response to audience member remarks? Anecdotal evidence suggests female speakers are given a harder time, although I could find no scientific analysis to support (or contradict) this claim. Nevertheless, most participants are also current (or future) journal referees. Neutral review feedback is inconsistent with non-neutral presentation feedback when originating from the same group.</Text>
            <Comments>This issue of mistaken beliefs is considered in [][Experience].
The corresponding figure for male-authored papers is 21 percent. The conclusion that more female-authored papers are submitted to peer review before submission to *Econometrica* is robust to controlling for year fixed effects (see [][AppendixDoubleBlind]).
This possibly biases estimates in [](#table7) downward.
A related theory is that women receive more critical feedback in conferences and seminars because they present their work more often. In a survey of economists, [#Sarsons2015;] finds that men and women are equally likely to present co-authored work but women are actually *less* likely to present solo-authored work.
A recent [article](https://chroniclevitae.com/news/1182-should-academic-conferences-have-codes-of-conduct) on Chronicle Vitae discusses the topic and provides specific examples[#Baker2015]. SXSW Interactive (a large technology conference that isn't specifically linked to academia) cancelled two 2015 panel discussions on issues related to gender in response to violent online harassment of the (female) speakers.
Even if this were the case, it implies an entrenched discipline-wide bias.</Comments>
        </Document>
        <Document ID="AD958E6B-34B0-4744-92D8-5CF04C4C3863">
            <Title>Condition 3</Title>
        </Document>
        <Document ID="E82A7A4C-7FBE-442B-AAC6-30137C46B264">
            <Title>362 (erinâs imac's conflicted copy 2017-09-26)</Title>
            <Text>To help organise the discussion, I develop a stylised model of readability's marginal impact on the editorial decision making process. It follows an author---denoted by $i$---who publishes several articles in prestigious academic journals over the course of his career. Each article is roughly equivalent in terms of topic, novelty and quality, but varies on readability.
At stage 0, author $i$ drafts his $t$th paper and submits it for peer review. Upon receipt, the journal's editorial office assigns the manuscript to a group of referees. The (finite) set of all potential review groups is represented by $\Sigma$; $\mu_i$ is the set of strictly positive probability measures on $\Sigma$. $\Sigma$ and $\mu_i$ are known to $i$.
Let $r_{0it}$ and $\widetilde r_{0i}^s$ denote manuscript $t$'s non-negative draft readability and the initial rejection threshold review group $s\in\Sigma$ applies to all papers by author $i$, respectively. $s$ rejects the paper at stage 0 if $$r_{0it}&lt;\widetilde r_{0i}^s.$$ $i$ is otherwise granted a "revise and resubmit" (R&amp;R), yet could still be rejected at stage 1 if the readability of his revised manuscript, $R_{it}=r_{0it}+r_{1it}$, does not meet a second threshold, $$R_{it}&lt;\widetilde R_i^s,$$ where $\widetilde R_{i}^s=\widetilde r_{0i}^s+\widetilde r_{1i}^s$. All rejections and acceptances are final. $\widetilde R_i^s\ne\widetilde r_{0i}^s$ to account for different standards at different stages of peer review. $r_{1it}$, $\widetilde r_{0i}^s$ and $\widetilde r_{1i}^s$ are non-negative; the latter two are independent.
To aid the revision process, $s$ writes a referee report from which $i$ forms expectations about $\widetilde R_i^s$ by assigning subjective probabilities $\pi_{1it}^s(R)$ to all $R$. Unfortunately, the concept of readability is complex, some referees write insufficiently detailed reports and inattentive or hypersensitive authors misconstrue even perfectly clear advice. This renders $i$'s interpretation of the report imprecise and his subsequent expectations about $\widetilde R_i^s$ inexact and possibly specious.
Conditional on $r_{0it}$, I assume referee reports by $s$ for $i$ are the same for all $t$ and that each is distinctive enough for $i$ to distinguish $s$ in $\Sigma$. Consequently, author $i$'s stage 1 choice of $R_{it}$ maximises his (immediate) subjective expected utility given $s$,
&lt;!--\input{$PPATH/equations/equationX1}--&gt;
 $\Pi_{1it}^s(R_{it})$ is the cumulative sum of $\pi_{1it}^s(R)$ for all $R\le R_{it}$; $u_{i}$ is the utility of having a paper accepted in a prestigious journal; $\phi_{i|r_{0it}}(r_{1it})=\phi_i(R_{it})-\phi_i(r_{0it})$ and $c_{i|r_{0it}}(r_{1it})=c_{i}(R_{it})-c_{i}(r_{0it})$ are the satisfaction and cost, respectively, from making changes $r_{1it}$ given the paper's initial readability $r_{0it}$. $\phi_i$ is increasing and concave in its arguments, $c_i$ increasing and convex---marginally increasing $R_{it}$ generates proportionally less satisfaction but needs more effort when the paper is already well written. $c_i(0)$ and $\phi_i(0)$ are both 0.
I assume authors' decisions at stage 0 are myopic; $i$'s choice of $r_{0it}$ maximises his initial subjective expected utility for the current paper,
&lt;!--\input{$PPATH/equations/equationX2}--&gt;
 where $\Pi_{0it}^s(r_{0it})$ is the cumulative sum for all $r\le r_{0it}$ of author $i$'s subjective probabilities $\pi_{0it}^s(r)$ about $\widetilde r_{0i}^s$; $v_{1it}^s$ is [](#equationX1) evaluated at the optimal $r_{1it}$.
Finally, I assume authors (i) observe each others' readability choices and publication outcomes; and (ii) use relevant information to update their subjective probabilities. These assumptions imply, at a minimum, that $i$ updates $\Pi_{0it}$ and $\Pi_{1it}$ based on (i) conclusive evidence derived from the choices and outcomes of [equivalent peers](#definition1); and (ii) knowledge acquired during his own prior experience in peer review.
&lt;!--\input{$PPATH/theorems/assumptions}--&gt;
[](#equationX1) and [](#equationX2) incorporate a variety of factors that potentially affect authors' readability choices---editorial standards ($\widetilde r_{0it}$ and $\widetilde R_{it}$); ambition ($u_i$); the cost of drafting and revising manuscripts ($c_i$); an otherwise unexplained intrinsic satisfaction from writing readable papers ($\phi_i$). Poor information, overconfidence and sensitivity to criticism are not explicitly included, on the assumption that people do not *want* to be poorly informed, overconfident and excessively sensitive. These factors nevertheless enter [](#equationX1) and [](#equationX2)---and hence influence choices---via the subjective expectations author $i$ forms about $\widetilde r_{0i}^s$ and $\widetilde R_i^s$.
A single observation $R_{it}$ cannot therefore establish if and to what extent author $i$'s choices are motivated by preferences and costs specific to him ($u_i$, $\phi_i$, $c_i$), editorial standards and/or referee assignment outside his control ($\widetilde r_{0it}$, $\widetilde R_{it}$, $\mu_i$) or the disparate confounding factors mopped by $\Pi_{0it}^s$ and $\Pi_{1it}^s$. An observed *increase* in $i$'s readability in two separate $t$, however, distinguishes the former explanation (preferences and costs) from the combined impact of the latter two. Gradual updating of subjective probabilities, however, slowly reduces the impact of confounding factors. Having ruled out preferences and costs, any difference in readability choices between equivalent peers that persists over time must therefore be attributable to 
Comparison over time to an equivalent peer suggests 

The *path* of $i$'s choices compared to an equivalent peer's identifies editorial standards and referee assigment all other confounded factors incorporated in $\Pi_{0it}^s$ and $\Pi_{1it}^s$.
This idea is captured in [](#theorem1). Let $\bm1_{0i}^s(r)$ and $\bm1_{1i}^s(R)$ are indicator functions equal to 1 if $r\ge\widetilde r_{0i}^s$ and $R\ge\widetilde R_i^s$, respectively, and $\{(r_{0it},R_{it})\}$ and $\{(r_{0kt},R_{kt})\}$ denote the sequence of readability choices $i$ and $k$ make over their lifetimes. Then, roughly, if $i$'s current papers are more readable than his past papers and more readable than $k$'s papers then either (i) $i$ has a higher probability, on average, of being accepted; (ii) $i$ is assigned "tougher" referees---*i.e.*, those with higher $\widetilde r_{0i}^s$ and/or $\widetilde R_i^s$; or (iii) referees, in general, are tougher on $i$'s papers---*i.e.*, $\widetilde r_{0k}^s&lt;\widetilde r_{0i}^s$ and/or $\widetilde R_k^s&lt;\widetilde R_i^s$ for at least one $s\in\Sigma$.
&lt;!--\input{$PPATH/theorems/theorem}--&gt;
$u_i$, $\phi_i$ and $c_i$ remain constant over time. Thus, changes to $r_{0it}$ and $R_{it}$ imply changes in the subjective probabilities author $i$ forms about $\widetilde r_{0it}$ and $\widetilde R_{it}$. Thus, when authors write current papers more readably than past or future papers, they do so because they expected to be rewarded for it with a higher probability of acceptance. Changes to the optimal choices $r_{0it}$ and $R_{it}$ are motivated entirely by a desire to achieve his other objective of getting his paper accepted into a prestigious academic journal.
[](#theorem1) indirectly implies that if author $i$ choses a higher readability than another author with an otherwise equivalent paper, then author $i$ anticipated a higher probability of acceptance, *ex ante*. Of course, that doesn't mean author $i$ is right---it is perfectly plausible that he's misinformed and there is no difference in their probabilities of acceptance. As $t\rightarrow\infty$ in a fixed state $s$ with perfect recall of his past histories or in any state with access to his peers' histories, author $i$ eventually figures this out.
</Text>
        </Document>
        <Document ID="E0CB79A3-753C-441D-8859-6A9F4C90370F">
            <Title>[](#table4), alternative measures of an article's "gender"</Title>
            <Text>[](#tableC4a), [](#tableC4b) and [](#tableC4c) repeat the analysis shown in [](#table4) using three alternative measures of an article's "gender". In [](#tableC4a), papers authored entirely by women are compared to papers authored entirely by men. In [](#tableC4b), papers are considered "female" if at least one author is female. In [](#tableC4c), article gender is represented by a binary variable equal to one if at least half of all authors are female; articles below that threshold with at least one female author are excluded.



&lt;!--\clearpage--&gt;</Text>
        </Document>
        <Document ID="489FA0A0-FB53-490C-9DE4-5CB820ED9C7D">
            <Title>Article-level analysis</Title>
            <Text>[](#p3table4)presents coefficients from an ordinary least squares regression of ratio of female co-authors on the five readability scores. To account for error correlation by editorial policy, observations are grouped by journal editor/editorial board and standard errors are adjusted accordingly.
Column (1) controls for journal: abstracts written only by women score about one point higher on the Flesch Reading Ease scale; according to the four grade-level measures, they take 1--6 fewer months of schooling to understand. Percentage-wise, women write between 1--3 percent better than men.
Column (2) includes 63 year dummies; column (3) adds another 182 journal and year interaction dummies; column (4) introduces the 98 institution dummies. Controlling for time and institution has little effect. Coefficients and standard errors are very similar to those in the first column.
The coefficients on the journal dummies in (2) are presented in [](#p3table5).They compare *AER*'s readability to the readability of *ECA*, *JPE* and *QJE*, providing a useful check on the reliability of readability formulas in the context of economic writing. As intuitively expected, all five scores agree that *Econometrica* is harder to read; four out of five scores suggest *JPE* is, too, while *QJE* is easier.
Column (5) in [](#p3table4) controls for primary *JEL* classification. Since only post-1990 *JEL* classifications are used, estimates in (5) exclude roughly 40 percent of the data. Nevertheless, coefficients are roughly equivalent---with the exception of the Flesch Reading Ease score which halves and loses significance.
[](#p3figure4) displays results from an ordinary least squares regression on the Dale-Chall score; regressors are: (i) ratio of female co-authors; (ii) dummies for each primary *JEL* code, (iii) interactions from (i) and (ii) and (iv) controls for journal, year, institution and editorial board. Due to small sample sizes---particularly of female authors---[](#p3figure4) includes 549 articles from *AER Papers &amp; Proceedings*. *AER Papers &amp; Proceedings* is coded as a separate journal and edited by the American Economic Association's president-elect.
The pink vertical line in [](#p3figure4)'s left-hand graph is the marginal effect of female authorship at the mean. Its estimate coincides with results in [](#p3table4)---women's papers require six fewer weeks of schooling to understand---and is highly significant (standard error 0.04). Points reflect marginal effects across *JEL* classification (bars represent 90 percent confidence intervals from standard errors clustered by editor). Women earn higher marks for clarity in 12 out of 15 categories; only three are significant: Q (Agricultural and Natural Resource Economics; Environmental and Ecological Economics), N (Economic History), and J (Labour Economics). Men may be better writers in L (Industrial Organisation), O (Economic Development, Innovation, Technological Change, and Growth) and H (Public Economics); none, however, are statistically different from zero. [](#p3figure4)'s right-hand graph displays coefficients from interacting the ratio of female co-authors with each *JEL* code. Q and N are significantly above the mean, O and H significantly below it. Remaining categories are not statistically different from the mean effect.
In general, sample sizes are small and estimates imprecise---only Labour Economics and Microeconomics contain more than 100 papers written only by women (the others average 35). Nevertheless, [](#p3figure4) suggests two things. First, the mostly insignificant interaction terms indicate outlier fields are probably not driving journals' gender readability gap---nor is any specific field bucking the trend. Second, the number of women in a field appears to have little effect on the size of the gap: Agriculture/Environment has one of the lowest concentrations of female-authored papers---but Economic History has one of the highest (Labour Economics falls between the two). Admittedly, Economic History papers are still overwhelmingly---as in 74 percent---penned just by men. But given the readability gap is present in subfields with both above- and below-average rates of sole female authorship, women may need to be better writers even where more of them publish.</Text>
            <Comments>\input{/Users/erinhengel/Desktop/tables/tex/p3table4}
*AER* and *Econometrica* employed a single individual to oversee editorial policy for the journals and time periods covered by the data. *JPE* and *QJE* do not generally name a single individual and instead rely on editorial boards composed of four to five faculty members at the University of Chicago and Harvard, respectively. Editorial boards are considered distinct if they differ by at least one member. In total, 74 groups are formed in this manner. Clustering at the journal-, volume- or paper-level results in very similar standard errors. 
Coefficients from regressions on Flesch-Kincaid, Gunning Fog, SMOG and Dale-Chall scores represent the marginal effect in years of schooling, multiplied by negative one. Monthly figures found by multiplying each by 12.
Quotient of the coefficient on ratio of female co-authors divided by the regression constant.
\input{/Users/erinhengel/Desktop/tables/tex/p3table5}
&lt;!--\label{fn2}--&gt;Codes A, B, P and M dropped due to insufficient number of female-authored papers: each had fewer than 10 papers authored only by women. No paper is classified under category Y.
&lt;!--\label{fn1}--&gt;*AER Papers &amp; Proceedings* does not publish abstracts in its print version; only select years and papers are available online (2003 and 2011--2015), all of which are included.
\input{/Users/erinhengel/Desktop/figures/tex/p3figure4}</Comments>
        </Document>
        <Document ID="4D568880-239D-43FC-BB78-5CC6C24B94A4">
            <Title>216 (erinâs imac's conflicted copy 2015-08-30)</Title>
            <Text>In this section, I test the validity of [](#prop5) and [](#prop6). To account for credit-rationing and control for firm-specific characteristics, I use data from the World Enterprise Surveys (WES). The WES are a series of company surveys conducted by the World Bank and European Bank for Reconstruction and Development. They cover business constraints in emerging and developing markets. The data start in 2005 and are generally repeated every three years; in total, they contain 117,105 observations from 135 countries.
One question asks the degree to which access to finance---including interest rates, fees and collateral---is an obstacle to business operations. Answers range from 0 (no obstacle) to 4 (very severe obstacle). I define $\text{Obstacle}_i$ as a binary variable equal to 1 if the respose was moderate (2) or above---roughly 47 percent of firms.
I control for age, size, sector, exporter and ownership to account for firm-specific characteristics. $\text{Age}_i$ is the difference between the year firm $i$ was established and the year in which the survey took place. $\text{Size}_i$ distinguishes between small (fewer than 20 employees), medium and large (more than 100 employees) companies. $\text{Exporter}_i$ is a binary variable equal to 1 if the firm exports all or part of its output; $\text{Foreign-owned}_i$ and $\text{State-owned}_i$ equal 1 if the firm is partly or wholly foreign- or state-owned, respectively. $\text{Manufacturing}_i$ equals 1 if firm $i$ operates in the manufacturing industry.
[](#FirmSpecifics) summarises firm-specific controls. Exporter status, foreign ownership and size are the same for firms reporting finance as an obstacle compared to those that don't. Although state-owned firms are less likely to find financing an obstacle, the difference is slight. Younger firms and those in manufacturing, however, report a tougher time accessing finance than their older, service-oriented peers. Firms which consider finance an obstacle are, on average, 22 years younger than those that don't; 52 percent of them operate in the manufacturing industry compared to only 44 percent of those which do not find finance to be an obstacle.
In addition to firm-specific controls, I include several controls for a country's general lending environment and development; each are culled from the World Bank WDI database and summarised in [](#LendingEnvironment).Inflation, domestic credit to the private sector and GDP and GNI per capita are originally sourced from national accounts and international financial statistics data compiled or estimated by the OECD and World Bank, International Monetary Fund. The percentage of firms using banks to finance investment is drawn from survey responses in the WES. Data are matched to the WES according to the exact country and closest year in which the survey took place.
To measure $p$ and $Y$, I use World Bank Doing Business data. Doing Business indicators cover the cost and complexity of administrative procedures and regulations imposed on firms. They are estimated based on responses to a set of standardised---yet mostly hypothetical---scenarios posed to local accountants, lawyers and other expert practitioners. 
For $p$ I construct an index of the depth, coverage and quality of financial information available on individuals and firms in a country. It is the product of the Doing Businessâs *Depth of credit information index* (an index measuring the comprehensiveness of credit-related data in the economy) and *Credit registry coverage* (percentage of individuals and firms covered by a public data registry) or *Credit bureau coverage* (percentage of individuals and firms covered by a private data bureau), whichever is greater.
The *Depth of credit information index* was substantially revised in 2015 but backward revised only until 2013. The old methodology scored countries based on six criteria; the new methodology appends two more features. To account for this difference, data prior to 2013 is normalised by six; data after that is normalised by eight.
In a given year, roughly 60 percent of countries have the minimum value of 0. Argentina had the highest possible value of 1 in 2006 and 2010; Honduras and Uruguay in 2010. The mean value is 0.15 (standard error of 0.01), about 0.05 points lower than the average of all countries covered in the Doing Business (0.2, standard error 0.01).
$Y$ is the Doing Business *Cost of enforcing contracts* measure. It estimates the cost of resolving a commercial dispute as a percentage of the claim in question. It includes attorney and court fees and all costs associated with enforcing the final judicial decision. Values range between 0 and 1.63 (indicating enforcing a contract costs 63 percent more than the value of the claim). The latter distinction goes to East Timor in 2009; the former to Bhutan. The mean is 0.35 (standard error 0.01)---identical to the population mean (standard error 0.006).
As shown in [](#prop4) from [][Rescue Culture] credit rationing---and presumably finance obstacles more generally---vanish in the presence of receivership procedures similar to those present in the U.K. prior to 2003 and especially 1986. Importantly, receivership procedures must allow the creditor full power to bypass bankruptcy entirely and determine for himself the insolvent firm's proper resolution.
To determine whether this is the case, I again rely on Doing Business indicators. Appendix X details my methodology explicitly, but roughly corresponds to whether floating charge liens are legally permissible, have absolute priority in bankruptcy, are not subject to an automatic stay and may be enforced out-of-court.
Fourteen countries employed a floating charge lien in at least one year; 132 did not. In the 187 countries in the Doing Business for which data are available, 15 used a floating charge. That is, the majority of countries with a well-functioning receivership regime are covered by the WES.</Text>
        </Document>
        <Document ID="49E5CAE9-D36E-4B00-8744-4B194A81892A">
            <Title>Model</Title>
        </Document>
        <Document ID="87359BED-D05B-4E6B-953E-158D57059402">
            <Title>Introduction</Title>
            <Text>In 1986 the U.K. introduced administration: bankruptcy procedures aimed at rehabilitating insolvent debtors. Called reorganisation elsewhere and analogous to U.S. Chapter 11, administration answered political concern that too many financially distressed firms were unnecessarily liquidated---or left to fail---despite a reasonable chance of survival[#Cork1982]. The "rescue culture" it fostered was thought to maximise profits, prevent job loss and uphold creditors' long-term interests.
In a comprehensive debt financing model, I show rescue culture neither maximises profits nor prevents job loss. Instead, it obstructs firm creation in the first place or supplants otherwise certain survival with upfront investment choices that make premature liquidation not just highly likely in financial distress but virtually guaranteed should the firm remain solvent. The only goal rescue culture does serve is creditors' long-term interests---by introducing a profitable niche lending market in an otherwise zero-profit industry.
My starting point is that administration is time-consuming, arduous and expensive. United Airlines spent $250 million on legal and professional services related to its reorganisation; WorldCom topped out at $620 million[#Berk2009]. In New York, procedures last three years[#Weiss1990] and fees eat up 2--4 percent of firm value[#LoPucki2004,Warner1977,Weiss1990]. Customers are reticent to buy goods[#Titman1984], suppliers hesitate to provide inventory and employees leave[#Berk2010]. Hungarian trustees delay liquidation[#Franks2013]; their counterparts in Russia embezzle cash[#Lambert-Mogiliansky2007].
These costs matter: they contribute to expensive debt, delayed liquidation, suboptimal investment and credit rationing. Limited liability makes creditors disproportionately responsible for reorganisation's impact on future earnings. Insolvent borrowers exploit the imbalance by demanding excessive debt write-downs in obvious violation to absolute priority. This cuts creditor earnings, discourages voluntary liquidation and sometimes renders lending unprofitable.
Insolvent firms that ought to continue always do---in settlements known as workouts. Firms with high expected earnings relative to upfront investment have the most to gain from a workout. So do their creditors. Reorganisation is expensive and probable: not only will equity never willingly liquidate, but if put to a judge she will likely agree. Bankruptcy has real consequences and creditors a real desire to avoid it; they quickly acquiesce to intemperate write-downs.
Potential borrowers lose. Workouts avoid wasteful reliance on an ineffective bankruptcy regime but lender write-downs are still proportional to the cost of that regime. As is often the case, however, it is firms' pre-borrowing selves and solvent peers who really pay the cost. For some, credit is merely more expensive. For others, it is rationed or obtained only by inducing premature liquidation. Higher interest rates encourage more workouts so creditors do not lend money no matter how much borrowers are willing to pay. To surmount this, firms over-invest in physical or tradable intangible assets to increase  both judicial likelihood of liquidation and a desire to voluntarily do so themselves.
Insolvent firms that ought to liquidate only sometimes do but nevertheless face fewer upfront repercussions in the lending market. Although they frequently continue when they shouldn't and intermittently even file for bankruptcy, their shorter expected lifespans mean equity usually forgo both to liquidate. From the creditor's perspective, workouts (and bankruptcy) are risker; returns are lower. They clearly prefer lending to borrowers who want them less often.
In fact---and despite a perfectly competitive credit market---lending to firms most prone to bankruptcy can actually be profitable. High interest rates encourage bankruptcy, sure, but when they are very high (and reorganisation very costly) it's no better than shutting down; entrepreneurs are indifferent so creditors liquidateâ¦while pocketing all of the firm's initial earnings. If creditors only recuperate their original investment when liquidation is guaranteed,  an exorbitant rate achieves this and makes them money in the process.
These conclusions suggest radically altered bankruptcy procedures such as immediate auctions or distributed options may miss the point[see\]\[][#Baird1986,Bebchuk1988,Bebchuk2000,Aghion1992a]. All focus on rigidly respecting debt's seniority over equity via a legally structured sale or bargaining process. I show, however, that violations to absolute priority are only harmful once expensive reorganisation forces debt forgiveness beyond that which creditors would otherwise swallow given low initial earnings. Since it is hard to see how auction- or option-based reforms are cheaper than traditional bankruptcy---and their inflexible design may cost more to implement---they are probably not the panacea many hoped them to be.
Far more suitable is the reorganisation law they sought to replace: Chapter 11. I show that any particular bankruptcy regulation that neither implicitly reduces future earnings or explicitly prioritises equity over debt does not constrict lending or distort investment decisions. When it helps quickly resolve reorganisation and doesn't disempower creditors, lending is cheaper and more widespread, investment decisions more efficient.
Most rules in Chapter 11 meet these criteria: allowing management to remain, equity first right to a restructuring plan, super-senior emergency financing and U.S.-style "cramdown"---judicial imposition of a reorganisation plan despite creditor objection. They cost little to implement, do not categorically prioritise equity over debt and their absence (or the alternative) does not strengthen creditors' bargaining position. They do reduce time spent in reorganisation[#Elayan2001], discourage excessive risk-taking and under-investment[#Eberhart1993,Gertner1991] and encourage prompt notification of financial distress[#Povel1999].
Notoriously absent from Chapter 11 are several unambiguously harmful regulations that implicitly cost money. Replacing management with court-appointed administrators---as is done in France---expels specialised skills administrators probably lack. U.K. and German regulations that insist on administrative oversight burden firms with additional salaries that have priority over debt. British advertising requirements publicise financial distress, causing consumers concerned about warranty validity to shop elsewhere[#Titman1984] and employees worried about their jobs to find new ones[#Berk2010].
Chapter 11 may be the best reorganisation law, but its wider bankruptcy system is always inferior to one without an automatic stay. Automatic stays are not only redundant, but weaken creditors' bargaining position. Insolvent borrowers that should continue offer reasonable workout proposals unless creditors are likely the residual claimants in reorganisation. In the latter case, however, lenders' desire to maximise their own recovery impels action that also maximises the insolvent firm's value. Automatic stays therefore anticipate motives that creditors do not have. Eliminating them makes lenders' returns in bankruptcy less risky; insolvent borrowers impose fewer write-downs during workouts.
If the market for equity finance were as competitive as traditional lending is theorised to be, credit-constrained borrowers could turn to family and friends, initial public offerings or angel investors. But not everyone is blessed with wealthy kin, issuing public stock is just not done by tiny enterprises and illiquid venture capital markets favour entrepreneurs with whom investors share social networks and other superficial similarities[#Hochberg2007,Bottazzi2011,Verheul2001]. Thus, eliminating automatic stays---by permitting floating charge liens---is the most straightforward antidote to bankruptcy's unpleasant side effects. Despite claims by the[#Cork1982;], lenders are not prone to asset grabbing and firms are not left to fail[#Franks2005a]. Distressed British companies secured by floating charges were less likely liquidated than their counterparts in countries more committed to their survival[#Davydenko2008]. Bankruptcies involving floating charges keep firms as going concerns far more frequently than reorganisation---and cost significantly less to implement[#Djankov2008].
Yet floating charges are no longer part of British bankruptcy law. The Insolvency Act 1986 weakened them; the Enterprise Act 2002 removed them. Rescue culture won. Paradoxically at the expense of its purported aim: "to recognise that the effects of insolvency are not limited to the private interests of the insolvent, his family, creditors or directors, shareholders and employees, but that other interests of society or other groups in society are vitally affectedâ¦and to ensure that these public interests are recognised and secured"[\]\[Â§198(i)][#Cork1982].</Text>
            <Comments>U.K. law reserves "bankruptcy" for financially distressed individuals and "insolvency" for their corporate counterparts. I use both words per colloquial definitions: bankruptcy is the legal status and insolvency the state of not being able to pay one's debts.
Earlier models by[#Bebchuk2002;] and [#Longhofer1997;] identified deviations to absolute priority as key causes of credit-rationing.
This conclusion mirrors empirical findings that economically viable firms prefer workouts to Chapter 11[#Chatterjee1996].
[#Franks1994;] find that both creditor recovery rates and absolute priority deviations in favour of equity are much higher in distressed exchanges of publicly traded debt than in Chapter 11.
[#Jensen1976;] discussed adjudication costs in bankruptcy as contributing to declining firm value.
Empirical evidence suggests lenders charge higher interest rates, require shorter maturities and demand more collateral when weak creditor protections allow borrowers to extract steep deviations to absolute priority[#Qian2007].
[#Bebchuk2002;] similarly finds violations to absolute priority encourage inefficiently risky investment decisions. 
[#Chatterjee1996;] show empirically that Chapter 11 is dominated by bankrupt firms that should be liquidated.
Although this paper focuses on corporate insolvency, a similar phenomenon may have been partially responsible for excessive home loans made in the U.S. before 2007. Lenders offered mortgages at exorbitant rates to low income borrowers that virtually guaranteed default---but likely not before the borrower made one or more interest payments. Cumbersome personal bankruptcy procedures, low probability of success and high legal fees induce insolvent borrowers to immediately foreclose; the lender resells the home. When housing prices are rising (or at least not falling), this loan is effectively risk free but generates a rate of return above the risk-free interest rate.
Only legal fees incurred in bankruptcy have priority over debt repayment. No other Chapter 11 rule implicitly reduces future earnings or explicitly prioritises equity, suggesting that the structure of Chapter 11 probably isn't the main cause of absolute priority deviations and credit-rationing[#Bebchuk2002]; the massive fees inherent in the U.S. legal system are. Although wages have similar priority, their existence does not depend on bankruptcy. Workers must be paid regardless of solvency, so expected firm value is unaffected by a requirement to pay them first. [][p1AppendixSuperSeniorFinancing] shows that incurring losses (*e.g.*, paying wages in excess of available cash flow) before repaying debt does not affect lending and investment decisions.
An automatic stay is a court order that prevents confiscating collateral or collecting debt payments. It is the defining characteristic of reorganisation, since without it nothing legally prevents creditors from *de facto* liquidating an insolvent firm by seizing the assets it needs to operate.
It's not hard to guess who has the toughest time getting cash---of *Business Insider*'s "[50 Early Stage Investors in Silicon Valley You Need to Know](http://www.businessinsider.com/sv-angel-50-2012-7?op=1&amp;IR=T&amp;IR=T)", only two are female and only one black.
Floating charges are debt secured by an entire business. In the event of default, the creditor with a floating-charge lien is granted control rights of the firm with little judicial interference. An alternative simply removes the specific regulation referring to automatic stays. This may, however, enable borrowers to secure multiple loans with a single asset and exacerbate lender co-ordination problems[#Jackson1986,Baird1986].</Comments>
        </Document>
        <Document ID="3B5F4B47-602B-4113-947F-9B3824E55EC7">
            <Title>Mixed logit regression</Title>
            <Text>Credit information's ultimate impact depends on firm viability, which is not identified. [](#p2table7) suggests an overall negative effect, but it is small and insignificant. To provide more nuance, I use a mixed logit regression.
Mixed logit models were developed to address unobserved heterogeneity in consumer choice experiments. They permit random taste variation in survey respondents presented with a series of alternatives over several choice occasions.
Despite not corresponding exactly to surveys that usually exploit mixed logit models, my dataset does contain many similar features. That is, although the same firm is never repeatedly sampled, the same type of firm is. Additionally, a particular type of firm should not only be repeatedly sampled in one country, but across countries facing different levels of credit information and contract enforcement costs. 
To apply the model, I decompose the dataset into groups. Each group contains firms that live in a country with similar GDP per capita and bankruptcy legislation and share the same approximate age, industry, size and export, foreign- and state-ownership status. Credit information and contract enforcement cost were averaged over group members that considered financing an obstacle, and separately for those that did not. 
The result is 389 âindividualsâ with a choice of two alternatives---is finance an obstacle or not? The differentiating factors are credit information and contract enforcement cost. As illustrated in [#Train2009;], by assuming the individual-specific random error component is distributed i.i.d. extreme value, [](#p2equation6) represents the probability $\text{obstacle}_i$ is $k$:where $k$ is 0 or 1, $\vect B$ is a $2\times1$ vector of normally distributed coefficients, $\phi$ their density and $\vect x_{i_k}$ individual $i$'s $2\times1$ vector of explanatory variables for choice $k$. 
[](#p2table8)presents the results. Columns (1) and (2) display means and standard deviations for $\vect B$. (3) shows results from a comparable logit regression. The gulf between coefficients in the mixed logit and standard logit and [probit](#p2table7) models suggests considerable unobserved heterogeneity.
$Y$ is again positive and significant, indicating quick and efficient reorganisation reduces financing obstacles. In the first result set, the coefficient follows a normal distribution; in the second, it is held fixed. The mean is the same regardless of specification. The standard deviation in (1) is close to zero and not significant, implying a very consistent impact throughout the population.
The coefficient on $p$ follows a normal distribution in both mixed logit regressions. Their means are about -28, meaning information considerably mitigates obstacles to finance for the average firm. Standard deviations are also roughly equivalent and significant. Their magnitude indicates substantial variation. Seventy percent of firms think credit information reduces obstacles to finance; for 30 percent, the opposite is true.
The ambiguous effect in [](#p2table8) is due to unobserved heterogeneity. [](#prop5)(ii) proposes one potential cause---firm viability in the context of court supervised reorganisation---but results in [](#p2table8) cannot directly attribute it to any specific factor. In conjunction with [](#p2table7), however, they do suggest the positive relationship is specific to jurisdictions without receivership, as theorised in [](#hypoth3).</Text>
            <Comments>GDP per capita and age were subdivided into two and three equal frequency groups, respectively. Bankruptcy legislation was subdivided into those countries with and without receivership. Industry refers to whether the firm operates in the manufacturing industry or not.
\input{$PPATH/p2/equations/p2equation6}
\input{$PPATH/p2/tables/tex/p2table8}
In a standard logit model, $\phi(\vect B)$ is 1 for a single parameter set and 0 for all others; [](#p2equation6) collapses to the term in parentheses---*i.e.*, the logit probability evaluated at fixed $\vect B$.
Errors in the latter two incorporate parameter variation and thus normalise coefficients by a larger number[see\]\[][#Revelt1998].
$\Phi(-b_n/s_n)$, where $b_n$ and $s_n$ are the estimated mean and standard deviation of the $n\text{th}$ coefficient, respectively.</Comments>
        </Document>
        <Document ID="C33716FF-C8C3-4471-BD56-6FBA82DADAE6">
            <Title>Scratch: Rescue Culture</Title>
            <Text>But inefficient continuation is not a consequence of bankruptcy's design nor is it singularly tied to limited liability; it is a product of poor implementation. Eliminate judicial mistakes *or* wasteful reorganisation and it goes away. When judges identify non-viable firms without error, all are liquidated in bankruptcy. Reorganisation may be expensive, but non-viable firms have no means to take advantage of that. 

When reorganisation isn't wasteful, on the other hand, workouts don't offer enough of a premium to forgo liquidation.  The gains from a possible liquidation are enough to induce the entrepreneur to prefer bankruptcy to a workout. Of course, bankruptcy's only advantage is liquidation, an outcome on which the creditor is anyway keen ([](#lem1)); non-viable projects are consistently liquidated at time 1.

Nevertheless, insolvency provides entrepreneurs the motivation and expensive reorganisation grants them the opportunity to extract excessive deviations from absolute priority. Thus, although the efficient outcome prevails, it does so only inefficiently.

One type of non-viable firm is liquidated regardless of time 1 earnings: those with liquidation values so high that their entrepreneurs would rather file for bankruptcy than engage in a workout, i.e., $V_1^C&lt;V_1^B$, The gains from a possible liquidation outweigh the costs of a conceivably expensive reorganisation making bankruptcy a worthwhile gamble. Of course, bankruptcy's only advantage is liquidation, an outcome on which the creditor is anyway keen ([](#lem1)); these projects are consistently liquidated at time 1.

Official procedures frequently employ regulations which favour the debtor---from giving him first crack at a reorganisation plan. It is rarely the case, however, that reorganisation explicitly transfers wealth from the creditor to the entrepreneur. Instead, it substitutes one outcome for another. 

Violations to absolute priority compound moral hazard. Creditors are forced to accept workouts precisely when the value of their fallback option---bankruptcy---is low. Threatening bankruptcy increases entrepreneurs' earnings by compelling creditors to forfeit some of theirs. 
But inefficient continuation is not a consequence of bankruptcy's design nor is it singularly tied to limited liability; it is a product of poor implementation. Eliminate judicial mistakes *or* wasteful reorganisation and it goes away. When judges identify non-viable firms without error, all are liquidated in bankruptcy. Reorganisation may be expensive, but non-viable firms have no means to take advantage of that. 

When reorganisation isn't wasteful, on the other hand, workouts don't offer enough of a premium to forgo liquidation.  The gains from a possible liquidation are enough to induce the entrepreneur to prefer bankruptcy to a workout. Of course, bankruptcy's only advantage is liquidation, an outcome on which the creditor is anyway keen ([](#lem1)); non-viable projects are consistently liquidated at time 1.

Nevertheless, insolvency provides entrepreneurs the motivation and expensive reorganisation grants them the opportunity to extract excessive deviations from absolute priority. Thus, although the efficient outcome prevails, it does so only inefficiently.

One type of non-viable firm is liquidated regardless of time 1 earnings: those with liquidation values so high that their entrepreneurs would rather file for bankruptcy than engage in a workout, i.e., $V_1^C&lt;V_1^B$, The gains from a possible liquidation outweigh the costs of a conceivably expensive reorganisation making bankruptcy a worthwhile gamble. Of course, bankruptcy's only advantage is liquidation, an outcome on which the creditor is anyway keen ([](#lem1)); these projects are consistently liquidated at time 1.

Official procedures frequently employ regulations which favour the debtor---from giving him first crack at a reorganisation plan. It is rarely the case, however, that reorganisation explicitly transfers wealth from the creditor to the entrepreneur. Instead, it substitutes one outcome for another. 

Violations to absolute priority compound moral hazard. Creditors are forced to accept workouts precisely when the value of their fallback option---bankruptcy---is low. Threatening bankruptcy increases entrepreneurs' earnings by compelling creditors to forfeit some of theirs. 
Finally, to weed out countries prone to excessive appeals or wasteful delays, I further require receivership winds up within two years. 
This paper's goal is to test the validity of [](#prop5) and [](#cor3). The problem and data are conducive to structural estimation via simulated method of moments. For the purposes of this paper, however, I employ a reduced form approach; my aim is simply to determine the average, aggregate sign of $p$ and $Y$ given data on firms in varying bankruptcy regimes.
Yet more nuance is needed to support [](#prop6), which predicts an ambiguous effect in countries without receivership. Security prices in efficient capital markets, stringent disclosure laws and well-developed credit-rating agencies and financial analysis sectors better reveal firm fundamentals. Before loans are made, this added information reduces borrower-lender asymmetry. During bankruptcy, it increases judicial precision. As long as judges systematically underestimate reorganisation's cost, more accurate decisions may mean fewer insolvent firms are liquidated---and so fewer firms get credit in the first place.
</Text>
        </Document>
        <Document ID="542D4130-D886-47DB-89FF-0A22139B8F8B">
            <Title>Endogenous collateral.</Title>
            <Text>Machines are generally available in several models, produced by more than one company and sold at various prices. Daily newspapers with circulation between 150,000--200,000 spend anywhere from $4--25 million on a press: cost depends on manufacturer reputation, post-purchase service quality, colour capacity and several additional features that reduce wastage, increase automation and speed up printing. Farm equipment is available in an even more dramatic range. John Deere's 2015 catalogue advertises 20+ tractors, priced $35,000--160,000. Models are differentiated by horsepower, warranty and special add-ons such as air conditioning, instrument panels, automatic transmission, bluetooth and surround sound speakers.
Differentiated products cater to diverse clientele. Larger newspapers need faster presses because they print more copies; farming in extreme heat makes air conditioned tractor cabs (almost) a necessity. It is possible, however, that a spectrum of products sold at various prices also relieves credit rationing caused by bankruptcy.
Assume the machine comes in a variety of models each of which is equally productive when employed by a specific project---utility tractors pick up just as many bales of hay whether equipped with subwoofers or not, and faster presses have little bottom-line impact when used by smaller newspapers. The price of a particular model is $\ul K_0\le K_0$, where $\ul K_0$ corresponds to the cheapest, or "base", model.
Every model retains its full value until time 1. After that, technological change causes the factors which differentiate models of the same machine to depreciate faster than the machine itself. Their value at time 2 is zero; every machine is worth $\ul K_2$ where $\ul K_2$ is the time 2 resale value of the base model.
PROPOSITION
When the product market for machines is sufficiently differentiated, no one is credit rationed. 
eprop
From [](#prop3), every project is funded. A sufficiently differentiated product market ensures a model exists that costs just enough to satisfy $\ol V_1^C&lt;\ol V_1^B$. Per [](#prop2)(ii), the entrepreneur who purchases this model guarantees his project is liquidated at time 1. Loaning him money is riskless, so he always gets credit.
Whether the entrepreneur actually buys an expensive model is a different question. Viable firms able to get credit for $\ul K_0$ choose the base model. Every other firm in every other circumstance, however, buys an expensive machine, *i.e.*, one that costs
$$\ol K_0=\max\l\{\ul K_0,(1-Y)(X_2^H+\ul K_2)\r\},$$
 where $X_2^H$ is the upper limit of $X_2$ ([](#cor2)).
COROLLARY
Viable firms choose $K_0=\ul K_0$ at $K_0\le D$ if creditors are willing to lend; all other firms choose $\ol K_0$ at $\ol K_0=D$.
ecor
Any credit rationed project viable at $\ul K_0$ won't get credit at any other $K_0$ that leaves [](#p1equation1) intact: [](#p1equation4) is decreasing in $K_0$ making loans harder to get as the machine's price rises. Since $\ul K_0$ was already the cheapest model, an entrepreneur's only option is to verifiably grant creditors more bargaining power during insolvency---by buying a machine expensive enough to reverse [](#p1equation1), converting a previously viable project into a newly non-viable one.
By "over-investing"---*i.e.*, purchasing a machine with greater capabilities (and a higher price tag) than what he actually needs---the entrepreneur fundamentally alters the relationship between his project's value in continuation and its assets' piecemeal resale value. Although his project now gets credit, he manages this only by shortening its lifespan.
Every entrepreneur of a non-viable firm---whether non-viable at $\ul K_0$ or because credit is only available if rendered non-viable---wants to subvert entirely his *ex interim* incentive to demand a workout. From [](#prop1), non-viable projects are sometimes operated a second period when time 1 earnings are low. But freedom at time 1 means a higher $D$ at time 0. *Ex ante*, its entrepreneur still prefers $D$ as small as possible ([](#lem3)); by extension he must want liquidation as probable as possible. Purchasing the expensive machine at $\ol K_0$ guarantees it.
$\ol K_0$ assures liquidation much like an exorbitant $D$ would per [](#prop2)(ii) but with one crucial difference: the entrepreneur retains control rights to $X_1$. By setting $D=\ol K_0$ the creditor recovers the loan principal in every state; the entrepreneur keeps all cash flows.
Over-investment by firms not viable at $\ul K_0$ contributes to a more socially desirable outcome. Entrepreneurs exploit the value of the machine at time 0 and its worth at time 2 to induce better behaviour at time 1: increasing $K_0$ reduces their desire to demand workouts or bankruptcy; by buying the model that costs $\ol K_0$ they eliminate it entirely.</Text>
            <Comments>For example, tractors aren't remarkably different from those produced in 1940 but today's audio systems are a big change from earlier gramophones. Similarly, according to [Kelley Blue Book](http://www.kbb.com), a standard equipped 2015 Toyota Camry SLE 4-door sedan currently costs 25.7 percent of the price of a standard equipped 2015 Mercedes-Benz S-Class S550 4-door sedan. That same Camry purchased in 2005, however, currently costs 89.9 percent of a comparable 2005 Mercedes-Benz S-Class. (Assumes all cars are in excellent condition.)
"Sufficient differentiation" in [](#prop3) required only that some $K_0$ exists such that $\ol V_1^C&lt;\ol V_1^B$. [](#cor2) applies only if "sufficient differentiation" includes a model which costs $\ol K_0$.</Comments>
        </Document>
        <Document ID="7EC30A3C-D3B1-4696-8D15-77F10174FB53">
            <Title>Conclusion</Title>
            <Text>This paper makes a curious discovery: female-authored articles in top economics journals are better written. After examining the difference, I conclude that higher standards applied by editors and/or referees are primarily to blame.
No prior study has uncovered convincing evidence of gender bias in journal acceptance rates. It's encouraging that sex is irrelevant to publication outcomes, but that does not mean it has no effect on the process---or on the productivity of female academics. When female authors endure unfair criticism in referee reports, clearer writing and longer review times follow. With less time to spend on new projects, research output slows down.
Higher standards impose a quantity vs. quality tradeoff that not only reconciles academia's "Publishing Paradox", but also rationalises many instances of female output. Work that is evaluated more critically at any point in the production process will be systematically better (holding prices fixed) or systematically cheaper (holding quality fixed). This reduces women's wages---for example, if judges require better writing in female-authored briefs, female attorneys must charge lower fees and/or under-report hours to compete with men---and distorts measurements of female productivity---billable hours and client revenue decline; female lawyers appear less productive than they truly are.
Finally, the topic of my study is narrow, but its methodology has wider applications. To the best of my knowledge, this paper is the first (in economics) to identify discrimination using the choices and behaviours of those discriminated against. Although applied to a specific context---peer review---the identifying logic equally suits any situation where people repeatedly receive and act on biased feedback. Moreover, this study is also the first to uncover subtle group differences with readability scores. These scores are not new---all are extensively tested with well-documented properties---but their use is mostly confined to determining whether text is appropriate for intended audiences. As this paper demonstrates, however, readability scores are also effective tools to evaluate asymmetry anywhere ideas are communicated orally or in writing and large amounts of source material are easily obtainable: journalism, speeches, student essays, business plans, Kickstarter campaigns, *etc.* Research potential is substantial.</Text>
            <Comments>&lt;!--\label{footnote81}--&gt;[#Ali2010;] identified readability scores as useful tools for social scientists. In a large scale analysis of news content, they found stories on sports (male dominated) and entertainment (female dominated) most readable. [#Stempel1981;] reports similar findings in popular U.S. newspapers.
&lt;!--\label{footnote82}--&gt;[#Long2011;], [#Lehavy2011;] and [#ThÃ¶rnqvist2015;] use readability scores in interesting, non-conventional ways. The former investigates whether a legal brief's Flesch Reading Ease score is correlated with its success on appeal (it is not); the latter two use readability measures to proxy for complex information in financial reports, finding less readable material is less informative[#Lehavy2011], especially for non-sophisticated investors[#ThÃ¶rnqvist2015]. Since releasing the first version of this working paper (September, 2015) research using readability scores has ballooned. See[#Benoit2017;] for a review of more recent research.</Comments>
        </Document>
        <Document ID="AA128736-711D-460A-9A46-DD744F260D9D">
            <Title>preface</Title>
        </Document>
        <Document ID="DF4250CB-567F-4424-BDA7-6CDF0DB4F649">
            <Title>Other data and control variables</Title>
        </Document>
        <Document ID="013663EA-2D88-427D-BC6E-5084AC67FE10">
            <Title>Bargaining power</Title>
            <Text>The following section sketches model conclusions when the entrepreneur and creditor share bargaining power during debt renegotiations. Assume first bargaining power is allocated entirely to the entrepreneur. Creditors accept any debt contract that is no worse than their expected returns in formal bankruptcy; entrepreneurs therefore offer a revised debt contract $\wt D$ that solves
$$!e[p1equation14]\ol C_1^W=\ol C_1^B.e!$$
Similarly ,when creditors have all the bargaining power, entrepreneurs accept any workout proposal that gives them at least as much as their expected earnings in formal bankruptcy. Now, $\wt D$ is the solution to
$$!e[p1equation15]\ol E_1^W=\ol E_1^B.e!$$
When entrepreneurs and creditors share bargaining power, each extracts a proportion of the surplus from [](#p1equation14) and [](#p1equation15), respectively. To illustrate the concept, I introduce a new variable, $z$, where $z$ is a number between zero and 1. In this framework, the new debt contract, $\wt D$, solves
$$!e[p1equation16]z(\ol E_1^W-\ol E_1^B)=(1-z)(\ol C_1^W-\ol C_1^B).e!$$
 $z=0$ grants the entrepreneur all bargaining power; $z=1$ grants it to the creditor; $z\in(0,1)$ permits sharing workout surplus. To simplify exposition, [](#p1equation17) expresses [](#p1equation16) in terms of creditor returns, only.
$$!e[p1equation17]\ol C_1^W=C_1^B+z(\ol V_1^C-\ol V_1^B).e!$$
If a settlement is reached, the creditor prefers liquidation to a workout if and only if $\ol C_1^W\le C_1^L$; plugging in [](#p1equation17), the condition is
$$!e[p1equation18]\ol C_1^B+z(\ol V_1^C-\ol V_1^B)\le C_1^L.e!$$
The entrepreneur prefers liquidation to a workout if and only if $\ol E_1^W\le E_1^L$, or equaivalently
$$!e[p1equation19]C_1^L\le C_1^B+z(\ol V_1^C-\ol V_1^B)-(\ol V_1^C-V_1^L).e!$$
$\ol V_1^C\le V_1^L$ is a necessary condition for both [](#p1equation18) and [](#p1equation19) to hold simultaneously; thus, for no $z\in[0,1]$ are viable firms ever inefficiently liquidated in a settlement. Since $\ol C_1^W$ is increasing in $z$, creditor losses from default decline---as do credit rationing and the cost of debt.
For non-viable firms, a necessary and sufficient condition for $\wt D$ to exist is for returns in continuation to exceed creditors' expected returns in bankruptcy plus their bargaining surplus awarded during renegotiations:
$$!e[p1equation20]\ol C_1^B+z(\ol V_1^C-\ol V_1^B)\le \ol V_1^C.e!$$
 Because $\ol C_1^B$ is increasing in $D$, $\ol C_1^B\le\ol V_1^B$. Thus a sufficient (but not *necessary*) condition for [](#p1equation20) to be satisfied is
$$!e[p1equation21]\ol V_1^B\le \ol V_1^C,e!$$
 *i.e.*, the same condition from [](#prop1).
Consider the set of firms that violate [](#p1equation21). As $z$ increases, the left-hand side of [](#p1equation20) declines. Workouts more likely exist. Because time 0 claims by creditors that they will have no choice but to file for bankruptcy are less credible, entrepreneurs are less willing to accept a loan contract with an exhorbitant $D$ per [](#prop2)(ii). The profitable nice lending market [market](#cor1) shrinks. When creditors have all the bargaining power ($z=0$), [](#p1equation20) collapses to $\ol C_1^B\le\ol V_1^B$---which is always satisfied. [](#prop2)(i) applies to all firms; no firm goes bankrupt and there are no supranormal profits in lending.</Text>
        </Document>
        <Document ID="C0314BE2-F81B-44F7-8F5F-BCCA3DE230AB">
            <Title>Introduction</Title>
            <Text>Firms that need large capital outlays may not get credit any time insolvency involves reorganisation. When those procedures are expensive, however, even projects with low upfront investment needs and high expected earnings are at risk. As discussed in [][rescue_culture], wasteful reorganisation exacerbates moral hazard and opportunistic behaviour by weakening creditors' bargaining positions during workouts. Even if formal procedures are rarely used, reducing their cost should still improve the lending environment.
This is indeed what I find empirically. Using data from a broad, cross-country, firm-level survey conducted over several years, I show cheaper reorganisation is significantly and consistently correlated with lower self-reported obstacles to finance. This finding supports empirical work suggesting that lenders are less willing to make loans, charge higher interest rates, require shorter maturities and demand more collateral in countries with weak creditor protections[#Qian2007]. Ironically, rescue attempts in these countries are rarely successful[#Djankov2008] and official procedures seldom used[#Claessens2005].
Poorly informed and/or biased courts are thought to exacerbate the problem---possibly spawning a glut of distressed firms kept alive at creditors' expense[*e.g.*, Hungary, see\]\[][#Franks2013]. General law and economics theory suggest eliminating this kind of under-liquidation requires judicial accuracy. The literature focuses on how best to coax honesty out of disputing parties[see, *e.g.*,\]\[][#Milgrom1986,Milgrom1981]. Different models recommend various designs that induce truth-telling[see, *e.g.*,\]\[][#Fishman1990,Hay1997,Shin1994,Shin1998,Sobel1985], thereby improving the quantity and quality of information the judge has at her disposal. It follows, therefore, that better information helps judges distinguish between viable and non-viable firms and leads to more desirable outcomes in bankruptcy[see, *e.g.*,\]\[][#Ayotte2007,Bergman2007].
Possibly, however, low quality information performs a public service. When reorganisation is expensive, poorly informed bankruptcy courts are more likely to liquidate high-earning firms---forcing them to accept greater responsibility during workouts. The most informative signal is therefore not necessarily the ideal signal; judicial error may partially correct for a dysfunctional reorganisation system.
The relationship between judicial accuracy and lending depends on firm-specific characteristics which are unidentifiable from the data. To partially overcome this, I simulate a choice occasion (is finance an obstacle or not?) for subgroups of firms with similar traits and apply a mixed logit model. It appears better informed courts significantly increase self-reported financing obstacles for roughly two-thirds of firms but reduce it for the rest. I find no such heterogeneity when similarly evaluating reorganisation's cost.
Without data on surveyed firms' capital investments I cannot definitively attribute these disparate responses to the prediction of the model. Nevertheless, it is telling that heterogeneity is concentrated in countries with Chapter 11-style bankruptcy procedures. Where receivership is employed (see [][rescue_culture], [][p1policy_implications]) more information is undeniably a good thing---and coincides with prior empirical work in which borrower information positively affects private sector lending[see, *e.g.*,\]\[][#Jappelli2002,Love2003,Galindo2001,Berger2005,Brown2009].
In the next section, I theoretically deduce the impact of the cost of reorganisation and quality and depth of financial information using the model of [][rescue_culture]. [][p2Data] describes the data; [][p2strategy] outlines the empirical strategy; [][p2Results] presents results.</Text>
            <Comments>In a typical game, a judge resolves a conflict by ruling in favour of one side or the other. She hopes her ruling is socially optimal but insufficient information and each party's incentive to exaggerate his grievance means she's never certain what the socially optimal choice actually is. 
This angle is similar to one by [#Bernhardt2004;]. They illustrate that higher quality information may lead to outcomes *ex post* which induce suboptimal behaviour *ex ante*.
[#Pagano1993;] abstract entirely from bankruptcy yet reach an analogous conclusion: more information reveals credit-worthiness, forcing "risky" borrowers to pay more---sometimes pricing them out of the market. In their model, safety and risk are exogenously determined; my model suggests a root cause---knowing a borrower is safe or risky is loosely related to knowing a firm is not viable or viable when reorganisation is on the table. A safe firm is a non-viable one; their shorter expected lifespans reduce losses. Instead, riskiness is attached to viable firms, since they are more likely reorganised in bankruptcy and exploit that during debt renegotiations.</Comments>
        </Document>
        <Document ID="65CC5F5E-498B-453A-8A20-433A92BAF329">
            <Title>Analysis by *JEL* code</Title>
            <Text>[](#figure1) displays results from an ordinary least squares regression on the Dale-Chall score; regressors are: (i) ratio of female co-authors; (ii) dummies for each primary *JEL* code; (iii) interactions from (i) and (ii); (iv) controls for editor, journal, year, institution and English fluency; and (v) quality controls---citation count and $\text{max. }T_j$ fixed effects. Again, due to small samples---particularly of female authors---[](#figure1) includes 561 articles from *AER Papers &amp; Proceedings*.
&lt;!--\input{$HOME/Dropbox/Readability/draft/tex/fixed/figure1}--&gt;
The pink vertical line in [](#figure1)'s left-hand graph is the marginal effect of female authorship at the mean. Its estimate coincides with results in [](#table4)---women's papers require six fewer weeks of schooling to understand---and is highly significant. Points reflect marginal effects across *JEL* classification; bars represent 90 percent confidence intervals from standard errors clustered by editor.
Women earn higher marks for clarity in 11 out of 15 categories; only three are at least weakly significant: Q (Agricultural and Natural Resource Economics; Environmental and Ecological Economics), N (Economic History), and J (Labour Economics). Men may be better writers in C (Mathematical and Quantitative Methods), L (Industrial Organisation), O (Economic Development, Innovation, Technological Change, and Growth) and H (Public Economics); none, however, are statistically different from zero. [](#figure1)'s right-hand graph displays coefficients from interacting the ratio of female co-authors with each *JEL* code. Q and N are significantly above the mean, O and H significantly below it. Remaining categories are not statistically different from the mean effect.
In general, sample sizes are small and estimates imprecise---only Labour Economics and Microeconomics contain more than 100 papers written only by women (the others average 35). Nevertheless, [](#figure1) suggests two things. First, the mostly insignificant interaction terms indicate outlier fields are probably not driving journals' gender readability gap---nor is any specific field bucking the trend. Second, the number of women in a field appears to have little effect on the size of the gap: Agriculture/Environment has one of the lowest concentrations of female-authored papers---but Economic History has one of the highest (Labour Economics falls between the two). Of course, Economic History papers are still overwhelmingly---as in 74 percent---penned just by men. But given the readability gap is present in subfields with both above- and below-average rates of sole female authorship, women may need to be better writers even where more of them publish.
In the remainder of the paper, I do not explicitly control for *JEL* classification (unless otherwise specified). Comparable codes are available for only a subset of the data and [](#table4) and [](#figure1) suggest they are relatively unimportant, anyway.</Text>
            <Comments>&lt;!--\label{footnote33}--&gt;Codes A, B, M and P are dropped due to insufficient number of female-authored papers: each had fewer than 10 papers authored only by women. No paper is classified under category Y.
See [\]\[pp. 42--43][#Hengel2016;] for a version of  [](#figure1) excluding *AER Papers &amp; Proceedings* articles.</Comments>
        </Document>
        <Document ID="9E9E5009-40A1-40C8-B32E-0F718FDC0797">
            <Title>dissertation</Title>
            <Text>Latex input: dissertation-header
Title: &lt;$projecttitle&gt;
Author: &lt;$author&gt;
Abstract One: &lt;$synopsis&gt;
Abstract Two: &lt;$synopsis&gt;
Abstract Three: &lt;$synopsis&gt;
My Preface: This dissertation is the result of my own work and includes nothing which is the outcome of work done in collaboration. It is not substantially the same as any that I have submitted, or, is being concurrently submitted for a degree or diploma or other qualification at the University of Cambridge or any other University or similar institution. I further state that no substantial part of my dissertation has already been submitted, or, is being concurrently submitted for any such degree, diploma or other qualification at the University of Cambridge or any other University of similar institution. Finally, this dissertation does not exceed the prescribed word limit for the relevant Degree Committee.
Base Header Level: 2
Latex input: dissertation-begin
Latex footer: dissertation-footer</Text>
        </Document>
        <Document ID="575DB6AE-23C1-4C53-94DC-C1976F581540">
            <Title>Untitled</Title>
        </Document>
        <Document ID="EF1B19EC-C260-447E-81A6-5CA09E20A791">
            <Title>rescue_culture</Title>
            <Text>LaTeX input: dissertation-header
Title: &lt;$title&gt;
My Short Title: &lt;$custom:ShortTitle&gt;
Author: &lt;$author&gt;
Affiliation: &lt;$custom:Affiliation&gt;
Email: &lt;$custom:E-mail&gt;
My Abstract: &lt;$synopsis&gt;
Thanks Bitches: &lt;$custom:Thanks&gt;
File ID: &lt;$custom:CustomID&gt;
Keywords: &lt;$custom:Keywords&gt;
Subject: &lt;$custom:Subject&gt;
Base Header Level: 2
LaTeX input: dissertation-begin-single
LaTeX footer: dissertation-footer</Text>
        </Document>
        <Document ID="7B75AF10-D60E-4946-A17B-AA58B5A61C3F">
            <Title>Impact on identifing discrimination</Title>
            <Text>[](#figure6) supports [](#Theorem1)'s implicit assumption that female authors learn about referees' thresholds over time. If the payoff from lucid exposition is high, people will catch on---either by internalising explicit comments on text readability in referee reports from earlier papers or making the (un)conscious connection that acceptance rates are higher---or review times are faster---when text is clearer. Applying that payoff only to women yields a succinct explanation for the gapâs observed growth.
Although [](#table12) concurs, viewing certain estimates in isolation give different, more orthodox impressions. First, panel one suggests that the readability gap declines over increasing $t$. This narrow view favours alternative explanations---*e.g.*, sensitivity, poor information and/or justified statistical discrimination---over bias by referees and/or editors. Only when complemented by [](#figure6) do we fully appreciate that the smaller gap *in* peer review is completely offset by a wider gap *before* peer review. Unfortunately, this reaction poses another identification problem: senior female economists adjust to biased treatment in ways that confuse underlying discrimination with voluntary choice. Both observations suggest that studies must account for all relevant decisions at a single point in time *and* the evolution of those decisions over time. Otherwise, they may underestimate discrimination and misallocate responsibility.</Text>
            <Comments>This study suffers from the same criticism. For example, it does not take into account the impact higher standards have on (potential) female economists' choice of field, specific topic or even their decisions to remain in the workforce.</Comments>
        </Document>
        <Document ID="33922920-97C7-4AB0-9891-78C3EB391B19">
            <Title>New Folder</Title>
        </Document>
        <Document ID="91D308F1-FCC7-4479-94C4-1AFE4E664CB2">
            <Title>Analyses and results</Title>
            <Text>Analyses and results are organised as follows. In [][ArticleLevel], I scrutinise readability at the article level, controlling for editor, journal, year, journal and year interactions, institution, author productivity, article quality, English fluency and field. The results suggest a gap does indeed exist. They also rule out obvious confounding factors---women writing on easier topics, editorial policies in earlier eras, *etc.* [Next][AuthorLevel], I investigate readability at the author-level in a fixed effects regression. This accounts for author-specific productivity, quality and other effects that influence writing---*e.g.*, innate talent---but are otherwise unconnected to peer review.
In [][NBER], I match published articles---which have gone through peer review---to earlier, draft versions of the same papers---which have not. Assuming timing independence, this isolates the effect of peer review and causally links it to the gender readability gap. [][Experience] takes the final step and causally links the gap to referees and/or editors. I first develop a dynamic model of an author's decision-making process to evaluate the remaining [alternatives][SEUModel]: gender differences in biology/behaviour and/or knowledge about referee expectations. Based on the model, I propose a method for identifying the impact of discrimination on authors' readability. I then use matching to estimate [it][SEUMatching].
I then document evidence that higher standards affect behaviour and lower productivity. First, prolonged peer review should be one observable repercussion from subjecting female authors to higher standards. Using submit-accept times from *Econometrica*, I evaluate this hypothesis, controlling for, *inter alia*, motherhood, childbirth, citations and [field][Duration]. As a final exercise, I investigate how women react to higher standards as they update beliefs about referees' [expectations][IndirectEffect].</Text>
        </Document>
        <Document ID="ED7F365B-08A0-413B-AD7D-9121D3DDACAD">
            <Title>Open review</Title>
        </Document>
        <Document ID="1725448A-E244-4906-B0E0-8215E1C00505">
            <Title>Data</Title>
        </Document>
        <Document ID="C9C72080-660C-4D9D-934B-A2CA475674D9">
            <Title>Robustness</Title>
        </Document>
        <Document ID="1E7FDF0C-FEAE-488C-804A-8632B4A3F129">
            <Title>Results</Title>
            <Text>&lt;!--\input{$HOME/Dropbox/Readability/draft/tex/generated/table9}--&gt;
[](#table9)'s first row compares equivalent authors (holding experience constant): senior female economists write more readably than their male counterparts with identical experience. [](#table9)'s last two rows compare authors before and after gaining that experience (holding gender and preferences constant): women write more clearly once they "learn the ropes" in peer review; equivalent men do not. Meanwhile, lifetime publication counts---a crude approximation for acceptance rates---indicate men's more poorly written papers are accepted at least as frequently as women's.
[](#table9) and average publication counts confirm [][SEUEmpirical]'s analysis. [](#table10) goes further. It tests if Conditions 1 and 2 are both satisfied within each matched pair. Its first and second panels display the mean (first column) and standard deviation (second column) of $\underline D_{ik}$---[](#equation11)'s conservative estimate of [$D_{ik}$](#Corollary1)---and observation counts (third column) from the set of matched pairs in which one member satisfies both conditions. In the first panel, the female member does---suggesting discrimination against women---in the second, it's the male member---indicating discrimination against men. Male scores are subtracted from female scores, so $\underline D_{ik}$ is positive in panel one and negative in panel two.
&lt;!--\input{$HOME/Dropbox/Readability/draft/tex/generated/table10}--&gt;
Evidence of discrimination was present in roughly 65 percent of matched pairs---and in three-quarters of those, the member discriminated against was female. Moreover, $\underline D_{ik}$ is (on average) almost twice as large (in absolute value) when discrimination is against women.
[](#figure4) displays $\underline D_{ik}$'s distribution across the five scores. Pink bars correspond to matched pairs in which $\underline D_{ik}$ is positive (discrimination against women); blue bars reflect those for which $\underline D_{ik}$ is negative (discrimination against men).
In the absence of systemic discrimination against women (or men), $\underline D_{ik}$ would symmetrically distribute around zero. It does not. When men are discriminated against, $\underline D_{ik}$ clusters at zero. When women are discriminated against, $\underline D_{ik}$ spreads out. Furthermore, instances of obvious discrimination are predominately against women: $\underline D_{ik}$ is seven times more likely to be one standard deviation above zero than below it.
[](#table10)'s final panel averages $\underline D_{ik}$ over all observations. To account for the 30--40 percent of pairs for which [](#Theorem1) is inconclusive, (1) sets $\underline D_{ik}=0$, while (2) sets $\underline D_{ik}=\widehat R_{i3}-\widehat R_{k3}$ if $\widehat R_{i3}&lt;\widehat R_{k3}$ ($i$ female, $k$ male) and zero, otherwise.
Results confirm conclusions drawn from [](#figure4) and the first two panels of [](#table10). Discrimination by editors and/or referees predominately affects female authors. Mean $\underline D_{ik}$ is positive and significant in both columns for all five scores. Thanks to higher standards, senior female economists write (at least) nine percent more clearly than they otherwise would.
&lt;!--\input{$HOME/Dropbox/Readability/draft/tex/fixed/figure4}--&gt;
[][AppendixConservative] replicates [](#table10) using [](#equation12) to estimate $D_{ik}$. Results are very similar (and conclusions identical) to the analysis presented here.</Text>
            <Comments>See [](#Footnote67) for a discussion of the limitations of using publication counts to proxy for acceptance rates. Please also refer to [][SEUEmpirical] for a more thorough review of the (substantial) prior research on gender neutrality and journals' acceptance rates. It too finds no female advantage in journals' acceptance rates.
The co-variates used to generate a match remain relatively balanced when the sample of observations is restricted to $\underline D_{ik}\ne0$ (see [][AppendixMatchingBalance] and the next section for a discussion).
For 30--40 percent of pairs, neither member satisfied both Conditions 1 and 2, rendering [](#Theorem1)'s test for discrimination inconclusive.
That is, if the experienced man writes more readably than the experienced woman, then the effect is always attributed to discrimination against men; if the experienced woman writes more readably than the experienced man, however, the effect is attributed to discrimination against women only if Condition 2 is likewise satisfied.
[](#table10), column (1) divided by the mean male $\widehat R_{k3}$ ([][AppendixConservative]).</Comments>
        </Document>
        <Document ID="A4941446-2CDC-467D-8237-CB36E2E1BE3B">
            <Title>Measurement error</Title>
            <Text>Readability scores fail to capture many elements relevant to reading comprehension, including grammar---*e.g.*, active vs. passive tense[#Coleman1964,Coleman1965]---legibility---*e.g.*, typeface and layout---and content---*e.g.*, coherence, organisation and general appeal[#Kintsch1984,Kemper1983,Meyer1982,Armbruster1984]. Nevertheless, "long sentences generally correspond to complex syntactic structures, infrequent words generally refer to complex concepts, and hard texts will generally lead to harder questions about their content"[\]\[p. 222,][#Kintsch1984]. Moreover, combining readability scores with measures that capture omitted features does not significantly increase in their predictive power[see, *e.g.*,\]\[][#Kemper1983].
Readability scores are, however, prone to measurement error. If this error stems from the scores' low causal power and does not partially correlate with the variable of interest (gender), then the analytical results I present in this paper are biased toward zero (classical measurement error). Otherwise, my results will be systematically biased in an unknown direction (non-classical measurement error).
Sources of non-classical measurement error are threefold: (a) grammatical/spelling/transcription errors in the textual input; (b) bugs in the software that estimates vocabulary complexity and sentence length; or (c) embodied in the jump from using hard words and sentence length to infer readability.
Conditional on accurate calculation, readability scores do combine very precise estimates of vocabulary complexity with almost perfect measures of sentence length. As I argue in [][Discussion], the weighted average of these two variables is informative in much the same way that inferences about readability are. Thus, measurement error related to (c) should only affect the superficial interpretation of observed gender differences. Fundamental conclusions deduced from them should remain intact.
A bigger threat to the identification mapped out in this paper is non-classical measurement error from (a) and (b). I have taken several steps to minimise them. First, I use abstracts---as opposed to the body of a paper---as textual input. Abstracts are self-contained, universally summarise the research and are the first and most frequently read part of an article[#King2006]. Moreover, their layout is relatively standardised compared to other parts of a paper: abstracts tend to be surrounded by ample whitespace and most editorial management systems anyway reproduce them in pre-formatted cover pages. These factors suggest a relatively homogenous degree of review across journals and subject matter and limit the impact physical layout and surrounding tables, figures and text have on readability.
Abstracts are additionally ideal for minimising grammatical/spelling/transcription errors. They contain few of the features of text which can distort readability scores, *e.g.*, citations, abbreviations and equations. Additionally, most have already (and conveniently) been converted to accurate machine-readable text by digital libraries and bibliographic databases.
A third concern is that some programs that calculate readability scores rely on unclear, inconsistent and possibly inaccurate algorithms to count words and syllables, identify sentence terminations and determine whether a word is on Dale-Chall's easy word list[for a discussion, see\]\[][#Sirico2007]. Additionally, features of the text---particularly full stops used in abbreviations and decimals in numbers---frequently underestimate average words per sentence and syllables per word. To transparently handle these issues and eliminate ambiguity in how the readability scores were calculated, I wrote the Python module `Textatistic`. Its code and detailed documentation is available at [GitHub](https://github.com/erinhengel/Textatistic). A brief description is provided in [][AppendixTextatistic].
As an additional robustness check, I calculate readability scores using the `R` [`readability` package](https://github.com/trinker/readability). Not only does this alternative software confirm the validity of my own---results are very similar to those reported here---but since the algorithms it uses to calculate the scores is slightly different from my own---*e.g.*, it includes semi-colons among its sentence-ending characters---it suggests the results are robust to minor simplifying changes required when calculating readability scores via computer program instead of manually by a human.</Text>
            <Comments>Like intelligence tests, implicit association tests and machine learning algorithms, readability scores are low in causal power.
Prior research has found authors write in a stylistically consistent manner across the abstract, introduction and discussion sections of a paper[#Hartley2003b, Plaven-Sigray2017].
Additionally, all four journals have explicit text layout instructions for submissions; this should further limit the impact layout differences may have had on readability.
Typesetting code used to render equations---common in *Econometrica* abstracts published before 1980---also affects the accuracy of readability scores. I therefore manually replaced all such code with equivalent unicode characters. When no exact replacement existed, characters were chosen that mimicked as much as possible the equation's original intent while maintaining the same character and word counts. Readability scores were determined using the modified text.
All five readability scores suggest counting as sentences each grammatically independent unit of thought, not just the groups of words set off by terminal punctuation. Therefore, if a writer uses a semicolon or a dash to connect two independent clauses, that counts as two sentences. Unfortunately, semi-colons are not always used in this manner. They are also used to differentiate items in a list, items which are not, however, independent thoughts. When generating readability scores manually, this is less of an issue---humans can easily determine if a semi-colon is being used in one way or the other. Machines, however, have more difficulty with this task.</Comments>
        </Document>
        <Document ID="E8DAE991-565B-41FD-BE39-B148D6E56713">
            <Title>Empirical strategy</Title>
            <Text>Creditor returns ($\ol C_0$) determine lending constraints. In countries with dual-chapter bankruptcy, $\ol C_0$ depends on the project-specific distribution of earnings---an unknown function possibly non-linear in both $Y$ and $p$. I rely on a second order Taylor expansion in both variables to approximate it:where $\ol C_{0i_{jt}}$ represents creditor returns when lending money to a firm $i$ operating in the country $j$ during the year $t$; $\vep_{i_{jt}}$ is the residual error term.
In countries with receivership, $\ol C_0=K_0$ in all states. According to [](#cor3), $Y$ and $p$ have no impact. To test this hypothesis, the following first-order expansion is sufficient:
$\ol C_{0i}^B$ and $\ol C_{0i}^R$ are unobserved but represented by the manifest variable $\text{obstacle}_i$. Firm $i$ perceives access to finance as an obstacle only when it has (or has had) difficulty obtaining a loan---which occurs when creditors expect negative returns by making [one](#prop2), *i.e.*,
[](#p2equation5) estimates the impact of $Y$ and $p$ on getting credit by combining [](#p2equation2), [](#p2equation3) and [](#p2equation4):where $B_{jt}$ is an indicator variable equal to one if country $j$ employed dual-chapter bankruptcy in year $t$ and zero, otherwise. $\vect X_{i_{jt}}$ is a vector of [firm](#p2table2) and [country](#p2table3) control variables and country and year fixed effects. $\vep_{i_{jt}}$ is the auxiliary random variable, assumed to follow a standard normal distribution with cumulative distribution function $\Phi$.</Text>
            <Comments>\input{$PPATH/p2/equations/p2equation2}
\input{$PPATH/p2/equations/p2equation3}
\input{$PPATH/p2/equations/p2equation4}
\input{$PPATH/p2/equations/p2equation5}</Comments>
        </Document>
        <Document ID="E0990667-B3D3-4C51-8EDC-D663EC5C51BB">
            <Title>dissertation</Title>
        </Document>
        <Document ID="3ED6CBCB-3CF0-4241-B5E3-CB8A9B50649F">
            <Title>[](#table7), alternative measures of an article's "gender"</Title>
            <Text>[](#tableXA), [](#tableXB) and [](#tableXC) repeat the analysis shown in [](#table7) using three alternative measures of an article's "gender". In [](#tableXA), papers authored entirely by women are compared to papers authored entirely by men. In [](#tableXB), papers are considered "female" if at least one author is female. In [](#tableXC), article gender is represented by a binary variable equal to one if at least half of all authors are female; articles below that threshold with at least one female author are excluded.
&lt;!--



\clearpage--&gt;
</Text>
        </Document>
        <Document ID="AEF385F2-9CAF-4121-A023-17E9C5E0F643">
            <Title>[][SEUConsistency]</Title>
        </Document>
        <Document ID="7DB3228B-44E4-4081-AD8B-DD93EE91A189">
            <Title>Abstract word limits</Title>
            <Text>I attribute the difference between NBER abstracts and published abstracts to the peer review process. However, NBER has no word limit while two journals in my sample do: *Econometrica* limits abstracts to 150 words; *AER* limits them to 100 words. It is therefore possible that the effects we observe are due to authors compressing the content of their abstracts rather than due to peer review.
As shown in [](#table6), female-authored NBER abstracts tend to be slightly longer (more words) than male-authored abstracts---but then slightly shorter (again, in terms of total words) in published form. [](#figureC7a) shows the distribution of abstract lengths by gender. The graph on the left displays abstract length in NBER working papers; the graph on the right displays the distribution of abstract lengths in the published paper, again broken down by gender. Word counts in NBER drafts are more spread out; they are much more tightly compacted in the final, published version of those same papers. There appears to be no massive gender difference in the distribution of these two figures; also, there appears to be no outlier papers driving the results.
&lt;!--\input{$HOME/Dropbox/Readability/draft/tex/fixed/figureC7a}--&gt;
I also repeat the analysis in [](#table7) but this time only on articles whose NBER abstracts fell below the official minimum word limit of the respective journal in which it was published. The subsequent analysis effectively dropped about 40 percent of the observations. Results are presented in [](#tableC7a). Although standard errors are somewhat larger than those presented in [](#table7), the coefficient magnitudes are very similar.
&lt;!--\input{$HOME/Dropbox/Readability/draft/tex/generated/tableC7a}
\clearpage--&gt;</Text>
        </Document>
        <Document ID="15222CF6-76C0-4039-8B17-F0666374F200">
            <Title>Adjudication</Title>
            <Text>Bankruptcy occurs only if the entrepreneur and his creditor cannot agree whether to continue operating or liquidate the firm. Both parties expect strictly higher returns under separate outcomes. As illustrated in [](#lem1) and its [proof](#p1appendixproofs), the concavity of creditor returns and convexity of entrepreneurs' bind the latter to reorganisation, the former to liquidation.
LEMMA
In bankruptcy, the entrepreneur prefers to reorganise the project; the creditor to liquidate it.
elem
Guided by legislation, a judge settles the conflict. Insolvency law calls for reorganisation when it is "reasonably likely"[#HarrisSimmons1989] to maintain "the survival of the company, and the whole or any part of its undertaking, as a going concern"[\]\[Â§2(3)(a)][#GreatBritain1986]. Specifically, viable projects---projects with continuation value greater than their assets' piecemeal resale value---are reorganised, *i.e.*,
$$!e[p1equation1]V_1^L&lt;\ol V_1^C,e!$$
 where $\ol V_1^C$ is the time 1 expected value of $V_2^C$. When the reverse is true, the project is non-viable: unlikely to survive as a going concern without "unnecessarily harm[ing] creditors as a whole"[][#HarrisSimmons1989]. Non-viable projects are liquidated at time 1.
The judge does not know whether a project before her is viable. Although the creditor and entrepreneur do, per [](#lem1) the latter has an incentive to present evidence that the project should be reorganised while the former will argue just as forcefully for liquidation. The judge, meanwhile, gathers publicly available material on the project's assets and earnings and independent market research on similar projects in the same industry. All together, this information colours her opinion of the firms' true value and forms the basis of her ruling. That ruling is correct with probability $p$, where $p\in(\sfrac{1}{2},1)$ depends on the quality and veracity of the documentation provided by the entrepreneur and creditor as well as the intelligence the judge gathers herself. Creditors' expected returns just before her official ruling are
$$\ol C_1^B=q\,\ol C_1^R+(1-q)\,C_1^L,$$
 where $q=p$ if the project is a viable one, $1-p$ if it isn't and $\ol C_1^R$ is the time 1 expected value of $C_2^R$.
Even with perfect information the judge's decision is biased in favour of reorganisation. $\ol V_1^C$ drives her ruling ([](#p1equation1)), but the time 1 expected value of $V_2^R$, $\ol V_1^R$, really determines whether a bankrupt project is viable or not. Only when $\ol V_1^R$ exceeds $V_1^L$ does the project's true value---including reorganisation costs---surpass the principal amount of the loan. Since $\ol V_1^R&lt;\ol V_1^C$, too few bankrupt firms are liquidated and too many are reorganised.</Text>
            <Comments>British administration orders are granted on even weaker terms. The court must only be satisfied that the project has a "real prospect of" or "good, arguable case for" profitability---*i.e.*, less than the balance of probabilities[][#HarrisSimmons1989].
$p$ is an inverse function of the variance of information available on the distribution of time 2 earnings. When financial records provided by the firm are accurate---*e.g.*, because disclosure laws are strong---and significant outside information exists on similar firms---*e.g.*, because the financial analysis sector is well developed---the variance of information is low; $p$ is close to 1. When the opposite is true, the variance of information is high; the judge's decision is random ($p=\sfrac{1}{2}$).
&lt;!--\label{p1footnote1}--&gt;Theoretically, the judge could base her decision either wholly or partially on [](#p1equation1). However, insolvency laws rarely (or only vaguely) reference such costs, giving judges little scope or even desire to adjust their rulings[*e.g.*, judges may prefer to rule in line with legal precedent to prevent being overturned on appeal; see\]\[][#Gennaioli2008]. Additionally, given reorganisation's costs are difficult to quantify and tend to occur long after a judge has ruled in a particular case, she has few opportunities to educate herself on their extent and incorporate them into future rulings. Indeed, she may not even wish to make an accurate decision to begin with. Forum-shopping and judges' desire to attract high-profile bankruptcy cases may lead to a preference for reorganisation over liquidation[][#Gennaioli2010].</Comments>
        </Document>
        <Document ID="FA72F2D4-234B-4D2B-B5ED-16F2115A1EC3">
            <Title>Model</Title>
            <Text>I adapt the model in[#Bartos2016;] to a single-stage decision maker's choice about a submission. Before the game starts, I assume papers have undergone an initial screening by editors to rule out submissions that fail to satisfy the following condition:

$$\EE[q]=q_G+q_1+\EE[q_2]&gt;0,$$

where $q$ is the unknown objective quality of the paper, and can be expressed as the sum of observable $q_G$ and $q_1$ and unobservable $q_2$. $q_G$ is the average quality of the author's gender group, $G$; $q_1$ was revealed during pre-screening and shows an initial indication of the degree to which the paper's quality deviates from the group average; $q_2$ is the unknown portion of the paper's quality.

For the decision maker, the decision to reject is based on the inherent unknown payoff $\pi$, which consists of two components,

$$\pi=q-d_G,$

where $d_G$ is the referee's distaste toward the author's gender $G$. $d_G$ is private information to the referee. The reservation payoff from rejecting the applicant is 0. 

Figure X illustrates the game in extensive form. Pre-screening reveals $q_G$ and $q_1$, both of which are then observed by the referee. Post pre-screening, the referee may either to accept the paper immediately or instead postpone the decision until he has fully reviewed the document.

In the latter case, the referee reads the paper in detail and writes a referee report. This stage also includes re-reviewing the paper in the event the referee requested clarifications of and/or changes to the paper. This process is assumed to be time-consuming for the referee, and this cost is captured by $c$. Once the process is complete, however, $q_2$ is revealed, thus the referee is rewarded by knowing $q$ precisely. At this stage, when all costs of information acquisition are sunk, the applicant is accepted if and only if $q&gt;d_G$.

Define $q_1^\star$ as the $q_1$ below which the paper is reviewed and above which it is accepted immediately. That is, $q_1^\star$ is the $q_1$ that satisfies the following condition:

$$c=-\EE[\min\{q_G+q_1^\star+q_2,0\}].$$

I now describe how the probability of spending longer in review---reflected by the fact that the paper is not immediately accepted---increases regardless of the specific form of discrimination that is present.

Proposition 1. Lower $q_G$ and $\sigma_2$ and higher $d_G$ are more heavily scrutinised in lemon-dropping markets.

A distaste toward a certain group is captured by the parameter $d_G$[#Becker1971]. Here, a higher distaste implies more attention is paid to the applicant. Costly information serves to fast track certain expected high quality submissions into acceptance to potentially save on the cost of reviewing them.

Next, I consider statistical discrimination[#Phelps1972,Arrow1973], which is driven by differences in beliefs about the applicant's quality. In the model, this channel is represented by a change in $q_G$. The implications of a reduction in $q_G$ are then the same for an increase in the distaste parameter $d_G$: more attention is paid to the applicant.

Last, I consider the effects on attention of a greater difficulty to understand signals from a culturally dissimilar group. In a departure from[#Bartos2016;], I interpret this in that less information was revealed in the pre-screening phase, and so $\sigma_2$ is higher. This results in a mean-preserving spread, and serves to increase the benefits of filtering out bad papers during review.</Text>
            <Comments>I assume revealing $d_G$ to the editor would cause reputational harm in ecess of $d_G$. This assumption prevents the referee from rejecting a paper without reviewing it first; I make it in order to rule out blatant discrimination. The only way to reject a paper without revealing this information is to justify rejection on some other grounds---a process which requires reviewing the paper.
Immediate rejection is ruled out, since doing so reveals $d_G$.
At this stage, I assume the referee is sufficiently familiar with the paper to justify rejecting it without revealing $d_G$.
An alternative way to interpret $d_G$ is t osuggest it captures the extent to which referees *feel* (wrongly) the initial screening phase by (unbiased) editors was unfair. That is, if referees feel that editors are pressured to publish more female-authored papers</Comments>
        </Document>
        <Document ID="BE72F41A-3C38-45B4-8D5D-06D35D41B25A">
            <Title>[](#table10), male effects</Title>
        </Document>
        <Document ID="1EDCF80E-2564-4FEF-B28C-EB4E1EBFD44C">
            <Title>Controls</Title>
            <Text>For every article I recorded authors' institutional affiliations. Individual universities in U.S. State University Systems were coded separately (*e.g.*, UCLA and UC Berkeley) but think tanks and research organisations operating under the umbrella of a single university were grouped together with that university (*e.g.*, the Cowles Foundation and Yale University). Institutions linked to multiple universities are coded as separate entities (*e.g.*, Ãcole des hautes Ã©tudes en sciences sociales).
In total, 1,039 different institutions were identified. I create 64 dummy variables, each of which represents one or more institution(s); groupings reflect counts of distinct articles in which an institution was listed as an affiliation. Specifically, institutions listed in 59 or fewer articles were grouped in bins of 10 to form six dummy variables: the 751 institutions mentioned in 0--9 articles were grouped to form the first dummy variable, the 92 mentioned in 10--19 articles were grouped to form the second, *etc.* Fifty-eight institutions were affiliated with 60 or more articles; each is assigned its own dummy variable. When multiple institutions are associated with an observation, only the dummy variable with the highest-rank is used, *i.e.*, the highest-ranked institution per author when data is analysed at the author-level and the highest-ranked institution for all authors when data is analysed at the article-level.
I control for article quality and author productivity in several ways. First, I use article citations from the [Web of Science](https://login.webofknowledge.com/error/Error?Error=IPError&amp;PathInfo=%2F&amp;RouterURL=https%3A%2F%2Fwww.webofknowledge.com%2F&amp;Domain=.webofknowledge.com&amp;Src=IP&amp;Alias=WOK5) database. Second, I generate 30 dummy variables that group authors by career-total publication counts in the four journals. For example, Daron Acemoglu and Jean Tirole form one group (each published 45 articles as of December 2015); Alvin Roth, Elhanan Helpman and Gene Grossman form another (27 articles). In [][NBER] and [][Duration], I additionally control for the number of prior top-four papers (at time of publication). For co-authored articles, only the data corresponding to the most prolific author is used.
To account for English fluency, most regressions include a dummy variable equal to one if an article is co-authored by at least one native (or almost native) English speaker. I assume an author is "native" if he: (i) was raised in an English-speaking country; (ii) obtained all post-secondary eduction from English speaking institutions; or (iii) spoke with no discernible (to me) non-native accent. This information was almost always found---by me or a research assistant---in authors' CVs, websites, Wikipedia articles, faculty bios or obituaries. In the few instances where the criteria were ambiguously satisfied---or no information was available---I asked friends and colleagues of the author or inferred English fluency from the author's first name, country of residence or surname (in that order).
I create dummy variables corresponding to the 20 primary and over 700 tertiary *JEL* categories to control for subject matter. The *JEL* system was significantly revised in 1990; because exact mapping from one system to another is not possible, I collected these data only for articles published post-reform---about 60 percent of the dataset. Codes were recorded whenever found in the text of an article or on the websites where bibliographic information was scraped. Remaining articles were classified using codes from the American Economic Association's Econlit database.
To control for editorial policy, I recorded editor/editorial board member names from issue mastheads. *AER* and *Econometrica* employ an individual to oversee policy. *JPE* and *QJE* do not generally name one lead editor and instead rely on boards composed of four to five faculty members at the University of Chicago and Harvard, respectively. Editor controls are based on distinct lead editor/editorial boards---*i.e.*, they differ by at least one member. In total, 74 groups are formed in this manner.
The matching exercise in [][SEUMatching] pairs authors using various factors, including their fraction of first-authored papers. First authors are those identified in the acknowledgements or listed first when authors are not ordered alphabetically.
To control for motherhood's impact on revision times, I recorded children's birth years for women with at least one 100 percent female-authored paper in *Econometrica*. I personally (and, I apologise, rather unsettlingly) gleaned this information from published profiles, CVs, acknowledgements, Wikipedia, personal websites, Facebook pages, [intelius.com](https://www.intelius.com) background checks and local school district/popular extra-curricular activity websites. Exact years were recorded whenever found; otherwise, they were approximated by subtracting a child's actual or estimated age from the date the source material was posted online. If an exhaustive search turned up no reference to children, I assumed the woman in question did not have any.</Text>
            <Comments>[#Blank1991;] ranks institutions by National Academy of Science departmental rankings. Those and similar official rankings are based largely on the number of papers published in the journals analysed here.
&lt;!--\label{fn7}--&gt;This quality/productivity control has several limitations: (i) it relies on publication counts---not necessarily an accurate measure of "quality"; (ii) it discounts current junior economists' productivity; and (iii) it generates somewhat inconsistent groupings---for example, two authors have published 45 articles, but only one author has published 37 (Andrei Shleifer).
In [\]\[p. 42 and p. 44][#Hengel2016;], I experiment with another measure of quality---the order an article appeared in an issue. It has no noticeable impact on the coefficient of interest or its standard error.
Non-native speakers who meet this criteria have been continuously exposed to spoken and written English since age 18. This continuous exposure likely means they write as well as native English speakers. To qualify as an English speaking institution, all courses---not just the course studied by an author---must be primarily taught in English. *E.g.*, McGill University is classified as English-speaking; University of Bonn is not (although most of its graduate economics instruction is in English).
I also conducted a primitive surname analysis [see\]\[pp. 35--36][#Hengel2016]. It suggests that the female authors in my data are no more or less likely to be native English speakers.
In recent years, *JPE* has been published under the aegis of a lead editor.
While the information I found was publicly available, I apologise for the obvious intrusion.
In several instances, I obtained this information from acquaintances, friends and colleagues or by asking the woman directly. Given its sensitive nature, children's birth years are not currently available on my website (unlike other data in this paper).</Comments>
        </Document>
        <Document ID="2D388486-9B1B-481D-8613-4488645916E0">
            <Title>Open review</Title>
            <Text>Academia's female productivity gap is as stubborn as the business world's pay gap; yet, if every paper a woman writes needs ***six more months*** to finish review, our "Publishing Paradox" seems much less paradoxical.
Is the answer double-blind review? Probably not. Double-blind review cannot stop referees from guessing authors' identities---which they did with surprising accuracy before the internet[#Blank1991], and presumably perfect accuracy after it.
Instead, eliminate single-blind review, too. A randomised controlled trial at the *British Journal of Psychiatry* suggests referee reports are better quality and less abusive when identities are known[#Walsh2000]. Posting them online---as the *British Medical Journal* does---virtually guarantees continuous, independent audits by outside researchers. Worries that reviews are less critical and/or relationships are strained are either unfounded or alleviated by the deep pool of referees common to general interest journals[#vanRooyen1999,vanRooyen2010]. Open review does incur costs---some people refuse to participate and those that don't spend marginally more time drafting reports[#vanRooyen1999,Walsh2000]---but if more accountability promotes fairer outcomes, ethical arguments in its favour should outweigh minor practical concerns.</Text>
            <Comments>&lt;!--\label{footnote77}--&gt;Virtually every study on gender differences in scientific publishing rates find men more productive than women[for a list, see\]\[][#Ceci2014]. It's no different in my data: women published on average 1.7 articles; men managed 2.4---and with far more concentration in the distribution's right tail (for example, 56 men have published 16 or more times in the data, but no woman). Women produce fewer papers even when they don't have any children[#Ceci2014]. Appropriate controls for teaching and service do not account for it[#Xie2005], and it isn't a question of time, since female academics work just as many hours as men[#Ceci2014,Ecklund2011].
The *BMJ* posts reviewers' signed reports, authors' responses and the original manuscript on its website. No documentation is posted for rejected papers, but doing so may be beneficial: (i) A very public review implies a very public rejection; concern for one's reputation could reduce the number of low quality submissions. (ii) The onus of discovering mistakes would be shared with the wider economics community. (iii) Other journals can make publication decisions based on posted reviews---possibly reducing time spent refereeing for the discipline, as a whole. Women may receive greater scrutiny online---as they do at the *Guardian*[#Gardiner2016]---but the difference can be mitigated if comments are non-anonymous, made only by verified members of an appropriate professional society and continuously (and publicly) audited for bias in quantity and quality of feedback.
Each study employed a different research design; nevertheless, both estimate roughly 12 percent of reviewers decline to participate because they oppose open peer review while signing reports increases time spent on the review by 25 minutes. When referees were told their signed reviews might be posted online, time rose by an additional half hour and refusal rates were much higher (55 percent)[#vanRooyen2010].</Comments>
        </Document>
        <Document ID="D6DE499D-5502-4E75-9D27-C7558A0DF419">
            <Title>Robustness</Title>
        </Document>
        <Document ID="EAAB808A-4643-4CB6-A498-D080AD766950">
            <Title>Anonymity</Title>
            <Text>Gender bias is possible only when authors' identity is known or can be reasonably guessed. Two factors make that feasible: non-blind review and Google. Do either exacerbate the readability gap between male- and female-authored abstracts? No and Yes. While the gap has grown with search engines' popularity and accuracy, double-blind review does not seem to dampen it. And it may make things worse.</Text>
            <Comments>Specifically, single-blind review (the author is blinded to the referee's identity but not the reverse) or review under which the identities of both parties are mutually known.</Comments>
        </Document>
        <Document ID="DAFC1B78-BEAA-4FDB-BCFD-EDC5F3230177">
            <Title>[][SEUMatching], $\widehat R_{it}$ regression output</Title>
            <Text>[](#tableC12) and [](#tableC13) displays output from time- and gender-specific regressions used to generate $\widehat R_{it}$. (Output for male authors at $t=1$ not shown.)
&lt;!--\input{$HOME/Dropbox/Readability/draft/tex/generated/tableC12}--&gt;
&lt;!--\input{$HOME/Dropbox/Readability/draft/tex/generated/tableC13}--&gt;</Text>
        </Document>
        <Document ID="098F9E05-1281-4B45-BB3E-77392F6CC463">
            <Title>Lending environment</Title>
            <Text>I also include several controls for a country's general lending environment. Data on inflation, domestic credit to the private sector, GDP per capita and income from abroad per capita are culled from the World Bank's World Development Indicators (WDI) database. The percentage of firms using banks to finance investment is drawn from survey responses in the WES. Data are summarised in [](#p2table2).
Lending environment averages in countries covered by the WES reflect their focus on developing and emerging markets. Mean inflation is 6.2 percent, about 3.8 percentage points above the 2014 world average. Domestic credit to the private sector is considerably lower than the world average: 37.4 percent as opposed to 125.2 percent. GDP per capita is about half: $5,364 versus $10,803. Income from abroad is negative, in contrast to a positive world average of $54.</Text>
            <Comments>&lt;!--\label{p2footnote1}--&gt;Data are originally sourced from national accounts and international financial statistics compiled or estimated by the OECD, World Bank and/or IMF. Uzbek and South Sudanese data on credit to the private sector from [#EBRD2009,EBRD2013;] and [#IMF2014;], respectively. WES data are merged with nearest year for which WDI data exist.
\input{$PPATH/p2/tables/tex/p2table2}
Weighted world averages from the WDI database.</Comments>
        </Document>
        <Document ID="CDA135C4-86E2-4B9D-B900-38896CD643A1">
            <Title>Tables</Title>
            <Text>&lt;!--\input{$PPATH/p2/tables/tex/p2tableB9}--&gt;</Text>
            <Notes>&lt;!--\end{appendices}--&gt;</Notes>
        </Document>
        <Document ID="2A425993-9056-4E20-BF99-9D618B607519">
            <Title>Dissertation</Title>
        </Document>
    </Documents>
</SearchIndexes>