{\rtf1\ansi\ansicpg1252\cocoartf2580
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\fswiss\fcharset0 Helvetica;\f1\fnil\fcharset204 PTSerif-Regular;\f2\fnil\fcharset204 PTSerif-Italic;
\f3\fnil\fcharset204 PTMono-Regular;}
{\colortbl;\red255\green255\blue255;\red0\green0\blue128;\red0\green128\blue64;}
{\*\expandedcolortbl;;\csgenericrgb\c0\c0\c50196;\csgenericrgb\c0\c50196\c25098;}
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\sl288\slmult1\sa200\pardirnatural\partightenfactor0

\f0\fs24 \cf0 <$Scr_Ps::0>
\f1\fs28 The data include every English-language article published with an abstract in *
\f2\i AER
\f1\i0 *, *
\f2\i ECA
\f1\i0 *, *
\f2\i JPE
\f1\i0 * and *
\f2\i QJE
\f1\i0 * between January 1950 and December 2015 (inclusive). The largest sample is from *
\f2\i Econometrica
\f1\i0 * which consistently published abstracts with its articles prior to 1950. *
\f2\i JPE
\f1\i0 * added them in the 1960s and *
\f2\i QJE
\f1\i0 * in 1980. *
\f2\i AER
\f1\i0 * came last in 1986. Errata and corrigenda are excluded, as are articles from *Papers & Proceedings* (*P&P*) issues of *AER*, unless otherwise mentioned. 
\f0\fs24 \cf2 <$Scr_Cs::1>
\f3\fs28 [][AppendixArticleCount]
\f0\fs24 \cf0 <!$Scr_Cs::1>
\f1\fs28  displays data coverage by journal and decade.\

\f0\fs24 <!$Scr_Ps::0>
\f1\fs28 For textual input, I use abstracts. Abstract readability is strongly positively correlated with the readability of other sections of a paper (see 
\f0\fs24 \cf2 <$Scr_Cs::1>
\f3\fs28 [](#figure3)
\f0\fs24 \cf0 <!$Scr_Cs::1>
\f1\fs28  and 
\f0\fs24 \cf3 <$Scr_Cs::2>
\f3\fs28 [#Hartley2003b,Plaven-Sigray2017;]
\f0\fs24 \cf0 <!$Scr_Cs::2>
\f1\fs28 ). Its structure is standardised in a manner optimal for computing readability scores. Many abstracts have also been converted to accurate machine readable text therefore curbing errors in transcription.\

\f0\fs24 <$Scr_Ps::0>
\f1\fs28 For the analysis in 
\f0\fs24 \cf2 <$Scr_Cs::1>
\f3\fs28 [][NBER]
\f0\fs24 \cf0 <!$Scr_Cs::1>
\f1\fs28 , I collected draft abstracts from NBER Technical and Working Paper Series. To match published articles with their NBER drafts, I used citation data from RePEc and searched NBER's database directly for unmatched papers authored by NBER family members. I eventually matched 1,988 NBER working papers to 1,986 published articles. (The mapping is not one-for-one because a small number of working papers were eventually published as multiple articles or combined into one.) This represents approximately one-fifth of all manuscripts in the data and a third of all manuscripts published between 1990--2015. Descriptive statistics are shown in 
\f0\fs24 \cf2 <$Scr_Cs::1>
\f3\fs28 [][NBERResults]
\f0\fs24 \cf0 <!$Scr_Cs::1>
\f1\fs28 .\

\f0\fs24 <!$Scr_Ps::0>
\f1\fs28 The analysis in 
\f0\fs24 \cf3 <$Scr_Cs::2>
\f3\fs28 [][Duration]
\f0\fs24 \cf0 <!$Scr_Cs::2>
\f1\fs28  compiles submit-accept times at *
\f2\i Econometrica
\f1\i0 * (1970--2015) and *
\f2\i REStud
\f1\i0 * (1976--2015), a fifth highly respected economics journal. (*AER*, *JPE* and *QJE* do not publish the dates manuscripts were submitted and accepted.) I obtained the data from journals' online archives or extracted it from digitised articles using the open source command utility `pdftotext`. 
\f0\fs24 \cf2 <$Scr_Cs::1>
\f3\fs28 [][Duration]
\f0\fs24 \cf0 <!$Scr_Cs::1>
\f1\fs28  displays and discusses basic summary statistics.\
In [][Quantification], I analyse readability at the author-level using both the entire sample and the sample of published articles matched with NBER working papers. To generate a panel dataset following author $i$ over the $t\\in\\\{1,\\ldots\\,T_i\\\}$ papers he publishes in a top-four journal, I duplicate each article $N_j$ times, where $N_j$ is the number of co-authors on paper $j$. I then assign observation $j_n$ article $j$'s $n\\text\{th\}\\in\\\{1,\\ldots,N_j\\\}$ co-author. To account for duplicate articles, observations in relevant estimates are weighted by $1/N_j$.\

\f0\fs24 <$Scr_Ps::0>
\f1\fs28 To control for the impact of blinded review, I constructed a dummy variable equal to one if a paper underwent double-blind review before the internet at *AER* and *QJE*, the only two journals with an official double-blind review policy in place at some point during the time period covered by the {\field{\*\fldinst{HYPERLINK "scrivcmt://EBCDD80C-790C-4ADB-B380-C4C115C28CED"}}{\fldrslt data.}} *QJE* employed double-blind procedures until 1 June, 2005; *AER* between 1 July, 1989 and 1 July, 2011. From 1 May 1987 to 31 May 1989, the *AER* conducted a randomised controlled trial whereby half of all submitted papers were evaluated by single-blind review; the remaining half were subjected to double-blind review[#Blank1991]. Since referees correctly identified at least one author in 45.6 percent of double-blind reviewed papers, however, only about a quarter of these manuscripts were truly blind reviewed. I therefore classify every paper published during the trial as having undergone single-blind review.\

\f0\fs24 <!$Scr_Ps::0>
\f1\fs28 Other control variables used in the analysis include editor fixed effects, dynamic institution fixed effects, primary and tertiary *
\f2\i JEL
\f1\i0 * fixed effects, controls for author prominence and seniority, English fluency dummies, citation counts (asinh), and controls for motherhood and childbirth (
\f0\fs24 \cf2 <$Scr_Cs::1>
\f3\fs28 [][Duration]
\f0\fs24 \cf0 <!$Scr_Cs::1>
\f1\fs28 , only). I additionally categorised each tertiary *JEL* code as either theory/methodology, empirical or other in order to roughly account for how theoretical vs. empirical a paper is. See 
\f0\fs24 \cf2 <$Scr_Cs::1>
\f3\fs28 [][AppendixControls]
\f0\fs24 \cf0 <!$Scr_Cs::1>
\f1\fs28  for further information on how each of these variables were calculated.}