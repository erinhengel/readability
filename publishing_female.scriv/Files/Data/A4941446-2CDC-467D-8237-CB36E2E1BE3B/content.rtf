{\rtf1\ansi\ansicpg1252\cocoartf2580
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\fswiss\fcharset0 Helvetica;\f1\fnil\fcharset204 PTSerif-Regular;\f2\fnil\fcharset204 PTSerif-Italic;
\f3\fnil\fcharset204 PTMono-Regular;}
{\colortbl;\red255\green255\blue255;\red0\green128\blue64;\red0\green0\blue128;\red16\green128\blue214;
}
{\*\expandedcolortbl;;\csgenericrgb\c0\c50196\c25098;\csgenericrgb\c0\c0\c50196;\csgenericrgb\c6100\c50000\c84000;
}
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\sl288\slmult1\sa200\pardirnatural\partightenfactor0

\f0\fs24 \cf0 <$Scr_Ps::0>
\f1\fs28 Readability scores fail to capture many elements relevant to reading comprehension, including gram\\\\-mar---*
\f2\i e.g.
\f1\i0 *, active vs. passive tense
\f0\fs24 \cf2 <$Scr_Cs::2>
\f3\fs28 [#Coleman1964,Coleman1965]
\f0\fs24 \cf0 <!$Scr_Cs::2>
\f1\fs28 ---legibility---*
\f2\i e.g.
\f1\i0 *, typeface or layout---and content---*
\f2\i e.g.
\f1\i0 *, coherence, organisation and general appeal
\f0\fs24 \cf2 <$Scr_Cs::2>
\f3\fs28 [#Kintsch1984,Kemper1983,Meyer1982,Armbruster1984]
\f0\fs24 \cf0 <!$Scr_Cs::2>
\f1\fs28 . Nevertheless, "long sentences generally correspond to complex syntactic structures, infrequent words generally refer to complex concepts, and hard texts will generally lead to harder questions about their content"
\f0\fs24 \cf2 <$Scr_Cs::2>
\f3\fs28 [\\]\\[p. 222][#Kintsch1984]
\f0\fs24 \cf0 <!$Scr_Cs::2>
\f1\fs28 .\
Still, readability scores' low causal power raises legitimate concerns about measurement error. As long as this error does not partially correlate with the variable of interest (gender), the analytical results I present in this paper attenuate toward zero (classical measurement error). Unfortunately, they are systematically biased in an unknown direction if it does (non-classical measurement error).\
Sources of non-classical measurement error are threefold: (a) grammatical, spelling and transcription errors in the textual input; (b) errors in the estimates of vocabulary complexity and sentence length introduced by automating their calculation; or (c) embodied in the jump from using these two variables to infer readability.\
Conditional on accurate calculation, readability scores combine very precise estimates of vocabulary complexity with almost perfect measures of sentence length
\f0\fs24 \cf2 <$Scr_Cs::2>
\f3\fs28 [for a discussion, see\\]\\[][#Chall1995]
\f0\fs24 \cf0 <!$Scr_Cs::2>
\f1\fs28 . The weighted average of these two variables is informative in much the same way that inferences about readability are. Thus, measurement error related to (c) should only shift superficial interpretation of observed gender differences---from "women are better writers" to "women use simpler words and write shorter sentences"---but leave conclusions deduced from them intact.\
Nevertheless, I try to minimise measurement error from (c) by using abstracts as textual input. Abstracts are self-contained, universally summarise the research and are the first and most frequently read part of an article
\f0\fs24 \cf2 <$Scr_Cs::2>
\f3\fs28 [#King2006]
\f0\fs24 \cf0 <!$Scr_Cs::2>
\f1\fs28 . Additionally, they follow a more standardised layout compared to other parts of a manuscript: they are generally surrounded by ample whitespace and most editorial management systems anyway reproduce them in pre-formatted cover pages. These factors suggest a relatively homogenous degree of review across journals and subject matter and limit the impact that physical layout, figures and surrounding text have on readability.\
Moreover, prior research suggests authors write in a stylistically consistent manner across the abstract, introduction and discussion sections of a paper. According to an analysis of published education and psychology articles, within-manuscript correlations of Flesch Reading Ease scores range from 0.64 (abstracts vs. introductions) to 0.74 (abstracts vs. discussions)
\f0\fs24 \cf2 <$Scr_Cs::2>
\f3\fs28 [#Hartley2003b]
\f0\fs24 \cf0 <!$Scr_Cs::2>
\f1\fs28 . 
\f0\fs24 \cf2 <$Scr_Cs::2>
\f3\fs28 [#Plaven-Sigray2017;]
\f0\fs24 \cf0 <!$Scr_Cs::2>
\f1\fs28  also found a strong positive correlation using full text articles from several scientific journals. 
\f0\fs24 \cf3 <$Scr_Cs::3>
\f3\fs28 [](#figure3)
\f0\fs24 \cf0 <!$Scr_Cs::3>
\f1\fs28  plots abstract readability against the readability of a passage from the introduction for 339 NBER Working Papers eventually published in a top-four journal. It suggests a similarly positive relationship holds in economics, as {\field{\*\fldinst{HYPERLINK "scrivcmt://9DAF6B82-C1BD-4588-A9F7-E2DE276F4133"}}{\fldrslt well.}}\
\pard\tx706\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\sl288\slmult1\sa240\pardirnatural\qc\partightenfactor0

\f0\fs24 \cf4 <!$Scr_Ps::0><$Scr_Ps::1>
\f3\fs18 <!--\\input\{$HOME/Dropbox/Readability/draft/0-tex/fixed/Figure-D.2\}-->
\f1\fs28 \cf0 \
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\sl288\slmult1\sa200\pardirnatural\partightenfactor0

\f0\fs24 \cf0 <!$Scr_Ps::1><$Scr_Ps::0>
\f1\fs28 In my opinion, non-classical measurement error from (a) and (b) poses a bigger concern to the identification mapped out in this paper. I have taken several steps to reduce it. First, abstract text is also ideal for calculating readability: 100--200 words containing few score-distorting features of academic writing---*
\f2\i e.g.
\f1\i0 *, citations, abbreviations and equations
\f0\fs24 \cf2 <$Scr_Cs::2>
\f3\fs28 [#Dale1948]
\f0\fs24 \cf0 <!$Scr_Cs::2>
\f1\fs28 . Additionally, most abstracts have been previously converted to accurate machine-readable text by digital libraries and bibliographic databases, curbing errors in transcription.\
Second, I carefully proofread the text in order to identify (and fix) remaining transcription {\field{\*\fldinst{HYPERLINK "scrivcmt://6761B48D-B326-4318-B14D-6528B457FD92"}}{\fldrslt errors,}} eliminate non-sentence-ending full stops, and replace typesetting code---typically used to render equations---with equivalent unicode {\field{\*\fldinst{HYPERLINK "scrivcmt://00E17AB3-7653-4CB3-A6FD-618B92B93F28"}}{\fldrslt characters.}} Readability scores were determined using the modified text.\
Finally, some programs that calculate scores rely on unclear, inconsistent and possibly inaccurate algorithms to count words and syllables, identify sentence terminations and check whether a word is on Dale-Chall's easy word list
\f0\fs24 \cf2 <$Scr_Cs::2>
\f3\fs28 [for a discussion, see\\]\\[][#Sirico2007]
\f0\fs24 \cf0 <!$Scr_Cs::2>
\f1\fs28 . To transparently handle these issues and eliminate ambiguity in how the scores were calculated, I wrote the Python module `Textatistic`. Its code and documentation are available on 
\f0\fs24 \cf2 <$Scr_Cs::2>
\f3\fs28 [GitHub]({\field{\*\fldinst{HYPERLINK "https://github.com/erinhengel/Textatistic"}}{\fldrslt \cf2 https://github.com/erinhengel/Textatistic}})
\f0\fs24 \cf0 <!$Scr_Cs::2>
\f1\fs28 ; a brief description is provided in 
\f0\fs24 \cf3 <$Scr_Cs::3>
\f3\fs28 [][AppendixTextatistic]
\f0\fs24 \cf0 <!$Scr_Cs::3>
\f1\fs28 .\
For added robustness, I also re-calculate scores and replicate most results using the 
\f0\fs24 \cf3 <$Scr_Cs::3>
\f3\fs28 [[`R` `readability` package](https://github.com/trinker/readability)][AppendixAlternativeReadability]
\f0\fs24 \cf0 <!$Scr_Cs::3>
\f1\fs28 . Coefficients are very similar to---and (to my chagrin) standard errors universally smaller than---those presented in the body of the paper.
\f0\fs24 <!$Scr_Ps::0>}