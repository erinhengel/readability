<?xml version="1.0" encoding="UTF-8"?>
<SnapshotIndexes Version="1.0" BinderUUID="7EC30A3C-D3B1-4696-8D15-77F10174FB53">
    <Snapshot Date="2016-05-19 16:19:39 +0100">
        <Title>Untitled Snapshot</Title>
        <Text>Using five well-known readability tests, this paper analyses every article abstract published in the top four economics journals since 1950. Abstracts by women are 2--6 percent more readable than those by men. The gap persists after controlling for editor, journal, year, author institution and primary *JEL* code. It does not go away by including author fixed-effects.
While the evidence does not definitively prove gender bias in peer review, no other alternative argument is quite as convincing.
While this paper is not the first to investigate gender bias in peer review, it is one of the first to find evidence of its existence. A large part of this is because journals and their editors are very good at what they do. 
This study focuses on the narrow field of academic peer review, but its novel methodology---it is the first study to use readability scores to uncover subtle group differences otherwise difficult to quantify---has far broader applications. Although these readability scores are not new, they have so far been used almost exclusively employed within education research---usually to determine whether a specific text is appropriate for its intended audience. But in that domain, they have been extensively studied and tested, meaning their properties are well established. Indeed, in the proposed research context, their only true limitation is their noise---but the proliferation of written material and improvements in text recognition software have never made that hurdle so easy to overcome. The research potential is substantial. Readability scores could measure group differences in any domain in which ideas must be communicated writing (or orally) and large quantities of source material is easily obtainable: journalism, political speeches, business plans, Kickstarter campaigns….
The largest draw-back is their imprecision, but that is easily overcome with large samples
These scores are easy to calculate and their properties have been extensively tested. Given the proliferation of written material and improvements in text recognition software made over the past two decades, there are may other research questions to which they can potentially be applied. This paper focuses on the narrow field of academic economics; similar analyses can question whether a similar phenomenon exists in the wider research domain, as well. The scope for this type of research, however, is potentially far broader. Beyond academia, one might apply the same question to any domain in which ideas must be communicated either orally or in writing: journalism, political speeches, business plans, Kickstarter campaigns….
 and they are easy to calculate. Given the proliferation of written material and improvement of text recognition software, applying them in th
I believe journals are sensitive to bias and quick to correct it
Research on gender bias in peer review process has has almost uniformly focused on whether or  not 
This article focuses on the narrow field of academic economics, but has broader implications, not only to the wider academia, but also to to other fields in which writing is a large part of the job, such as journalism.
that is the most straightforward explanation for the empirical results presented in this paper. 
rule out every alternative explication, it does provide </Text>
    </Snapshot>
</SnapshotIndexes>